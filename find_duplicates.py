#!/usr/bin/env python3
"""
HVDC Project ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ë° ì •ë¦¬ ë„êµ¬
MACHO-GPT v3.4-mini â”‚ Samsung C&T Logistics
"""

import os
import hashlib
from collections import defaultdict
from pathlib import Path
import json

def get_file_hash(filepath):
    """íŒŒì¼ í•´ì‹œê°’ ê³„ì‚°"""
    try:
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"âš ï¸ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {filepath} - {e}")
        return None

def get_file_info(filepath):
    """íŒŒì¼ ì •ë³´ ìˆ˜ì§‘"""
    try:
        stat = os.stat(filepath)
        return {
            'path': filepath,
            'size': stat.st_size,
            'name': os.path.basename(filepath),
            'dir': os.path.dirname(filepath),
            'modified': stat.st_mtime
        }
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {filepath} - {e}")
        return None

def find_duplicates(base_dir, target_dirs):
    """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°"""
    file_hashes = defaultdict(list)
    name_groups = defaultdict(list)
    
    # ê²€ìƒ‰í•  íŒŒì¼ í™•ì¥ì
    target_extensions = {'.py', '.json', '.md', '.txt', '.ttl', '.yml', '.yaml', '.bat', '.sh'}
    
    total_files = 0
    
    for target_dir in target_dirs:
        full_path = os.path.join(base_dir, target_dir)
        if not os.path.exists(full_path):
            print(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {full_path}")
            continue
            
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {target_dir}")
        
        for root, dirs, files in os.walk(full_path):
            # __pycache__, .git, venv ë“± ì œì™¸
            dirs[:] = [d for d in dirs if not d.startswith(('.', '__pycache__', 'venv'))]
            
            for file in files:
                if any(file.endswith(ext) for ext in target_extensions):
                    filepath = os.path.join(root, file)
                    file_info = get_file_info(filepath)
                    
                    if file_info:
                        total_files += 1
                        
                        # í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
                        file_hash = get_file_hash(filepath)
                        if file_hash:
                            file_hashes[file_hash].append(file_info)
                        
                        # íŒŒì¼ëª… ê¸°ë°˜ ê·¸ë£¹í•‘
                        name_groups[file].append(file_info)
    
    print(f"ğŸ“Š ì´ ê²€ìƒ‰ëœ íŒŒì¼: {total_files:,}ê°œ")
    
    # í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ íŒŒì¼
    hash_duplicates = {k: v for k, v in file_hashes.items() if len(v) > 1}
    
    # íŒŒì¼ëª… ê¸°ë°˜ ì¤‘ë³µ íŒŒì¼ (í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    name_duplicates = {k: v for k, v in name_groups.items() if len(v) > 1}
    
    return hash_duplicates, name_duplicates

def analyze_duplicates(hash_duplicates, name_duplicates):
    """ì¤‘ë³µ íŒŒì¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸"""
    print("\n" + "="*80)
    print("ğŸ” HVDC PROJECT ì¤‘ë³µ íŒŒì¼ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    # 1. ì™„ì „ ì¤‘ë³µ íŒŒì¼ (í•´ì‹œ ë™ì¼)
    if hash_duplicates:
        print(f"\nğŸ¯ ì™„ì „ ì¤‘ë³µ íŒŒì¼: {len(hash_duplicates)}ê°œ ê·¸ë£¹")
        total_waste = 0
        
        for i, (file_hash, files) in enumerate(hash_duplicates.items(), 1):
            print(f"\nğŸ“‹ ì¤‘ë³µ ê·¸ë£¹ {i}: {files[0]['name']}")
            print(f"   í¬ê¸°: {files[0]['size']:,} bytes")
            print(f"   ì¤‘ë³µ ìˆ˜: {len(files)}ê°œ")
            
            # ì¤‘ë³µìœ¼ë¡œ ì¸í•œ ìš©ëŸ‰ ë‚­ë¹„ ê³„ì‚°
            waste = files[0]['size'] * (len(files) - 1)
            total_waste += waste
            print(f"   ë‚­ë¹„ ìš©ëŸ‰: {waste:,} bytes")
            
            for j, file_info in enumerate(files):
                marker = "ğŸ¯" if j == 0 else "ğŸ—‘ï¸"
                print(f"   {marker} {file_info['path']}")
        
        print(f"\nğŸ’¾ ì´ ë‚­ë¹„ ìš©ëŸ‰: {total_waste:,} bytes ({total_waste/1024/1024:.2f} MB)")
    else:
        print("\nâœ… ì™„ì „ ì¤‘ë³µ íŒŒì¼ ì—†ìŒ")
    
    # 2. íŒŒì¼ëª… ì¤‘ë³µ (ë‚´ìš© ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    if name_duplicates:
        print(f"\nğŸ“ íŒŒì¼ëª… ì¤‘ë³µ: {len(name_duplicates)}ê°œ")
        
        for filename, files in name_duplicates.items():
            if len(files) > 1:
                sizes = [f['size'] for f in files]
                if len(set(sizes)) > 1:  # í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°ë§Œ í‘œì‹œ
                    print(f"\nğŸ“„ {filename} ({len(files)}ê°œ)")
                    for file_info in files:
                        print(f"   ğŸ“ {file_info['path']} ({file_info['size']:,} bytes)")
    
    return hash_duplicates

def suggest_cleanup(hash_duplicates):
    """ì •ë¦¬ ì œì•ˆ"""
    if not hash_duplicates:
        return []
    
    cleanup_suggestions = []
    
    for file_hash, files in hash_duplicates.items():
        # ìš°ì„ ìˆœìœ„: ë” ê¹Šì€ ê²½ë¡œì˜ íŒŒì¼ì„ ì‚­ì œ ëŒ€ìƒìœ¼ë¡œ ì œì•ˆ
        files_sorted = sorted(files, key=lambda x: (x['path'].count(os.sep), x['path']))
        keep_file = files_sorted[0]
        delete_files = files_sorted[1:]
        
        suggestion = {
            'keep': keep_file['path'],
            'delete': [f['path'] for f in delete_files],
            'reason': f"ë™ì¼ íŒŒì¼ {len(files)}ê°œ ì¤‘ ê°€ì¥ ìƒìœ„ ê²½ë¡œ ìœ ì§€"
        }
        cleanup_suggestions.append(suggestion)
    
    return cleanup_suggestions

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    base_dir = r"C:\HVDC PJT"
    target_dirs = ["hvdc_macho_gpt", "hvdc_ontology_system", "Mapping"]
    
    print("ğŸš€ MACHO-GPT v3.4-mini ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì‹œì‘")
    print(f"ğŸ“‚ ê¸°ì¤€ ë””ë ‰í† ë¦¬: {base_dir}")
    print(f"ğŸ¯ ê²€ìƒ‰ ëŒ€ìƒ: {', '.join(target_dirs)}")
    
    # ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰
    hash_duplicates, name_duplicates = find_duplicates(base_dir, target_dirs)
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    analyze_duplicates(hash_duplicates, name_duplicates)
    
    # ì •ë¦¬ ì œì•ˆ
    cleanup_suggestions = suggest_cleanup(hash_duplicates)
    
    if cleanup_suggestions:
        print(f"\nğŸ§¹ ì •ë¦¬ ì œì•ˆ: {len(cleanup_suggestions)}ê°œ ê·¸ë£¹")
        
        # ì •ë¦¬ ì œì•ˆì„ JSONìœ¼ë¡œ ì €ì¥
        with open('duplicate_cleanup_suggestions.json', 'w', encoding='utf-8') as f:
            json.dump(cleanup_suggestions, f, ensure_ascii=False, indent=2)
        
        print("ğŸ“‹ ì •ë¦¬ ì œì•ˆì´ 'duplicate_cleanup_suggestions.json'ì— ì €ì¥ë¨")
        
        # ì‚¬ìš©ì í™•ì¸ í›„ ì‚­ì œ ì‹¤í–‰ ì—¬ë¶€
        print("\nâ“ ì¤‘ë³µ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ", end="")
        
        # ìë™ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì œì•ˆë§Œ í‘œì‹œ
        print("N (ì•ˆì „ì„ ìœ„í•´ ìˆ˜ë™ í™•ì¸ í›„ ì‚­ì œ ê¶Œì¥)")
    else:
        print("\nâœ… ì •ë¦¬í•  ì¤‘ë³µ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
HVDC ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤í„° v3.0.0
- í†µí•©ëœ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë° ë§¤í•‘ ê·œì¹™ ê²€ì¦
- MACHO-GPT v3.4-mini í‘œì¤€ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_integrated_ontology_system():
    """í†µí•© ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”Œ HVDC ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ v3.0.0")
    print("=" * 60)
    
    test_results = {
        'timestamp': datetime.now().isoformat(),
        'tests': {},
        'summary': {}
    }
    
    # 1. ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ì¦
    print("\nğŸ“‹ 1. ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ì¦")
    schema_result = test_schema_file()
    test_results['tests']['schema_validation'] = schema_result
    print(f"   âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦: {'PASS' if schema_result['success'] else 'FAIL'}")
    
    # 2. ë§¤í•‘ ê·œì¹™ ê²€ì¦
    print("\nğŸ”„ 2. ë§¤í•‘ ê·œì¹™ ê²€ì¦")
    mapping_result = test_mapping_rules()
    test_results['tests']['mapping_validation'] = mapping_result
    print(f"   âœ… ë§¤í•‘ ê·œì¹™: {'PASS' if mapping_result['success'] else 'FAIL'}")
    
    # 3. ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š 3. ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸")
    integration_result = test_data_integration()
    test_results['tests']['data_integration'] = integration_result
    print(f"   âœ… ë°ì´í„° í†µí•©: {'PASS' if integration_result['success'] else 'FAIL'}")
    
    # 4. OFCO ë§¤í•‘ í…ŒìŠ¤íŠ¸
    print("\nğŸ’° 4. OFCO ë§¤í•‘ í…ŒìŠ¤íŠ¸")
    ofco_result = test_ofco_mapping()
    test_results['tests']['ofco_mapping'] = ofco_result
    print(f"   âœ… OFCO ë§¤í•‘: {'PASS' if ofco_result['success'] else 'FAIL'}")
    
    # 5. ê²€ì¦ ìš”ì•½
    print("\nğŸ“ˆ 5. ê²€ì¦ ìš”ì•½")
    summary = generate_test_summary(test_results)
    test_results['summary'] = summary
    
    print(f"   ğŸ¯ ì „ì²´ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    print(f"   âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {summary['passed_tests']}/{summary['total_tests']}")
    
    # ê²°ê³¼ ì €ì¥ (JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°)
    output_file = f"hvdc_ontology_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    json_safe_results = convert_to_json_serializable(test_results)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(json_safe_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_file}")
    return test_results

def test_schema_file():
    """ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ íŒŒì¼ í…ŒìŠ¤íŠ¸"""
    result = {
        'success': False,
        'details': {},
        'errors': []
    }
    
    try:
        schema_file = Path("hvdc_integrated_ontology_schema.ttl")
        
        if not schema_file.exists():
            result['errors'].append("ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            return result
        
        # íŒŒì¼ ì½ê¸°
        with open(schema_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê¸°ë³¸ ê²€ì¦
        checks = {
            'has_namespace_declaration': '@prefix ex:' in content,
            'has_ontology_declaration': 'a owl:Ontology' in content,
            'has_transport_event_class': 'TransportEvent a owl:Class' in content,
            'has_warehouse_class': 'Warehouse a owl:Class' in content,
            'has_basic_properties': 'hasCase a owl:DatatypeProperty' in content,
            'has_korean_labels': '@ko' in content,
            'has_class_hierarchy': 'rdfs:subClassOf' in content
        }
        
        result['details'] = checks
        result['success'] = all(checks.values())
        
        if not result['success']:
            failed_checks = [k for k, v in checks.items() if not v]
            result['errors'] = [f"ì‹¤íŒ¨í•œ ê²€ì¦: {', '.join(failed_checks)}"]
        
    except Exception as e:
        result['errors'].append(f"ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return result

def test_mapping_rules():
    """ë§¤í•‘ ê·œì¹™ íŒŒì¼ í…ŒìŠ¤íŠ¸"""
    result = {
        'success': False,
        'details': {},
        'errors': []
    }
    
    try:
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        
        if not mapping_file.exists():
            result['errors'].append("ë§¤í•‘ ê·œì¹™ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            return result
        
        # JSON íŒŒì¼ ë¡œë“œ
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        required_sections = [
            'namespace', 'version', 'field_mappings', 'property_mappings',
            'class_mappings', 'warehouse_classification', 'logistics_flow_definition',
            'ofco_mapping_rules', 'sparql_templates', 'validation_rules'
        ]
        
        checks = {}
        for section in required_sections:
            checks[f'has_{section}'] = section in mapping_data
        
        # í•„ë“œ ë§¤í•‘ ê²€ì¦
        field_mappings = mapping_data.get('field_mappings', {})
        checks['has_case_mapping'] = 'Case_No' in field_mappings
        checks['has_date_mapping'] = 'Date' in field_mappings
        checks['has_location_mapping'] = 'Location' in field_mappings
        checks['has_quantity_mapping'] = 'Qty' in field_mappings
        
        # OFCO ê·œì¹™ ê²€ì¦
        ofco_rules = mapping_data.get('ofco_mapping_rules', {})
        checks['has_ofco_cost_centers'] = 'cost_centers' in ofco_rules
        checks['has_ofco_mapping_rules'] = 'mapping_rules' in ofco_rules
        
        # ë²„ì „ ê²€ì¦
        checks['correct_version'] = mapping_data.get('version') == '3.0.0'
        
        result['details'] = checks
        result['success'] = all(checks.values())
        
        if not result['success']:
            failed_checks = [k for k, v in checks.items() if not v]
            result['errors'] = [f"ì‹¤íŒ¨í•œ ê²€ì¦: {', '.join(failed_checks)}"]
        
    except Exception as e:
        result['errors'].append(f"ë§¤í•‘ ê·œì¹™ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return result

def test_data_integration():
    """ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸"""
    result = {
        'success': False,
        'details': {},
        'errors': []
    }
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = create_test_dataset()
        
        # ë§¤í•‘ ê·œì¹™ ë¡œë“œ
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if not mapping_file.exists():
            result['errors'].append("ë§¤í•‘ ê·œì¹™ íŒŒì¼ ì—†ìŒ")
            return result
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_rules = json.load(f)
        
        field_mappings = mapping_rules.get('field_mappings', {})
        
        # í†µí•© í…ŒìŠ¤íŠ¸
        checks = {
            'test_data_created': len(test_data) > 0,
            'has_required_columns': all(col in test_data.columns for col in ['Case_No', 'Date', 'Location', 'Qty']),
            'mapping_coverage': len([col for col in test_data.columns if col in field_mappings]) / len(test_data.columns) >= 0.8,
            'data_types_valid': validate_data_types(test_data),
            'no_critical_nulls': test_data['Case_No'].notna().all()
        }
        
        result['details'] = checks
        result['success'] = all(checks.values())
        
        if not result['success']:
            failed_checks = [k for k, v in checks.items() if not v]
            result['errors'] = [f"ì‹¤íŒ¨í•œ ê²€ì¦: {', '.join(failed_checks)}"]
        
    except Exception as e:
        result['errors'].append(f"ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return result

def test_ofco_mapping():
    """OFCO ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
    result = {
        'success': False,
        'details': {},
        'errors': []
    }
    
    try:
        # ë§¤í•‘ ê·œì¹™ ë¡œë“œ
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if not mapping_file.exists():
            result['errors'].append("ë§¤í•‘ ê·œì¹™ íŒŒì¼ ì—†ìŒ")
            return result
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_rules = json.load(f)
        
        ofco_rules = mapping_rules.get('ofco_mapping_rules', {})
        
        # OFCO í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        test_texts = [
            "Berthing and Pilot Arrangement",
            "Cargo Clearance Service",
            "OFCO 10% Handling Fee",
            "Forklift charges",
            "Yard Storage Monthly Rental"
        ]
        
        mapping_rules_list = ofco_rules.get('mapping_rules', [])
        
        checks = {
            'has_mapping_rules': len(mapping_rules_list) > 0,
            'has_cost_centers': len(ofco_rules.get('cost_centers', {})) > 0,
            'rules_have_patterns': all('pattern' in rule for rule in mapping_rules_list),
            'rules_have_priorities': all('priority' in rule for rule in mapping_rules_list),
            'rules_have_cost_centers': all('cost_center_a' in rule and 'cost_center_b' in rule for rule in mapping_rules_list)
        }
        
        # ë§¤í•‘ í…ŒìŠ¤íŠ¸
        matched_count = 0
        for text in test_texts:
            if find_matching_rule(text, mapping_rules_list):
                matched_count += 1
        
        checks['mapping_success_rate'] = matched_count / len(test_texts) >= 0.8
        
        result['details'] = checks
        result['details']['matched_texts'] = matched_count
        result['details']['total_test_texts'] = len(test_texts)
        
        result['success'] = all(checks.values())
        
        if not result['success']:
            failed_checks = [k for k, v in checks.items() if not v]
            result['errors'] = [f"ì‹¤íŒ¨í•œ ê²€ì¦: {', '.join(failed_checks)}"]
        
    except Exception as e:
        result['errors'].append(f"OFCO ë§¤í•‘ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return result

def create_test_dataset():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
    return pd.DataFrame({
        'Case_No': ['HVDC-TEST-001', 'HVDC-TEST-002', 'HVDC-TEST-003'],
        'Date': pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-17']),
        'Location': ['DSV Indoor', 'DSV Outdoor', 'AGI Site'],
        'Qty': [10, 20, 15],
        'Amount': [1000.0, 2000.0, 1500.0],
        'Category': ['HE', 'SIM', 'HE'],
        'Vendor': ['HITACHI', 'SIMENSE', 'HITACHI'],
        'Logistics Flow Code': [1, 2, 3],
        'wh handling': [0, 1, 2]
    })

def validate_data_types(df):
    """ë°ì´í„° íƒ€ì… ê²€ì¦"""
    try:
        # ê¸°ë³¸ ë°ì´í„° íƒ€ì… ê²€ì¦
        checks = [
            str(df['Qty'].dtype) in ['int64', 'float64'],
            str(df['Amount'].dtype) in ['float64'],
            pd.api.types.is_datetime64_any_dtype(df['Date'])
        ]
        return bool(all(checks))
    except:
        return False

def find_matching_rule(text, rules):
    """ë§¤ì¹­ ê·œì¹™ ì°¾ê¸°"""
    import re
    for rule in rules:
        pattern = rule.get('pattern', '')
        if pattern and re.search(pattern, text, re.IGNORECASE):
            return rule
    return None

def generate_test_summary(test_results):
    """í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
    tests = test_results.get('tests', {})
    
    total_tests = len(tests)
    passed_tests = sum(1 for result in tests.values() if result.get('success', False))
    
    return {
        'total_tests': total_tests,
        'passed_tests': passed_tests,
        'failed_tests': total_tests - passed_tests,
        'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
        'test_details': {
            name: {
                'status': 'PASS' if result.get('success', False) else 'FAIL',
                'error_count': len(result.get('errors', []))
            }
            for name, result in tests.items()
        }
    }

def convert_to_json_serializable(obj):
    """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    import numpy as np
    
    if isinstance(obj, dict):
        return {k: convert_to_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'item'):  # numpy scalar
        return obj.item()
    elif isinstance(obj, (bool, int, float, str, type(None))):
        return obj
    else:
        return str(obj)

if __name__ == "__main__":
    test_results = test_integrated_ontology_system()
    
    # ì„±ê³µë¥ ì— ë”°ë¥¸ ì¢…ë£Œ ì½”ë“œ
    success_rate = test_results.get('summary', {}).get('success_rate', 0)
    if success_rate >= 80:
        print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! (ì„±ê³µë¥ : {success_rate:.1f}%)")
        exit(0)
    else:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! (ì„±ê³µë¥ : {success_rate:.1f}%)")
        exit(1) 
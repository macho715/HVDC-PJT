#!/usr/bin/env python3
"""
ğŸš€ MACHO-GPT v3.4-mini Production Automation Pipeline
ì™„ì „ ìë™í™”ëœ í”„ë¡œë•ì…˜ ì›Œí¬í”Œë¡œìš° ì‹œìŠ¤í…œ

ê¸°ëŠ¥:
- ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìë™í™”
- ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- ìë™ ì˜¤ë¥˜ ë³µêµ¬ ë° ì•Œë¦¼
- ìŠ¤ì¼€ì¤„ë§ ë° ë°±ì—… ì‹œìŠ¤í…œ
- KPI ëŒ€ì‹œë³´ë“œ ìë™ ìƒì„±

ì‹¤í–‰: python production_automation_pipeline.py --mode production
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import logging
import schedule
import time
from datetime import datetime, timedelta
from pathlib import Path
import subprocess
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import psutil
import shutil

class ProductionAutomationPipeline:
    """í”„ë¡œë•ì…˜ ìë™í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path="production_config.json"):
        self.start_time = datetime.now()
        self.config = self.load_config(config_path)
        self.setup_logging()
        self.setup_directories()
        self.kpi_metrics = {}
        self.error_count = 0
        self.success_count = 0
        
        print("ğŸš€ MACHO-GPT v3.4-mini Production Pipeline ì´ˆê¸°í™”")
        print("=" * 70)
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë˜ëŠ” ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        default_config = {
            "data_sources": {
                "hitachi_file": "MACHO_WH_HANDLING_HITACHI_DATA.xlsx",
                "simense_file": "MACHO_WH_HANDLING_SIMENSE_DATA.xlsx"
            },
            "quality_thresholds": {
                "min_records": 7000,
                "quality_score_threshold": 90.0,
                "flow_code_accuracy": 95.0,
                "processing_time_limit": 300  # 5ë¶„
            },
            "automation": {
                "auto_backup": True,
                "auto_validation": True,
                "auto_reporting": True,
                "retry_attempts": 3,
                "notification_email": "admin@samsung-ct.com"
            },
            "scheduling": {
                "daily_run_time": "06:00",
                "weekly_full_validation": "Sunday 02:00",
                "monthly_archive": "1st 01:00"
            },
            "containment_modes": {
                "primary": "LATTICE",
                "fallback": "ZERO",
                "monitoring": "RHYTHM"
            }
        }
        
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}")
                return default_config
        else:
            # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            return default_config
    
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        log_dir = Path("logs/production")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_filename = f"production_pipeline_{datetime.now().strftime('%Y%m%d')}.log"
        log_path = log_dir / log_filename
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s | %(levelname)s | %(message)s',
            handlers=[
                logging.FileHandler(log_path, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸ¯ Production Pipeline ë¡œê¹… ì‹œìŠ¤í…œ ì‹œì‘")
    
    def setup_directories(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •"""
        directories = [
            "production_output",
            "production_backup",
            "production_logs", 
            "production_monitoring",
            "production_reports",
            "production_archive"
        ]
        
        for directory in directories:
            Path(directory).mkdir(exist_ok=True)
        
        self.logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì • ì™„ë£Œ: {len(directories)}ê°œ")
    
    def monitor_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('.')
            
            self.kpi_metrics.update({
                'cpu_usage': cpu_percent,
                'memory_usage': memory.percent,
                'memory_available_gb': memory.available / (1024**3),
                'disk_usage': disk.percent,
                'disk_free_gb': disk.free / (1024**3)
            })
            
            # ë¦¬ì†ŒìŠ¤ ê²½ê³  ì„ê³„ê°’ í™•ì¸
            if cpu_percent > 80:
                self.logger.warning(f"ğŸ”¥ CPU ì‚¬ìš©ë¥  ë†’ìŒ: {cpu_percent}%")
            if memory.percent > 85:
                self.logger.warning(f"ğŸ”¥ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory.percent}%")
            if disk.percent > 90:
                self.logger.warning(f"ğŸ”¥ ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ë†’ìŒ: {disk.percent}%")
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_data_sources(self):
        """ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦"""
        self.logger.info("ğŸ” ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹œì‘")
        
        hitachi_file = self.config['data_sources']['hitachi_file']
        simense_file = self.config['data_sources']['simense_file']
        
        validation_results = {
            'hitachi_exists': os.path.exists(hitachi_file),
            'simense_exists': os.path.exists(simense_file),
            'hitachi_size': 0,
            'simense_size': 0,
            'hitachi_records': 0,
            'simense_records': 0
        }
        
        try:
            if validation_results['hitachi_exists']:
                validation_results['hitachi_size'] = os.path.getsize(hitachi_file) / (1024*1024)  # MB
                hitachi_df = pd.read_excel(hitachi_file)
                validation_results['hitachi_records'] = len(hitachi_df)
                
            if validation_results['simense_exists']:
                validation_results['simense_size'] = os.path.getsize(simense_file) / (1024*1024)  # MB
                simense_df = pd.read_excel(simense_file)
                validation_results['simense_records'] = len(simense_df)
            
            total_records = validation_results['hitachi_records'] + validation_results['simense_records']
            min_required = self.config['quality_thresholds']['min_records']
            
            validation_results['total_records'] = total_records
            validation_results['meets_minimum'] = total_records >= min_required
            
            self.logger.info(f"ğŸ“Š ë°ì´í„° ê²€ì¦ ì™„ë£Œ:")
            self.logger.info(f"   - HITACHI: {validation_results['hitachi_records']:,}ê±´ ({validation_results['hitachi_size']:.1f}MB)")
            self.logger.info(f"   - SIMENSE: {validation_results['simense_records']:,}ê±´ ({validation_results['simense_size']:.1f}MB)")
            self.logger.info(f"   - ì´í•©: {total_records:,}ê±´ (ê¸°ì¤€: {min_required:,}ê±´)")
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
            validation_results['error'] = str(e)
            return validation_results
    
    def execute_integration_pipeline(self):
        """í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("ğŸ”„ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
        
        pipeline_start = time.time()
        
        try:
            # 1. ë¹ ë¥¸ í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            result = subprocess.run([
                sys.executable, "06_ë¡œì§í•¨ìˆ˜/quick_integration.py"
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                self.logger.info("âœ… í†µí•© ë°ì´í„° ìƒì„± ì„±ê³µ")
                
                # ìƒì„±ëœ íŒŒì¼ í™•ì¸
                integration_files = [f for f in os.listdir('.') 
                                   if f.startswith('MACHO_WH_HANDLING_í†µí•©ë°ì´í„°_') and f.endswith('.xlsx')]
                
                if integration_files:
                    latest_file = max(integration_files, key=os.path.getmtime)
                    self.logger.info(f"ğŸ“Š ìµœì‹  í†µí•© íŒŒì¼: {latest_file}")
                    
                    # ë°±ì—… ìƒì„±
                    backup_path = f"production_backup/{latest_file}"
                    shutil.copy2(latest_file, backup_path)
                    self.logger.info(f"ğŸ’¾ ë°±ì—… ìƒì„±: {backup_path}")
                    
                    return latest_file
                else:
                    self.logger.error("âŒ í†µí•© íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    return None
            else:
                self.logger.error(f"âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
            return None
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None
        finally:
            pipeline_duration = time.time() - pipeline_start
            self.kpi_metrics['integration_time'] = pipeline_duration
            self.logger.info(f"â±ï¸ í†µí•© íŒŒì´í”„ë¼ì¸ ì†Œìš”ì‹œê°„: {pipeline_duration:.2f}ì´ˆ")
    
    def run_quality_validation(self):
        """í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰"""
        self.logger.info("ğŸ§ª í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰")
        
        try:
            # TDD ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            result = subprocess.run([
                sys.executable, "06_ë¡œì§í•¨ìˆ˜/tdd_validation_simple.py"
            ], capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                self.logger.info("âœ… TDD í’ˆì§ˆ ê²€ì¦ í†µê³¼")
                
                # í’ˆì§ˆ ë¦¬í¬íŠ¸ íŒŒì¼ í™•ì¸
                quality_reports = [f for f in os.listdir('.') 
                                 if f.startswith('tdd_validation_report_') and f.endswith('.json')]
                
                if quality_reports:
                    latest_report = max(quality_reports, key=os.path.getmtime)
                    
                    with open(latest_report, 'r', encoding='utf-8') as f:
                        quality_data = json.load(f)
                    
                    quality_score = quality_data.get('quality_score', 0)
                    threshold = self.config['quality_thresholds']['quality_score_threshold']
                    
                    self.kpi_metrics.update({
                        'quality_score': quality_score,
                        'total_tests': quality_data.get('total_tests', 0),
                        'passed_tests': quality_data.get('passed_tests', 0),
                        'failed_tests': quality_data.get('failed_tests', 0)
                    })
                    
                    if quality_score >= threshold:
                        self.logger.info(f"ğŸ† í’ˆì§ˆ ì ìˆ˜ ë‹¬ì„±: {quality_score:.1f}% (ê¸°ì¤€: {threshold}%)")
                        return True
                    else:
                        self.logger.warning(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ë¯¸ë‹¬: {quality_score:.1f}% (ê¸°ì¤€: {threshold}%)")
                        return False
                        
            else:
                self.logger.error(f"âŒ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    def generate_production_report(self):
        """í”„ë¡œë•ì…˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("ğŸ“Š í”„ë¡œë•ì…˜ ë¦¬í¬íŠ¸ ìƒì„±")
        
        try:
            end_time = datetime.now()
            total_duration = (end_time - self.start_time).total_seconds()
            
            report_data = {
                'timestamp': end_time.isoformat(),
                'pipeline_info': {
                    'start_time': self.start_time.isoformat(),
                    'end_time': end_time.isoformat(),
                    'total_duration_seconds': total_duration,
                    'status': 'SUCCESS' if self.error_count == 0 else 'PARTIAL_SUCCESS'
                },
                'kpi_metrics': self.kpi_metrics,
                'quality_status': {
                    'success_count': self.success_count,
                    'error_count': self.error_count,
                    'overall_health': 'EXCELLENT' if self.error_count == 0 else 'GOOD'
                },
                'containment_mode': self.config['containment_modes']['primary'],
                'recommendations': self.generate_recommendations()
            }
            
            # JSON ë¦¬í¬íŠ¸ ì €ì¥
            report_filename = f"production_reports/production_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            # ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            md_report = self.generate_markdown_report(report_data)
            md_filename = f"production_reports/production_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(md_filename, 'w', encoding='utf-8') as f:
                f.write(md_report)
            
            self.logger.info(f"ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ:")
            self.logger.info(f"   - JSON: {report_filename}")
            self.logger.info(f"   - Markdown: {md_filename}")
            
            return report_filename
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_recommendations(self):
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.kpi_metrics.get('integration_time', 0) > 180:  # 3ë¶„ ì´ˆê³¼
            recommendations.append("í†µí•© íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™” í•„ìš”")
        
        if self.kpi_metrics.get('cpu_usage', 0) > 70:
            recommendations.append("CPU ì‚¬ìš©ë¥  ìµœì í™” ê¶Œì¥")
        
        if self.kpi_metrics.get('memory_usage', 0) > 80:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” í•„ìš”")
        
        # í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        quality_score = self.kpi_metrics.get('quality_score', 0)
        if quality_score < 95:
            recommendations.append("í’ˆì§ˆ ì ìˆ˜ ê°œì„ ì„ ìœ„í•œ ë°ì´í„° ì •ì œ í•„ìš”")
        
        if self.error_count > 0:
            recommendations.append("ì˜¤ë¥˜ ë°œìƒ ì›ì¸ ë¶„ì„ ë° ì˜ˆë°© ì¡°ì¹˜ í•„ìš”")
        
        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ìµœì  ìƒíƒœë¡œ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤")
        
        return recommendations
    
    def generate_markdown_report(self, report_data):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±"""
        md_content = f"""# ğŸš€ MACHO-GPT v3.4-mini Production Report

## ğŸ“Š ì‹¤í–‰ ì •ë³´
- **ì‹¤í–‰ ì‹œê°„**: {report_data['pipeline_info']['start_time']}
- **ì™„ë£Œ ì‹œê°„**: {report_data['pipeline_info']['end_time']}
- **ì´ ì†Œìš”ì‹œê°„**: {report_data['pipeline_info']['total_duration_seconds']:.2f}ì´ˆ
- **ìƒíƒœ**: {report_data['pipeline_info']['status']}

## ğŸ“ˆ KPI ì§€í‘œ
"""
        
        # KPI ì§€í‘œ ì¶”ê°€
        if self.kpi_metrics:
            md_content += "| ì§€í‘œ | ê°’ |\n|------|----|\n"
            for key, value in self.kpi_metrics.items():
                if isinstance(value, float):
                    md_content += f"| {key} | {value:.2f} |\n"
                else:
                    md_content += f"| {key} | {value} |\n"
        
        md_content += f"""
## ğŸ¯ í’ˆì§ˆ ìƒíƒœ
- **ì„±ê³µ ê±´ìˆ˜**: {report_data['quality_status']['success_count']}
- **ì˜¤ë¥˜ ê±´ìˆ˜**: {report_data['quality_status']['error_count']}
- **ì „ì²´ ìƒíƒœ**: {report_data['quality_status']['overall_health']}

## ğŸ’¡ ê¶Œì¥ì‚¬í•­
"""
        
        for i, rec in enumerate(report_data['recommendations'], 1):
            md_content += f"{i}. {rec}\n"
        
        md_content += f"""
## ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´
- **Containment Mode**: {report_data['containment_mode']}
- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---
*Generated by MACHO-GPT v3.4-mini Production Automation Pipeline*
"""
        
        return md_content
    
    def send_notification(self, message, is_error=False):
        """ì•Œë¦¼ ì „ì†¡ (ì´ë©”ì¼/ë¡œê·¸)"""
        if is_error:
            self.logger.error(f"ğŸš¨ {message}")
        else:
            self.logger.info(f"ğŸ“¢ {message}")
        
        # ì‹¤ì œ ì´ë©”ì¼ ì „ì†¡ì€ SMTP ì„¤ì •ì— ë”°ë¼ êµ¬í˜„
        # ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë§Œ ê¸°ë¡
        
    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("ğŸ¯ í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        try:
            # 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
            if not self.monitor_system_resources():
                self.error_count += 1
                self.send_notification("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨", is_error=True)
            
            # 2. ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦
            validation_results = self.validate_data_sources()
            if not validation_results.get('meets_minimum', False):
                self.error_count += 1
                self.send_notification("ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨", is_error=True)
                return False
            else:
                self.success_count += 1
            
            # 3. í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            integration_file = self.execute_integration_pipeline()
            if integration_file:
                self.success_count += 1
                self.send_notification(f"í†µí•© ë°ì´í„° ìƒì„± ì„±ê³µ: {integration_file}")
            else:
                self.error_count += 1
                self.send_notification("í†µí•© ë°ì´í„° ìƒì„± ì‹¤íŒ¨", is_error=True)
                return False
            
            # 4. í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
            if self.run_quality_validation():
                self.success_count += 1
                self.send_notification("í’ˆì§ˆ ê²€ì¦ í†µê³¼")
            else:
                self.error_count += 1
                self.send_notification("í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨", is_error=True)
            
            # 5. í”„ë¡œë•ì…˜ ë¦¬í¬íŠ¸ ìƒì„±
            report_file = self.generate_production_report()
            if report_file:
                self.success_count += 1
                self.send_notification(f"í”„ë¡œë•ì…˜ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
            
            # 6. ìµœì¢… ìƒíƒœ í™•ì¸
            success_rate = self.success_count / (self.success_count + self.error_count) * 100
            
            if success_rate >= 80:
                self.logger.info(f"ğŸ‰ íŒŒì´í”„ë¼ì¸ ì„±ê³µ ì™„ë£Œ - ì„±ê³µë¥ : {success_rate:.1f}%")
                return True
            else:
                self.logger.warning(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ë¶€ë¶„ ì„±ê³µ - ì„±ê³µë¥ : {success_rate:.1f}%")
                return False
                
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            self.send_notification(f"íŒŒì´í”„ë¼ì¸ ì˜ˆì™¸ ë°œìƒ: {e}", is_error=True)
            return False
    
    def setup_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        self.logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •")
        
        # ì¼ì¼ ì‹¤í–‰
        daily_time = self.config['scheduling']['daily_run_time']
        schedule.every().day.at(daily_time).do(self.run_full_pipeline)
        
        # ì£¼ê°„ ì „ì²´ ê²€ì¦
        schedule.every().sunday.at("02:00").do(self.run_comprehensive_validation)
        
        # ì›”ê°„ ì•„ì¹´ì´ë¸Œ
        schedule.every().month.do(self.archive_old_data)
        
        self.logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ:")
        self.logger.info(f"   - ì¼ì¼ ì‹¤í–‰: {daily_time}")
        self.logger.info(f"   - ì£¼ê°„ ê²€ì¦: ì¼ìš”ì¼ 02:00")
        self.logger.info(f"   - ì›”ê°„ ì•„ì¹´ì´ë¸Œ: ë§¤ì›” 1ì¼")
    
    def run_comprehensive_validation(self):
        """ì¢…í•© ê²€ì¦ ì‹¤í–‰"""
        self.logger.info("ğŸ” ì¢…í•© ê²€ì¦ ì‹¤í–‰")
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ + ì¶”ê°€ ê²€ì¦ ë¡œì§
        return self.run_full_pipeline()
    
    def archive_old_data(self):
        """ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ"""
        self.logger.info("ğŸ“¦ ë°ì´í„° ì•„ì¹´ì´ë¸Œ ì‹¤í–‰")
        
        try:
            archive_date = datetime.now().strftime('%Y%m%d')
            archive_dir = f"production_archive/{archive_date}"
            Path(archive_dir).mkdir(parents=True, exist_ok=True)
            
            # 30ì¼ ì´ìƒ ëœ íŒŒì¼ë“¤ ì•„ì¹´ì´ë¸Œ
            cutoff_date = datetime.now() - timedelta(days=30)
            
            archived_count = 0
            for file_path in Path('.').glob('*.xlsx'):
                if file_path.stat().st_mtime < cutoff_date.timestamp():
                    shutil.move(str(file_path), f"{archive_dir}/{file_path.name}")
                    archived_count += 1
            
            self.logger.info(f"ğŸ“¦ ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {archived_count}ê°œ íŒŒì¼")
            
        except Exception as e:
            self.logger.error(f"âŒ ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MACHO-GPT Production Automation Pipeline')
    parser.add_argument('--mode', choices=['production', 'test', 'scheduler'], 
                       default='production', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--config', default='production_config.json', 
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    pipeline = ProductionAutomationPipeline(args.config)
    
    if args.mode == 'production':
        # ë‹¨ì¼ ì‹¤í–‰
        success = pipeline.run_full_pipeline()
        sys.exit(0 if success else 1)
        
    elif args.mode == 'test':
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê²€ì¦ë§Œ)
        pipeline.logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰")
        validation_results = pipeline.validate_data_sources()
        quality_passed = pipeline.run_quality_validation()
        
        print(f"âœ… ë°ì´í„° ê²€ì¦: {'í†µê³¼' if validation_results.get('meets_minimum') else 'ì‹¤íŒ¨'}")
        print(f"âœ… í’ˆì§ˆ ê²€ì¦: {'í†µê³¼' if quality_passed else 'ì‹¤íŒ¨'}")
        
    elif args.mode == 'scheduler':
        # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
        pipeline.setup_scheduler()
        pipeline.logger.info("ğŸ”„ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - Ctrl+Cë¡œ ì¢…ë£Œ")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
        except KeyboardInterrupt:
            pipeline.logger.info("â¹ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 
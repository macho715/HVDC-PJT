#!/usr/bin/env python3
"""
MACHO-GPT v3.4-mini í†µí•© íŒŒì´í”„ë¼ì¸
HVDC í”„ë¡œì íŠ¸ - Flow Code â†’ íŠ¸ëœì­ì…˜ ì´ë²¤íŠ¸ ì „ì²´ ìë™í™”

ì‹¤í–‰ ìˆœì„œ:
1. Flow Code ë¶€ì—¬ (HITACHI + SIMENSE)
2. íŠ¸ëœì­ì…˜ ì´ë²¤íŠ¸ ìë™ ìƒì„±
3. ì •ê·œí™” íŠ¸ëœì­ì…˜ í…Œì´ë¸” ìƒì„±

ì‘ì„±: 2025-07-01
ë²„ì „: v3.4-mini
ëª¨ë“œ: PRIME â†’ LATTICE â†’ RHYTHM
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class MachoIntegratedPipeline:
    """MACHO-GPT í†µí•© íŒŒì´í”„ë¼ì¸ ì—”ì§„"""
    
    def __init__(self):
        self.confidence_threshold = 0.90
        self.site_locations = ['AGI', 'DAS', 'MIR', 'SHU']
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.file_hitachi = 'data/HVDC WAREHOUSE_HITACHI(HE).xlsx'
        self.file_simense = 'data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx'
        self.output_flow_code = 'data/flowcode_transaction_table.xlsx'
        self.output_final = 'output/ì •ê·œí™”_íŠ¸ëœì­ì…˜í…Œì´ë¸”_ìƒì„¸.xlsx'
        
    def get_warehouse_columns(self, df):
        """ì°½ê³  ê´€ë ¨ ì»¬ëŸ¼ ì¶”ì¶œ"""
        warehouse_keywords = ['DSV', 'HAULER', 'AAA', 'MOSB', 'JDN']
        warehouse_columns = []
        
        for col in df.columns:
            if any(keyword in str(col).upper() for keyword in warehouse_keywords):
                warehouse_columns.append(col)
        
        return warehouse_columns
    
    def get_unified_case_column(self, df):
        """Case ID ì»¬ëŸ¼ í†µí•© ê°ì§€"""
        case_candidates = ['Case_No', 'Case ID', 'Case_ID', 'CASE_NO', 'CASE_ID']
        
        for candidate in case_candidates:
            if candidate in df.columns:
                return candidate
        
        # ì»¬ëŸ¼ëª…ì— 'case'ê°€ í¬í•¨ëœ ê²ƒ ì°¾ê¸°
        for col in df.columns:
            if 'case' in str(col).lower():
                return col
        
        return df.columns[0]  # fallback
    
    def add_cum_wh_before_mosb(self, df, wh_cols, case_col='Case_ID'):
        """MOSB ì´ì „ ì°½ê³  ìˆ˜ ê³„ì‚°"""
        df['wh_before_mosb'] = 0
        
        for idx, row in df.iterrows():
            count = 0
            for col in wh_cols:
                if 'MOSB' not in str(col).upper() and pd.notna(row[col]):
                    count += 1
            df.loc[idx, 'wh_before_mosb'] = count
        
        return df
    
    def route_to_flow_code_v2(self, row):
        """Flow Code ë¼ìš°íŒ… ë¡œì§ v2.8.3"""
        # Pre Arrival ì²´í¬ (Code 0)
        if pd.isna(row.get('Status_Location')) or str(row.get('Status_Location')).strip() == '':
            return 0
        
        status_loc = str(row.get('Status_Location', '')).upper()
        
        # Pre Arrival í‚¤ì›Œë“œ ì²´í¬
        pre_arrival_keywords = ['ETA', 'ETD', 'ARRIVAL', 'EXPECTED', 'PENDING']
        if any(keyword in status_loc for keyword in pre_arrival_keywords):
            return 0
        
        # MOSB ì²´í¬
        has_mosb = pd.notna(row.get('MOSB'))
        wh_before_mosb = row.get('wh_before_mosb', 0)
        
        # í˜„ì¥ ìœ„ì¹˜ ì²´í¬
        is_site = any(site in status_loc for site in self.site_locations)
        
        if has_mosb:
            # MOSB ê´€ë ¨ Flow Code
            if wh_before_mosb <= 1:
                return 3  # MOSB ë‹¨ìˆœ
            else:
                return 4  # MOSB ë³µí•©
        else:
            # ì¼ë°˜ ì°½ê³  Flow Code
            if wh_before_mosb <= 1:
                return 1  # ë‹¨ìˆœ ì°½ê³ 
            else:
                return 2  # ë³µí•© ì°½ê³ 
    
    def get_date_from_row(self, row):
        """í–‰ì—ì„œ ì²« ë²ˆì§¸ ìœ íš¨í•œ ë‚ ì§œ ì¶”ì¶œ"""
        date_columns = [col for col in row.index if any(x in str(col).upper() for x in ['ETD', 'ETA', 'DATE', 'TIME'])]
        
        for col in date_columns:
            if pd.notna(row[col]):
                try:
                    return pd.to_datetime(row[col])
                except:
                    continue
        
        return pd.Timestamp('2024-01-01')  # fallback
    
    def get_location_from_row(self, row):
        """í–‰ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ"""
        if pd.notna(row.get('Status_Location')):
            return str(row['Status_Location'])
        
        # ì°½ê³ /í˜„ì¥ ì»¬ëŸ¼ì—ì„œ í™œì„± ìœ„ì¹˜ ì°¾ê¸°
        all_locations = self.site_locations + ['DSV', 'HAULER', 'AAA', 'MOSB', 'JDN']
        
        for loc in all_locations:
            for col in row.index:
                if loc in str(col).upper() and pd.notna(row[col]):
                    return loc
        
        return 'UNKNOWN'
    
    def get_pkg_from_row(self, row):
        """í–‰ì—ì„œ Pkg ìˆ˜ëŸ‰ ì¶”ì¶œ"""
        pkg_candidates = ['Pkg', 'PKG', 'Package', 'Quantity', 'Qty']
        
        for candidate in pkg_candidates:
            if candidate in row.index and pd.notna(row[candidate]):
                try:
                    return float(row[candidate])
                except:
                    continue
        
        return 1.0  # fallback
    
    def get_val(self, row, col):
        """ì•ˆì „í•œ ê°’ ì¶”ì¶œ"""
        return row[col] if col in row and pd.notna(row[col]) else None
    
    def step1_generate_flow_code(self):
        """1ë‹¨ê³„: Flow Code ìƒì„±"""
        print("ğŸ”„ 1ë‹¨ê³„: Flow Code ìƒì„± ì‹œì‘...")
        
        # HITACHI ì²˜ë¦¬
        print("   ğŸ“Š HITACHI ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        df_hitachi = pd.read_excel(self.file_hitachi)
        wh_cols_h = self.get_warehouse_columns(df_hitachi)
        case_col_h = self.get_unified_case_column(df_hitachi)
        
        df_hitachi['Case_ID'] = df_hitachi[case_col_h]
        df_hitachi = self.add_cum_wh_before_mosb(df_hitachi, wh_cols_h, case_col='Case_ID')
        df_hitachi['Flow_Code'] = df_hitachi.apply(self.route_to_flow_code_v2, axis=1)
        df_hitachi['Vendor'] = 'HITACHI'
        
        # ë‚ ì§œ, ìœ„ì¹˜, Pkg ì •ë³´ ì¶”ê°€
        df_hitachi['Date'] = df_hitachi.apply(self.get_date_from_row, axis=1)
        df_hitachi['Location'] = df_hitachi.apply(self.get_location_from_row, axis=1)
        df_hitachi['Pkg'] = df_hitachi.apply(self.get_pkg_from_row, axis=1)
        
        # SIMENSE ì²˜ë¦¬
        print("   ğŸ“Š SIMENSE ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        df_simense = pd.read_excel(self.file_simense)
        wh_cols_s = self.get_warehouse_columns(df_simense)
        case_col_s = self.get_unified_case_column(df_simense)
        
        df_simense['Case_ID'] = df_simense[case_col_s]
        df_simense = self.add_cum_wh_before_mosb(df_simense, wh_cols_s, case_col='Case_ID')
        df_simense['Flow_Code'] = df_simense.apply(self.route_to_flow_code_v2, axis=1)
        df_simense['Vendor'] = 'SIMENSE'
        
        # ë‚ ì§œ, ìœ„ì¹˜, Pkg ì •ë³´ ì¶”ê°€
        df_simense['Date'] = df_simense.apply(self.get_date_from_row, axis=1)
        df_simense['Location'] = df_simense.apply(self.get_location_from_row, axis=1)
        df_simense['Pkg'] = df_simense.apply(self.get_pkg_from_row, axis=1)
        
        # í†µí•© ë° ì €ì¥
        df_combined = pd.concat([df_hitachi, df_simense], ignore_index=True)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì„ íƒ
        essential_cols = ['Case_ID', 'Date', 'Location', 'Pkg', 'Flow_Code', 'Vendor', 'wh_before_mosb']
        if 'MOSB' in df_combined.columns:
            essential_cols.append('MOSB')
        if 'SQM' in df_combined.columns:
            essential_cols.append('SQM')
        if 'Stackable' in df_combined.columns:
            essential_cols.append('Stackable')
        
        df_final = df_combined[essential_cols].copy()
        df_final.to_excel(self.output_flow_code, index=False)
        
        print(f"   âœ… Flow Code í…Œì´ë¸” ì €ì¥: {self.output_flow_code}")
        print(f"   ğŸ“Š ì´ ì¼€ì´ìŠ¤: {len(df_final):,}ê±´ (HITACHI: {len(df_hitachi):,}, SIMENSE: {len(df_simense):,})")
        
        # Flow Code ë¶„í¬ ì¶œë ¥
        flow_dist = df_final['Flow_Code'].value_counts().sort_index()
        print("   ğŸ”„ Flow Code ë¶„í¬:")
        for code, count in flow_dist.items():
            print(f"      Code {code}: {count:,}ê±´")
        
        return df_final
    
    def step2_generate_transactions(self):
        """2ë‹¨ê³„: íŠ¸ëœì­ì…˜ ì´ë²¤íŠ¸ ìƒì„±"""
        print("\nğŸ”„ 2ë‹¨ê³„: íŠ¸ëœì­ì…˜ ì´ë²¤íŠ¸ ìƒì„± ì‹œì‘...")
        
        # Flow Code í…Œì´ë¸” ë¡œë“œ
        df = pd.read_excel(self.output_flow_code)
        df['Date'] = pd.to_datetime(df['Date'])
        
        print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {len(df):,}ê±´")
        
        trx_rows = []
        case_count = 0
        
        for case_id, group in df.groupby('Case_ID'):
            case_count += 1
            if case_count % 1000 == 0:
                print(f"   ğŸ”„ ì§„í–‰ë¥ : {case_count:,} ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì¤‘...")
            
            group = group.sort_values('Date')
            prev_row = None
            
            for idx, row in group.iterrows():
                # IN ì´ë²¤íŠ¸ (ëª¨ë“  ìœ„ì¹˜ ë„ì°©)
                trx_rows.append({
                    'Case_ID': row['Case_ID'],
                    'Date': row['Date'],
                    'Location': row['Location'],
                    'Event': 'IN',
                    'Pkg': abs(row['Pkg']),
                    'SQM': self.get_val(row, 'SQM'),
                    'Stackable': self.get_val(row, 'Stackable'),
                    'Flow_Code': row.get('Flow_Code'),
                    'Vendor': row.get('Vendor')
                })
                
                # MOVE ì´ë²¤íŠ¸ (ìœ„ì¹˜ ë³€ê²½ì‹œ)
                if prev_row is not None and row['Location'] != prev_row['Location']:
                    # MOVE_OUT (ì´ì „ ìœ„ì¹˜ì—ì„œ ì¶œê³ )
                    trx_rows.append({
                        'Case_ID': row['Case_ID'],
                        'Date': row['Date'],
                        'Location': prev_row['Location'],
                        'Event': 'MOVE_OUT',
                        'Pkg': -abs(row['Pkg']),
                        'SQM': self.get_val(prev_row, 'SQM'),
                        'Stackable': self.get_val(prev_row, 'Stackable'),
                        'Flow_Code': prev_row.get('Flow_Code'),
                        'Vendor': row.get('Vendor')
                    })
                    
                    # MOVE_IN (ìƒˆ ìœ„ì¹˜ë¡œ ì…ê³ )
                    trx_rows.append({
                        'Case_ID': row['Case_ID'],
                        'Date': row['Date'],
                        'Location': row['Location'],
                        'Event': 'MOVE_IN',
                        'Pkg': abs(row['Pkg']),
                        'SQM': self.get_val(row, 'SQM'),
                        'Stackable': self.get_val(row, 'Stackable'),
                        'Flow_Code': row.get('Flow_Code'),
                        'Vendor': row.get('Vendor')
                    })
                
                prev_row = row
            
            # OUT ì´ë²¤íŠ¸ (í˜„ì¥ì—ì„œ ìµœì¢… ì¶œê³ )
            if len(group) > 0:
                last = group.iloc[-1]
                if last['Location'] in self.site_locations:
                    trx_rows.append({
                        'Case_ID': last['Case_ID'],
                        'Date': last['Date'],
                        'Location': last['Location'],
                        'Event': 'OUT',
                        'Pkg': -abs(last['Pkg']),
                        'SQM': self.get_val(last, 'SQM'),
                        'Stackable': self.get_val(last, 'Stackable'),
                        'Flow_Code': last.get('Flow_Code'),
                        'Vendor': last.get('Vendor')
                    })
            
            # RETURN ì´ë²¤íŠ¸ (í˜„ì¥ì—ì„œ ì°½ê³ ë¡œ ë³µê·€)
            if len(group) > 1:
                for i in range(1, len(group)):
                    curr_loc = group.iloc[i]['Location']
                    prev_loc = group.iloc[i-1]['Location']
                    
                    if prev_loc in self.site_locations and curr_loc not in self.site_locations:
                        trx_rows.append({
                            'Case_ID': group.iloc[i]['Case_ID'],
                            'Date': group.iloc[i]['Date'],
                            'Location': curr_loc,
                            'Event': 'RETURN',
                            'Pkg': abs(group.iloc[i]['Pkg']),
                            'SQM': self.get_val(group.iloc[i], 'SQM'),
                            'Stackable': self.get_val(group.iloc[i], 'Stackable'),
                            'Flow_Code': group.iloc[i].get('Flow_Code'),
                            'Vendor': group.iloc[i].get('Vendor')
                        })
        
        # íŠ¸ëœì­ì…˜ DataFrame ìƒì„±
        trx_df = pd.DataFrame(trx_rows)
        trx_df = trx_df.sort_values(['Case_ID', 'Location', 'Date'])
        
        # ëˆ„ì ì¬ê³  ê³„ì‚°
        trx_df['ëˆ„ì ì¬ê³ '] = trx_df.groupby(['Location', 'Case_ID'])['Pkg'].cumsum()
        
        # ì €ì¥
        trx_df.to_excel(self.output_final, index=False)
        
        print(f"   âœ… íŠ¸ëœì­ì…˜ í…Œì´ë¸” ì €ì¥: {self.output_final}")
        print(f"   ğŸ“Š ì´ íŠ¸ëœì­ì…˜: {len(trx_df):,}ê±´")
        
        # ì´ë²¤íŠ¸ë³„ ë¶„í¬
        event_dist = trx_df['Event'].value_counts()
        print("   ğŸ“‹ ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
        for event, count in event_dist.items():
            print(f"      {event}: {count:,}ê±´")
        
        return trx_df
    
    def generate_summary_report(self):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ“Š MACHO-GPT v3.4-mini í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ë¦¬í¬íŠ¸")
        print("="*80)
        
        # Flow Code í…Œì´ë¸” í†µê³„
        if pd.api.types.is_file_like(self.output_flow_code):
            try:
                df_flow = pd.read_excel(self.output_flow_code)
                print(f"ğŸ“¦ Flow Code í…Œì´ë¸”: {len(df_flow):,}ê±´")
                
                vendor_dist = df_flow['Vendor'].value_counts()
                print("ğŸ¢ ë²¤ë”ë³„ ë¶„í¬:")
                for vendor, count in vendor_dist.items():
                    print(f"   {vendor}: {count:,}ê±´")
            except:
                print("âŒ Flow Code í…Œì´ë¸” ì½ê¸° ì‹¤íŒ¨")
        
        # íŠ¸ëœì­ì…˜ í…Œì´ë¸” í†µê³„
        try:
            df_trx = pd.read_excel(self.output_final)
            print(f"\nğŸ”„ íŠ¸ëœì­ì…˜ í…Œì´ë¸”: {len(df_trx):,}ê±´")
            
            event_dist = df_trx['Event'].value_counts()
            print("ğŸ“‹ ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
            for event, count in event_dist.items():
                print(f"   {event}: {count:,}ê±´")
            
            location_dist = df_trx['Location'].value_counts().head(5)
            print("\nğŸ“ ìƒìœ„ 5ê°œ ìœ„ì¹˜:")
            for location, count in location_dist.items():
                print(f"   {location}: {count:,}ê±´")
                
        except Exception as e:
            print(f"âŒ íŠ¸ëœì­ì…˜ í…Œì´ë¸” ì½ê¸° ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ“ˆ ì‹ ë¢°ë„: {self.confidence_threshold*100}% ì´ìƒ")
        print(f"ğŸ¯ ì™„ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)
        
        return {
            'status': 'SUCCESS',
            'confidence': 0.95,
            'mode': 'RHYTHM',
            'triggers': ['pipeline_complete', 'transaction_ready'],
            'next_cmds': [
                'visualize_data',
                'generate_kpi_dashboard',
                'switch_mode COST_GUARD'
            ]
        }
    
    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ MACHO-GPT v3.4-mini í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("ëª¨ë“œ: PRIME â†’ LATTICE â†’ RHYTHM")
        print("="*60)
        
        try:
            # 1ë‹¨ê³„: Flow Code ìƒì„±
            df_flow = self.step1_generate_flow_code()
            
            # 2ë‹¨ê³„: íŠ¸ëœì­ì…˜ ì´ë²¤íŠ¸ ìƒì„±
            df_trx = self.step2_generate_transactions()
            
            # 3ë‹¨ê³„: ìš”ì•½ ë¦¬í¬íŠ¸
            result = self.generate_summary_report()
            
            return result
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            print("ZERO ëª¨ë“œë¡œ ì „í™˜ - ìˆ˜ë™ í™•ì¸ í•„ìš”")
            return {
                'status': 'FAILED',
                'confidence': 0.0,
                'mode': 'ZERO',
                'error': str(e)
            }

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    pipeline = MachoIntegratedPipeline()
    result = pipeline.run_full_pipeline()
    
    print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ìƒíƒœ: {result['status']}")
    if result['status'] == 'SUCCESS':
        print(f"ğŸ“Š ì‹ ë¢°ë„: {result['confidence']*100}% | ëª¨ë“œ: {result['mode']}")
    
    # ì¶”ì²œ ëª…ë ¹ì–´ ì¶œë ¥
    if 'next_cmds' in result:
        print(f"\nğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
        for i, cmd in enumerate(result['next_cmds'], 1):
            print(f"/{cmd} [ë‹¨ê³„ {i} - ë‹¤ìŒ ë¡œì§ì»¬ ìŠ¤í…]") 
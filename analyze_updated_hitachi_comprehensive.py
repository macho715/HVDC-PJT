#!/usr/bin/env python3
"""
MACHO-GPT v3.4-mini | ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ì¢…í•© ë¶„ì„
Samsung C&T Ã— ADNOC DSV Partnership | HVDC í”„ë¡œì íŠ¸

ì°¸ì¡° ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„:
1.   .md
2. ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°.md  
3. hvdc_logi_master_integrated.py

TDD ë°©ë²•ë¡  ì ìš©: Red â†’ Green â†’ Refactor
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
import logging
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - MACHO-GPT - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class UpdatedHitachiAnalyzer:
    """ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ì¢…í•© ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.mode = "LATTICE"
        
        # TDD ë³´ê³ ì„œ ê¸°ì¤€ê°’ (ì°¸ì¡° ë¬¸ì„œ ê¸°ë°˜)
        self.tdd_baseline = {
            'hitachi_count': 5346,
            'flow_code_2_target': 886,
            'flow_code_2_achieved': 886,  # 100% ë‹¬ì„±
            'overall_success_rate': 0.599  # 59.9%
        }
        
        # ì°½ê³ /í˜„ì¥ ì›”ë³„ ì‹œíŠ¸ êµ¬ì¡° ìš”êµ¬ì‚¬í•­
        self.warehouse_columns = [
            'DSV Indoor', 'DSV Outdoor', 'DSV Al Markaz', 
            'DSV MZP', 'MOSB', 'AAA Storage', 'Hauler Indoor'
        ]
        self.site_columns = ['AGI', 'DAS', 'MIR', 'SHU']
        
        # í•„ìˆ˜ ì»¬ëŸ¼ (TDD ë³´ê³ ì„œ ê¸°ì¤€)
        self.essential_columns = [
            'Case No.', 'Package', 'DSV Indoor', 'DSV Outdoor',
            'AGI', 'DAS', 'MIR', 'SHU'
        ]
        
        # ë¶„ì„ ê²°ê³¼
        self.analysis_results = {}
        
    def load_updated_hitachi_data(self) -> pd.DataFrame:
        """ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¡œë“œ ì‹œì‘")
        print(f"ğŸ¯ {self.mode} ëª¨ë“œ: ì •ë°€ ë°ì´í„° ë¶„ì„")
        
        file_paths = [
            "hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_HITACHI(HE).xlsx",
            "HVDC WAREHOUSE_HITACHI(HE).xlsx",
            "hvdc_ontology_system/data/HVDC WAREHOUSE_HITACHI(HE).xlsx"
        ]
        
        for file_path in file_paths:
            if os.path.exists(file_path):
                try:
                    file_size = os.path.getsize(file_path) / (1024 * 1024)
                    print(f"ğŸ“‚ íŒŒì¼ ë°œê²¬: {file_path}")
                    print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                    
                    df = pd.read_excel(file_path)
                    print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df):,}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼")
                    
                    return df
                    
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
                    continue
        
        print(f"âŒ HITACHI ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return pd.DataFrame()
    
    def analyze_data_changes(self, df: pd.DataFrame) -> dict:
        """ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„ (TDD ë³´ê³ ì„œ ê¸°ì¤€)"""
        print("\n" + "="*80)
        print("ğŸ” TDD ì‹œìŠ¤í…œ ë¡œì§ ë³´ì • ì˜í–¥ ë¶„ì„")
        print("="*80)
        
        if df.empty:
            return {'error': 'No data to analyze'}
        
        changes = {
            'record_count_analysis': {},
            'column_structure_analysis': {},
            'tdd_impact_assessment': {},
            'flow_code_validation': {},
            'data_quality_metrics': {}
        }
        
        # 1. ë ˆì½”ë“œ ìˆ˜ ë³€í™” ë¶„ì„
        current_count = len(df)
        baseline_count = self.tdd_baseline['hitachi_count']
        difference = current_count - baseline_count
        change_percentage = (difference / baseline_count) * 100 if baseline_count > 0 else 0
        
        changes['record_count_analysis'] = {
            'baseline_count': baseline_count,
            'current_count': current_count,
            'difference': difference,
            'change_percentage': change_percentage,
            'significant_change': abs(difference) > 100
        }
        
        print(f"ğŸ“Š ë ˆì½”ë“œ ìˆ˜ ë³€í™” ë¶„ì„:")
        print(f"   - TDD ê¸°ì¤€ê°’: {baseline_count:,}ê±´")
        print(f"   - í˜„ì¬ ë°ì´í„°: {current_count:,}ê±´")
        print(f"   - ì°¨ì´: {difference:+,}ê±´ ({change_percentage:+.1f}%)")
        
        if abs(difference) > 100:
            print(f"   ğŸš¨ ì¤‘ëŒ€í•œ ë³€í™” ê°ì§€ - Flow Code ë¡œì§ ì¬ê²€ì¦ í•„ìš”")
        else:
            print(f"   âœ… ë³€í™” ë¯¸ë¯¸ - ê¸°ì¡´ TDD ë¡œì§ ìœ ì§€ ê°€ëŠ¥")
        
        # 2. ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„
        current_columns = set(df.columns)
        required_columns = set(self.essential_columns)
        warehouse_present = set(self.warehouse_columns) & current_columns
        site_present = set(self.site_columns) & current_columns
        
        missing_essential = required_columns - current_columns
        new_columns = current_columns - required_columns
        
        changes['column_structure_analysis'] = {
            'total_columns': len(current_columns),
            'missing_essential': list(missing_essential),
            'new_columns': list(new_columns),
            'warehouse_columns_present': list(warehouse_present),
            'site_columns_present': list(site_present),
            'structure_compatible': len(missing_essential) == 0
        }
        
        print(f"\nğŸ“‹ ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„:")
        print(f"   - ì „ì²´ ì»¬ëŸ¼: {len(current_columns)}ê°œ")
        print(f"   - ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {missing_essential if missing_essential else 'ì—†ìŒ'}")
        print(f"   - ì°½ê³  ì»¬ëŸ¼ ì¡´ì¬: {len(warehouse_present)}ê°œ {list(warehouse_present)}")
        print(f"   - í˜„ì¥ ì»¬ëŸ¼ ì¡´ì¬: {len(site_present)}ê°œ {list(site_present)}")
        
        # 3. TDD ì˜í–¥ë„ í‰ê°€
        flow_code_columns = [col for col in df.columns if 'FLOW' in col.upper()]
        has_flow_code = len(flow_code_columns) > 0
        
        changes['tdd_impact_assessment'] = {
            'flow_code_columns_found': flow_code_columns,
            'flow_code_logic_intact': has_flow_code,
            'warehouse_monthly_structure_viable': len(warehouse_present) >= 5,
            'site_monthly_structure_viable': len(site_present) == 4,
            'overall_compatibility_score': self._calculate_compatibility_score(changes)
        }
        
        print(f"\nğŸ¯ TDD ì‹œìŠ¤í…œ ë¡œì§ ë³´ì • ì˜í–¥:")
        print(f"   - Flow Code ì»¬ëŸ¼: {flow_code_columns}")
        print(f"   - FLOW CODE 2 ë¡œì§ 100% ë‹¬ì„± ìœ ì§€ ê°€ëŠ¥: {'âœ…' if has_flow_code else 'âŒ'}")
        print(f"   - ì°½ê³  ì›”ë³„ ì‹œíŠ¸ ìƒì„± ê°€ëŠ¥: {'âœ…' if len(warehouse_present) >= 5 else 'âŒ'}")
        print(f"   - í˜„ì¥ ì›”ë³„ ì‹œíŠ¸ ìƒì„± ê°€ëŠ¥: {'âœ…' if len(site_present) == 4 else 'âŒ'}")
        
        # 4. Flow Code ê²€ì¦
        if has_flow_code and flow_code_columns:
            flow_col = flow_code_columns[0]
            flow_distribution = df[flow_col].value_counts().sort_index().to_dict()
            
            # TDD ë³´ê³ ì„œ ê¸°ì¤€ Flow Code 2 ê²€ì¦
            flow_code_2_current = flow_distribution.get(2, 0)
            flow_code_2_target = self.tdd_baseline['flow_code_2_target']
            flow_code_2_accuracy = 1 - abs(flow_code_2_current - flow_code_2_target) / flow_code_2_target if flow_code_2_target > 0 else 0
            
            changes['flow_code_validation'] = {
                'distribution': flow_distribution,
                'flow_code_2_current': flow_code_2_current,
                'flow_code_2_target': flow_code_2_target,
                'flow_code_2_accuracy': flow_code_2_accuracy,
                'maintains_100_percent_achievement': flow_code_2_accuracy > 0.95
            }
            
            print(f"\nğŸ”§ Flow Code ê²€ì¦:")
            print(f"   - í˜„ì¬ ë¶„í¬: {flow_distribution}")
            print(f"   - FLOW CODE 2 í˜„ì¬/ëª©í‘œ: {flow_code_2_current}/{flow_code_2_target}")
            print(f"   - FLOW CODE 2 ì •í™•ë„: {flow_code_2_accuracy:.1%}")
            print(f"   - 100% ë‹¬ì„± ìœ ì§€: {'âœ…' if flow_code_2_accuracy > 0.95 else 'âŒ'}")
        
        # 5. ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
        if 'Case No.' in df.columns:
            duplicates = df.duplicated(subset=['Case No.']).sum()
            null_case_no = df['Case No.'].isna().sum()
            duplicate_rate = duplicates / len(df) if len(df) > 0 else 0
            null_rate = null_case_no / len(df) if len(df) > 0 else 0
        else:
            duplicates = null_case_no = duplicate_rate = null_rate = 0
        
        total_cells = len(df) * len(df.columns) if not df.empty else 1
        filled_cells = total_cells - df.isna().sum().sum() if not df.empty else 0
        quality_score = (filled_cells / total_cells) * 100
        
        changes['data_quality_metrics'] = {
            'duplicate_count': duplicates,
            'duplicate_rate': duplicate_rate,
            'null_case_no_count': null_case_no,
            'null_case_no_rate': null_rate,
            'overall_quality_score': quality_score,
            'quality_grade': self._get_quality_grade(quality_score)
        }
        
        print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ ì§€í‘œ:")
        print(f"   - Case No. ì¤‘ë³µ: {duplicates}ê±´ ({duplicate_rate:.1%})")
        print(f"   - Case No. ëˆ„ë½: {null_case_no}ê±´ ({null_rate:.1%})")
        print(f"   - ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}%")
        print(f"   - í’ˆì§ˆ ë“±ê¸‰: {self._get_quality_grade(quality_score)}")
        
        return changes
    
    def _calculate_compatibility_score(self, changes: dict) -> float:
        """í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ë ˆì½”ë“œ ìˆ˜ ë³€í™” ì ìˆ˜ (30%)
        if not changes['record_count_analysis']['significant_change']:
            score += 30
        
        # ì»¬ëŸ¼ êµ¬ì¡° ì ìˆ˜ (40%)
        if changes['column_structure_analysis']['structure_compatible']:
            score += 40
        
        # TDD ì˜í–¥ë„ ì ìˆ˜ (30%)
        tdd_assessment = changes['tdd_impact_assessment']
        if tdd_assessment['flow_code_logic_intact']:
            score += 10
        if tdd_assessment['warehouse_monthly_structure_viable']:
            score += 10
        if tdd_assessment['site_monthly_structure_viable']:
            score += 10
        
        return score
    
    def _get_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if score >= 95:
            return "A+ (ìš°ìˆ˜)"
        elif score >= 90:
            return "A (ì–‘í˜¸)"
        elif score >= 80:
            return "B (ë³´í†µ)"
        elif score >= 70:
            return "C (ë¯¸í¡)"
        else:
            return "D (ë¶ˆëŸ‰)"
    
    def generate_hvdc_logi_master_integration(self, df: pd.DataFrame, changes: dict) -> dict:
        """HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹œìŠ¤í…œ ì ìš©"""
        print("\n" + "="*80)
        print("ğŸš€ HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹œìŠ¤í…œ ì ìš©")
        print("="*80)
        
        if df.empty:
            return {'error': 'No data for integration'}
        
        integration_result = {
            'containment_mode': self.mode,
            'processing_timestamp': self.timestamp,
            'warehouse_monthly_data': {},
            'site_monthly_data': {},
            'confidence_score': 0.0,
            'next_commands': []
        }
        
        try:
            # ì°½ê³ ë³„ ì›”ë³„ ë°ì´í„° ìƒì„± (ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°.md ê¸°ì¤€)
            warehouse_data = self._generate_warehouse_monthly_data(df)
            integration_result['warehouse_monthly_data'] = warehouse_data
            
            # í˜„ì¥ë³„ ì›”ë³„ ë°ì´í„° ìƒì„±
            site_data = self._generate_site_monthly_data(df)
            integration_result['site_monthly_data'] = site_data
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence_score(df, changes)
            integration_result['confidence_score'] = confidence
            
            # ë‹¤ìŒ ëª…ë ¹ì–´ ì¶”ì²œ
            next_commands = self._generate_next_commands(changes, confidence)
            integration_result['next_commands'] = next_commands
            
            print(f"âœ… HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì™„ë£Œ")
            print(f"ğŸ“Š ì‹ ë¢°ë„ ì ìˆ˜: {confidence:.1%}")
            print(f"ğŸ¯ ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ: {self.mode}")
            
        except Exception as e:
            print(f"âŒ í†µí•© ì‹œìŠ¤í…œ ì ìš© ì‹¤íŒ¨: {e}")
            integration_result['error'] = str(e)
        
        return integration_result
    
    def _generate_warehouse_monthly_data(self, df: pd.DataFrame) -> dict:
        """ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  ë°ì´í„° ìƒì„±"""
        warehouse_data = {}
        
        # ê¸°ë³¸ ì›”ë³„ ë²”ìœ„ (2023-02 ~ 2025-06)
        months = pd.date_range('2023-02', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        
        for warehouse in self.warehouse_columns:
            if warehouse in df.columns:
                # í•´ë‹¹ ì°½ê³ ì— ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì˜ ì…ê³  ê±´ìˆ˜
                inbound_count = df[warehouse].notna().sum()
                
                # ì¶œê³ ëŠ” ë‹¤ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™í•œ ê²ƒìœ¼ë¡œ ì¶”ì •
                outbound_count = max(0, inbound_count - int(inbound_count * 0.1))  # 90% ì¶œê³  ê°€ì •
                
                # ì›”ë³„ ë¶„ë°° (í˜„ì¬ëŠ” ë‹¨ìˆœ ë¶„ë°°)
                monthly_inbound = self._distribute_monthly(inbound_count, months)
                monthly_outbound = self._distribute_monthly(outbound_count, months)
                
                warehouse_data[warehouse] = {
                    'total_inbound': inbound_count,
                    'total_outbound': outbound_count,
                    'monthly_inbound': monthly_inbound,
                    'monthly_outbound': monthly_outbound
                }
        
        return warehouse_data
    
    def _generate_site_monthly_data(self, df: pd.DataFrame) -> dict:
        """í˜„ì¥ë³„ ì›”ë³„ ì…ê³ ì¬ê³  ë°ì´í„° ìƒì„±"""
        site_data = {}
        
        # ê¸°ë³¸ ì›”ë³„ ë²”ìœ„ (2024-01 ~ 2025-06)
        months = pd.date_range('2024-01', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        
        for site in self.site_columns:
            if site in df.columns:
                # í•´ë‹¹ í˜„ì¥ì— ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì˜ ì…ê³  ê±´ìˆ˜
                inbound_count = df[site].notna().sum()
                
                # ì¬ê³ ëŠ” ì…ê³ ì˜ ëˆ„ì ìœ¼ë¡œ ì¶”ì •
                inventory_count = inbound_count  # ë‹¨ìˆœí™”
                
                # ì›”ë³„ ë¶„ë°°
                monthly_inbound = self._distribute_monthly(inbound_count, months)
                monthly_inventory = self._calculate_cumulative_inventory(monthly_inbound)
                
                site_data[site] = {
                    'total_inbound': inbound_count,
                    'total_inventory': inventory_count,
                    'monthly_inbound': monthly_inbound,
                    'monthly_inventory': monthly_inventory
                }
        
        return site_data
    
    def _distribute_monthly(self, total_count: int, months: list) -> dict:
        """ì´ ê±´ìˆ˜ë¥¼ ì›”ë³„ë¡œ ë¶„ë°°"""
        if total_count == 0 or not months:
            return {month: 0 for month in months}
        
        # ë‹¨ìˆœ ê· ë“± ë¶„ë°° (í–¥í›„ ê³„ì ˆì„± ë“± ê³ ë ¤ ê°€ëŠ¥)
        base_count = total_count // len(months)
        remainder = total_count % len(months)
        
        monthly_data = {}
        for i, month in enumerate(months):
            monthly_data[month] = base_count + (1 if i < remainder else 0)
        
        return monthly_data
    
    def _calculate_cumulative_inventory(self, monthly_inbound: dict) -> dict:
        """ì›”ë³„ ëˆ„ì  ì¬ê³  ê³„ì‚°"""
        cumulative = {}
        running_total = 0
        
        for month in sorted(monthly_inbound.keys()):
            running_total += monthly_inbound[month]
            cumulative[month] = running_total
        
        return cumulative
    
    def _calculate_confidence_score(self, df: pd.DataFrame, changes: dict) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        if df.empty:
            return 0.0
        
        score = 0.0
        
        # ë°ì´í„° í’ˆì§ˆ (40%)
        quality_score = changes.get('data_quality_metrics', {}).get('overall_quality_score', 0)
        score += (quality_score / 100) * 40
        
        # ì»¬ëŸ¼ ì™„ì „ì„± (30%)
        structure_score = 100 if changes.get('column_structure_analysis', {}).get('structure_compatible', False) else 50
        score += (structure_score / 100) * 30
        
        # TDD í˜¸í™˜ì„± (30%)
        compatibility_score = changes.get('tdd_impact_assessment', {}).get('overall_compatibility_score', 0)
        score += (compatibility_score / 100) * 30
        
        return score / 100
    
    def _generate_next_commands(self, changes: dict, confidence: float) -> list:
        """ë‹¤ìŒ ì¶”ì²œ ëª…ë ¹ì–´ ìƒì„±"""
        commands = []
        
        # ìƒí™©ë³„ ì¶”ì²œ
        if changes.get('record_count_analysis', {}).get('significant_change', False):
            commands.append('/flow_code_validation [Flow Code ë¡œì§ ì¬ê²€ì¦]')
        
        if confidence >= 0.90:
            commands.append('/generate_warehouse_monthly_report [ì°½ê³  ì›”ë³„ ë¦¬í¬íŠ¸ ìƒì„±]')
            commands.append('/create_site_monthly_analysis [í˜„ì¥ ì›”ë³„ ë¶„ì„]')
        else:
            commands.append('/data_quality_improvement [ë°ì´í„° í’ˆì§ˆ ê°œì„ ]')
            commands.append('/tdd_logic_verification [TDD ë¡œì§ ê²€ì¦]')
        
        commands.append('/hvdc_logi_master_integration [ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹¤í–‰]')
        
        return commands
    
    def create_excel_report(self, df: pd.DataFrame, changes: dict, integration: dict) -> str:
        """ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°.xlsx í˜¸í™˜ Excel ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ“Š Excel ë¦¬í¬íŠ¸ ìƒì„± (ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°.xlsx í˜¸í™˜)")
        print("="*80)
        
        output_file = f"HITACHI_Updated_Analysis_Report_{self.timestamp}.xlsx"
        
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                
                # Sheet 1: ì „ì²´ ë°ì´í„° (ì›ë³¸)
                if not df.empty:
                    df.to_excel(writer, sheet_name='ì „ì²´_íŠ¸ëœì­ì…˜_ë°ì´í„°', index=False)
                
                # Sheet 2: ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³ 
                warehouse_sheet = self._create_warehouse_monthly_sheet(integration['warehouse_monthly_data'])
                warehouse_sheet.to_excel(writer, sheet_name='ì°½ê³ _ì›”ë³„_ì…ì¶œê³ ', index=False)
                
                # Sheet 3: í˜„ì¥ë³„ ì›”ë³„ ì…ê³ ì¬ê³ 
                site_sheet = self._create_site_monthly_sheet(integration['site_monthly_data'])
                site_sheet.to_excel(writer, sheet_name='í˜„ì¥_ì›”ë³„_ì…ê³ ì¬ê³ ', index=False)
                
                # Sheet 4: ë¶„ì„ ê²°ê³¼ ìš”ì•½
                analysis_summary = self._create_analysis_summary_sheet(changes, integration)
                analysis_summary.to_excel(writer, sheet_name='ë¶„ì„_ê²°ê³¼_ìš”ì•½', index=False)
                
                # Sheet 5: TDD ì˜í–¥ë„ í‰ê°€
                tdd_impact = self._create_tdd_impact_sheet(changes)
                tdd_impact.to_excel(writer, sheet_name='TDD_ì˜í–¥ë„_í‰ê°€', index=False)
            
            print(f"âœ… Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"âŒ Excel ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def _create_warehouse_monthly_sheet(self, warehouse_data: dict) -> pd.DataFrame:
        """ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  ì‹œíŠ¸ ìƒì„± (Multi-level í—¤ë”)"""
        if not warehouse_data:
            return pd.DataFrame()
        
        months = pd.date_range('2023-02', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        months.append('Total')
        
        # ì»¬ëŸ¼ í—¤ë” êµ¬ì„±
        warehouses = list(warehouse_data.keys())
        columns = ['Location']
        
        # Multi-level í—¤ë”: ì…ê³ /ì¶œê³  Ã— ì°½ê³ ëª…
        for warehouse in warehouses:
            columns.extend([f'ì…ê³ _{warehouse}', f'ì¶œê³ _{warehouse}'])
        
        # ë°ì´í„° ìƒì„±
        data = []
        for month in months:
            row = [month]
            for warehouse in warehouses:
                if month == 'Total':
                    row.extend([
                        warehouse_data[warehouse]['total_inbound'],
                        warehouse_data[warehouse]['total_outbound']
                    ])
                else:
                    inbound = warehouse_data[warehouse]['monthly_inbound'].get(month, 0)
                    outbound = warehouse_data[warehouse]['monthly_outbound'].get(month, 0)
                    row.extend([inbound, outbound])
            data.append(row)
        
        return pd.DataFrame(data, columns=columns)
    
    def _create_site_monthly_sheet(self, site_data: dict) -> pd.DataFrame:
        """í˜„ì¥ë³„ ì›”ë³„ ì…ê³ ì¬ê³  ì‹œíŠ¸ ìƒì„± (Multi-level í—¤ë”)"""
        if not site_data:
            return pd.DataFrame()
        
        months = pd.date_range('2024-01', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        months.append('í•©ê³„')
        
        # ì»¬ëŸ¼ í—¤ë” êµ¬ì„±
        sites = list(site_data.keys())
        columns = ['Location']
        
        # Multi-level í—¤ë”: ì…ê³ /ì¬ê³  Ã— í˜„ì¥ëª…
        for site in sites:
            columns.extend([f'ì…ê³ _{site}', f'ì¬ê³ _{site}'])
        
        # ë°ì´í„° ìƒì„±
        data = []
        for month in months:
            row = [month]
            for site in sites:
                if month == 'í•©ê³„':
                    row.extend([
                        site_data[site]['total_inbound'],
                        site_data[site]['total_inventory']
                    ])
                else:
                    inbound = site_data[site]['monthly_inbound'].get(month, 0)
                    inventory = site_data[site]['monthly_inventory'].get(month, 0)
                    row.extend([inbound, inventory])
            data.append(row)
        
        return pd.DataFrame(data, columns=columns)
    
    def _create_analysis_summary_sheet(self, changes: dict, integration: dict) -> pd.DataFrame:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì‹œíŠ¸ ìƒì„±"""
        summary_data = []
        
        # ê¸°ë³¸ ì •ë³´
        record_analysis = changes.get('record_count_analysis', {})
        summary_data.extend([
            {'êµ¬ë¶„': 'ë¶„ì„ ì‹œê°', 'ë‚´ìš©': self.timestamp},
            {'êµ¬ë¶„': 'ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ', 'ë‚´ìš©': self.mode},
            {'êµ¬ë¶„': 'TDD ê¸°ì¤€ ê±´ìˆ˜', 'ë‚´ìš©': f"{record_analysis.get('baseline_count', 0):,}ê±´"},
            {'êµ¬ë¶„': 'í˜„ì¬ ë°ì´í„° ê±´ìˆ˜', 'ë‚´ìš©': f"{record_analysis.get('current_count', 0):,}ê±´"},
            {'êµ¬ë¶„': 'ê±´ìˆ˜ ë³€í™”', 'ë‚´ìš©': f"{record_analysis.get('difference', 0):+,}ê±´"},
            {'êµ¬ë¶„': 'ì¤‘ëŒ€í•œ ë³€í™” ì—¬ë¶€', 'ë‚´ìš©': 'ì˜ˆ' if record_analysis.get('significant_change', False) else 'ì•„ë‹ˆì˜¤'}
        ])
        
        # í’ˆì§ˆ ì§€í‘œ
        quality_metrics = changes.get('data_quality_metrics', {})
        summary_data.extend([
            {'êµ¬ë¶„': 'ë°ì´í„° í’ˆì§ˆ ì ìˆ˜', 'ë‚´ìš©': f"{quality_metrics.get('overall_quality_score', 0):.1f}%"},
            {'êµ¬ë¶„': 'í’ˆì§ˆ ë“±ê¸‰', 'ë‚´ìš©': quality_metrics.get('quality_grade', 'N/A')},
            {'êµ¬ë¶„': 'ì‹ ë¢°ë„ ì ìˆ˜', 'ë‚´ìš©': f"{integration.get('confidence_score', 0):.1%}"}
        ])
        
        # TDD ì˜í–¥ë„
        tdd_impact = changes.get('tdd_impact_assessment', {})
        summary_data.extend([
            {'êµ¬ë¶„': 'Flow Code ë¡œì§ ìœ ì§€', 'ë‚´ìš©': 'ê°€ëŠ¥' if tdd_impact.get('flow_code_logic_intact', False) else 'ë¶ˆê°€ëŠ¥'},
            {'êµ¬ë¶„': 'ì°½ê³  ì›”ë³„ ì‹œíŠ¸ ìƒì„±', 'ë‚´ìš©': 'ê°€ëŠ¥' if tdd_impact.get('warehouse_monthly_structure_viable', False) else 'ë¶ˆê°€ëŠ¥'},
            {'êµ¬ë¶„': 'í˜„ì¥ ì›”ë³„ ì‹œíŠ¸ ìƒì„±', 'ë‚´ìš©': 'ê°€ëŠ¥' if tdd_impact.get('site_monthly_structure_viable', False) else 'ë¶ˆê°€ëŠ¥'},
            {'êµ¬ë¶„': 'ì „ì²´ í˜¸í™˜ì„± ì ìˆ˜', 'ë‚´ìš©': f"{tdd_impact.get('overall_compatibility_score', 0):.1f}%"}
        ])
        
        return pd.DataFrame(summary_data)
    
    def _create_tdd_impact_sheet(self, changes: dict) -> pd.DataFrame:
        """TDD ì˜í–¥ë„ í‰ê°€ ì‹œíŠ¸ ìƒì„±"""
        impact_data = []
        
        # Flow Code ê²€ì¦ ê²°ê³¼
        flow_validation = changes.get('flow_code_validation', {})
        if flow_validation:
            distribution = flow_validation.get('distribution', {})
            for code, count in distribution.items():
                impact_data.append({
                    'Flow Code': code,
                    'í˜„ì¬ ê±´ìˆ˜': count,
                    'ë¹„ìœ¨': f"{count/sum(distribution.values())*100:.1f}%" if distribution else "0%",
                    'ìƒíƒœ': 'ì •ìƒ' if code in [0, 1, 2, 3, 4] else 'ë¹„ì •ìƒ'
                })
            
            # FLOW CODE 2 íŠ¹ë³„ ë¶„ì„
            flow_2_current = flow_validation.get('flow_code_2_current', 0)
            flow_2_target = flow_validation.get('flow_code_2_target', 0)
            flow_2_accuracy = flow_validation.get('flow_code_2_accuracy', 0)
            
            impact_data.append({
                'Flow Code': 'FLOW CODE 2 (íŠ¹ë³„)',
                'í˜„ì¬ ê±´ìˆ˜': flow_2_current,
                'ëª©í‘œ ê±´ìˆ˜': flow_2_target,
                'ì •í™•ë„': f"{flow_2_accuracy:.1%}",
                '100% ë‹¬ì„± ìœ ì§€': 'ì˜ˆ' if flow_validation.get('maintains_100_percent_achievement', False) else 'ì•„ë‹ˆì˜¤'
            })
        
        if not impact_data:
            impact_data.append({
                'Flow Code': 'N/A',
                'í˜„ì¬ ê±´ìˆ˜': 0,
                'ìƒíƒœ': 'Flow Code ì •ë³´ ì—†ìŒ'
            })
        
        return pd.DataFrame(impact_data)
    
    def run_comprehensive_analysis(self) -> dict:
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ MACHO-GPT v3.4-mini | ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ì¢…í•© ë¶„ì„")
        print("ğŸ¯ ì°¸ì¡° ë¬¸ì„œ ê¸°ë°˜ TDD ì‹œìŠ¤í…œ ë¡œì§ ë³´ì • ì˜í–¥ ë¶„ì„")
        print("Samsung C&T Ã— ADNOC DSV Partnership | HVDC í”„ë¡œì íŠ¸")
        print("="*80)
        
        final_result = {
            'analysis_timestamp': self.timestamp,
            'containment_mode': self.mode,
            'status': 'SUCCESS',
            'data_changes': {},
            'hvdc_integration': {},
            'excel_report_file': '',
            'recommendations': []
        }
        
        try:
            # 1. ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ë¡œë“œ
            df = self.load_updated_hitachi_data()
            
            if df.empty:
                print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤")
                final_result['status'] = 'FAILED'
                final_result['error'] = 'No data loaded'
                return final_result
            
            # 2. ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„
            changes = self.analyze_data_changes(df)
            final_result['data_changes'] = changes
            
            # 3. HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹œìŠ¤í…œ ì ìš©
            integration = self.generate_hvdc_logi_master_integration(df, changes)
            final_result['hvdc_integration'] = integration
            
            # 4. Excel ë¦¬í¬íŠ¸ ìƒì„±
            excel_file = self.create_excel_report(df, changes, integration)
            final_result['excel_report_file'] = excel_file
            
            # 5. ìµœì¢… ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_final_recommendations(changes, integration)
            final_result['recommendations'] = recommendations
            
            # 6. ìµœì¢… ê²°ê³¼ ìš”ì•½
            self._print_final_summary(final_result)
            
            return final_result
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {e}")
            final_result['status'] = 'FAILED'
            final_result['error'] = str(e)
            return final_result
    
    def _generate_final_recommendations(self, changes: dict, integration: dict) -> list:
        """ìµœì¢… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë°ì´í„° ë³€í™” ê¸°ë°˜ ì¶”ì²œ
        record_change = changes.get('record_count_analysis', {}).get('significant_change', False)
        confidence = integration.get('confidence_score', 0)
        
        if record_change:
            recommendations.append({
                'priority': 'HIGH',
                'category': 'TDD_LOGIC_VERIFICATION',
                'title': 'TDD ì‹œìŠ¤í…œ ë¡œì§ ì¬ê²€ì¦',
                'description': 'ë°ì´í„° ê±´ìˆ˜ ë³€í™”ë¡œ ì¸í•œ Flow Code ë¡œì§ ì˜í–¥ë„ ì¬í‰ê°€ í•„ìš”',
                'action': 'Flow Code 0, 1, 2, 3 ë¡œì§ ëª¨ë‘ ì¬ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰'
            })
        
        if confidence < 0.90:
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'DATA_QUALITY_IMPROVEMENT',
                'title': 'ë°ì´í„° í’ˆì§ˆ ê°œì„ ',
                'description': f'í˜„ì¬ ì‹ ë¢°ë„ {confidence:.1%}, 90% ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ í’ˆì§ˆ ê°œì„ ',
                'action': 'ëˆ„ë½ ë°ì´í„° ë³´ì™„, ì¤‘ë³µ ì œê±°, ì»¬ëŸ¼ êµ¬ì¡° ì •ê·œí™”'
            })
        
        # ì›”ë³„ ì‹œíŠ¸ êµ¬ì¡° ì¶”ì²œ
        warehouse_viable = changes.get('tdd_impact_assessment', {}).get('warehouse_monthly_structure_viable', False)
        site_viable = changes.get('tdd_impact_assessment', {}).get('site_monthly_structure_viable', False)
        
        if warehouse_viable and site_viable:
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'MONTHLY_STRUCTURE_GENERATION',
                'title': 'ì°½ê³ /í˜„ì¥ ì›”ë³„ ì‹œíŠ¸ êµ¬ì¡° ìƒì„±',
                'description': 'ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ë¡œ ì›”ë³„ ì‹œíŠ¸ êµ¬ì¡° ì¬ìƒì„± ê°€ëŠ¥',
                'action': 'hvdc_logi_master_integrated.py ì‹œìŠ¤í…œì„ í™œìš©í•œ ì›”ë³„ ë¦¬í¬íŠ¸ ìƒì„±'
            })
        
        # FLOW CODE 2 100% ë‹¬ì„± ìœ ì§€ ì¶”ì²œ
        flow_2_maintains = changes.get('flow_code_validation', {}).get('maintains_100_percent_achievement', False)
        if flow_2_maintains:
            recommendations.append({
                'priority': 'LOW',
                'category': 'PERFORMANCE_OPTIMIZATION',
                'title': 'FLOW CODE 2 ë¡œì§ 100% ë‹¬ì„± ìƒíƒœ ìœ ì§€',
                'description': 'TDD ë³´ê³ ì„œì˜ FLOW CODE 2 ë¡œì§ 100% ë‹¬ì„± ìƒíƒœ ì§€ì† ê°€ëŠ¥',
                'action': 'ê¸°ì¡´ ë¡œì§ ìœ ì§€ ë° ì„±ëŠ¥ ìµœì í™”ì— ì§‘ì¤‘'
            })
        
        return recommendations
    
    def _print_final_summary(self, result: dict):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*100)
        print("ğŸ† HITACHI ë°ì´í„° ì—…ë°ì´íŠ¸ ë¶„ì„ ì™„ë£Œ")
        print("="*100)
        
        changes = result.get('data_changes', {})
        integration = result.get('hvdc_integration', {})
        
        # í•µì‹¬ ì§€í‘œ
        record_change = changes.get('record_count_analysis', {})
        confidence = integration.get('confidence_score', 0)
        
        print(f"ğŸ“Š í•µì‹¬ ë¶„ì„ ê²°ê³¼:")
        print(f"   - ë¶„ì„ ìƒíƒœ: {result['status']}")
        print(f"   - ë°ì´í„° ê±´ìˆ˜ ë³€í™”: {record_change.get('difference', 0):+,}ê±´")
        print(f"   - ì¤‘ëŒ€í•œ ë³€í™”: {'ğŸš¨ ì˜ˆ' if record_change.get('significant_change', False) else 'âœ… ì•„ë‹ˆì˜¤'}")
        print(f"   - ì‹ ë¢°ë„ ì ìˆ˜: {confidence:.1%}")
        print(f"   - Excel ë¦¬í¬íŠ¸: {result.get('excel_report_file', 'N/A')}")
        
        # TDD ì˜í–¥ë„
        tdd_impact = changes.get('tdd_impact_assessment', {})
        print(f"\nğŸ¯ TDD ì‹œìŠ¤í…œ ë¡œì§ ë³´ì • ì˜í–¥:")
        print(f"   - Flow Code ë¡œì§ ìœ ì§€: {'âœ…' if tdd_impact.get('flow_code_logic_intact', False) else 'âŒ'}")
        print(f"   - ì°½ê³  ì›”ë³„ ì‹œíŠ¸ ìƒì„±: {'âœ…' if tdd_impact.get('warehouse_monthly_structure_viable', False) else 'âŒ'}")
        print(f"   - í˜„ì¥ ì›”ë³„ ì‹œíŠ¸ ìƒì„±: {'âœ…' if tdd_impact.get('site_monthly_structure_viable', False) else 'âŒ'}")
        print(f"   - ì „ì²´ í˜¸í™˜ì„±: {tdd_impact.get('overall_compatibility_score', 0):.1f}%")
        
        # ì¶”ì²œì‚¬í•­
        recommendations = result.get('recommendations', [])
        high_priority = [r for r in recommendations if r.get('priority') == 'HIGH']
        
        print(f"\nğŸ“‹ ì¶”ì²œì‚¬í•­:")
        if high_priority:
            print(f"   ğŸš¨ ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”: {len(high_priority)}ê°œ")
            for rec in high_priority:
                print(f"      - {rec['title']}")
        else:
            print(f"   âœ… ê¸´ê¸‰ ì¡°ì¹˜ ë¶ˆí•„ìš”")
        
        print(f"   ğŸ“Š ì´ ì¶”ì²œì‚¬í•­: {len(recommendations)}ê°œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”Œ MACHO-GPT v3.4-mini ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ")
    print("Enhanced MCP Integration | Samsung C&T Logistics")
    print("TDD ë°©ë²•ë¡  ê¸°ë°˜ | Kent Beck's Red-Green-Refactor")
    print("="*80)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    analyzer = UpdatedHitachiAnalyzer()
    final_result = analyzer.run_comprehensive_analysis()
    
    # ì¢…ë£Œ ì½”ë“œ ê²°ì •
    if final_result['status'] == 'FAILED':
        exit_code = 2  # ì˜¤ë¥˜
    elif final_result.get('data_changes', {}).get('record_count_analysis', {}).get('significant_change', False):
        exit_code = 1  # ì¤‘ëŒ€í•œ ë³€í™” - ì£¼ì˜ í•„ìš”
    else:
        exit_code = 0  # ì •ìƒ
    
    print(f"\nğŸ ë¶„ì„ ì™„ë£Œ (ì¢…ë£Œ ì½”ë“œ: {exit_code})")
    return exit_code

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 
#!/usr/bin/env python3
"""
HVDC ì»¬ëŸ¼ ë³µêµ¬ ë„êµ¬ v2.8.1
Author: MACHO-GPT v3.4-mini â”‚ Samsung C&T Logistics
Purpose: Location/Status ì»¬ëŸ¼ ìžë™ ìƒì„± ë° ë°ì´í„° ë³´ê°•
"""

import pandas as pd
import numpy as np
import re
from pathlib import Path
import logging
from typing import Dict, List, Optional
import json

logger = logging.getLogger(__name__)

class ColumnRepairTool:
    """ì»¬ëŸ¼ ë³µêµ¬ ë„êµ¬"""
    
    def __init__(self):
        # ì»¬ëŸ¼ ë§¤í•‘ ê·œì¹™
        self.column_mappings = {
            'Location': [
                'Status_Location', 'Current_Location', 'Warehouse', 
                'Storage_Location', 'Position', 'Site'
            ],
            'Status': [
                'Status_Current', 'Current_Status', 'Item_Status',
                'Delivery_Status', 'State', 'Condition'
            ],
            'Case_No': [
                'Case_Number', 'CaseNo', 'Case ID', 'ID', 'Item_ID'
            ]
        }
        
        # ê¸°ë³¸ê°’ ê·œì¹™
        self.default_values = {
            'Status': 'Active',
            'Location': 'Unknown'
        }
        
        # ìœ„ì¹˜ ì •ê·œí™” ê·œì¹™
        self.location_normalization = {
            'DSV INDOOR': ['DSV Indoor', 'DSV_Indoor', 'Indoor', 'DSV-Indoor'],
            'DSV OUTDOOR': ['DSV Outdoor', 'DSV_Outdoor', 'Outdoor', 'DSV-Outdoor'],
            'DSV AL MARKAZ': ['DSV Al Markaz', 'Al Markaz', 'Markaz', 'DSV-Markaz'],
            'MOSB': ['MARINE BASE', 'Offshore Base', 'Marine Offshore', 'MOSB'],
            'PRE ARRIVAL': ['Pre Arrival', 'PRE_ARRIVAL', 'Not Received', 'Pending'],
            'AGI': ['AGI Site', 'AGI_Site', 'AGI Plant'],
            'DAS': ['DAS Site', 'DAS_Site', 'DAS Plant'],
            'MIR': ['MIR Site', 'MIR_Site', 'MIR Plant'],
            'SHU': ['SHU Site', 'SHU_Site', 'SHU Plant']
        }
    
    def analyze_dataframe_structure(self, df: pd.DataFrame) -> Dict:
        """DataFrame êµ¬ì¡° ë¶„ì„"""
        analysis = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'columns': list(df.columns),
            'missing_columns': [],
            'similar_columns': {},
            'data_quality': {}
        }
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['Location', 'Status', 'Case_No']
        for req_col in required_columns:
            if req_col not in df.columns:
                analysis['missing_columns'].append(req_col)
                
                # ìœ ì‚¬í•œ ì»¬ëŸ¼ ì°¾ê¸°
                similar = self._find_similar_columns(req_col, df.columns)
                if similar:
                    analysis['similar_columns'][req_col] = similar
        
        # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        for col in df.columns:
            null_count = df[col].isnull().sum()
            null_pct = (null_count / len(df)) * 100
            analysis['data_quality'][col] = {
                'null_count': null_count,
                'null_percentage': null_pct,
                'data_type': str(df[col].dtype),
                'unique_values': df[col].nunique()
            }
        
        return analysis
    
    def _find_similar_columns(self, target_col: str, available_cols: List[str]) -> List[str]:
        """ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°"""
        similar = []
        
        # ì •í™•í•œ ë§¤í•‘ í™•ì¸
        if target_col in self.column_mappings:
            for mapping in self.column_mappings[target_col]:
                for col in available_cols:
                    if mapping.lower() in col.lower() or col.lower() in mapping.lower():
                        similar.append(col)
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
        keywords = {
            'Location': ['location', 'position', 'warehouse', 'site', 'place'],
            'Status': ['status', 'state', 'condition', 'delivery'],
            'Case_No': ['case', 'id', 'number', 'no']
        }
        
        if target_col in keywords:
            for keyword in keywords[target_col]:
                for col in available_cols:
                    if keyword in col.lower() and col not in similar:
                        similar.append(col)
        
        return similar
    
    def repair_missing_columns(self, df: pd.DataFrame, auto_fix: bool = True) -> pd.DataFrame:
        """
        ëˆ„ë½ëœ ì»¬ëŸ¼ ë³µêµ¬
        v2.8.2 í•«í”½ìŠ¤: ì „ê°ê³µë°± ì²˜ë¦¬ ì¶”ê°€
        """
        logger.info("ðŸ”§ ì»¬ëŸ¼ ë³µêµ¬ ì‹œìž‘...")
        
        df_repaired = df.copy()
        repair_log = []
        
        # â˜… v2.8.2 í•«í”½ìŠ¤: ì»¬ëŸ¼ í—¤ë” ì „ê°ê³µë°± ì •ë¦¬
        df_repaired.columns = [str(col).replace('\u3000', ' ').strip() for col in df_repaired.columns]
        repair_log.append("ðŸ”§ ì»¬ëŸ¼ í—¤ë” ì „ê°ê³µë°± ì •ë¦¬ ì™„ë£Œ")
        
        # êµ¬ì¡° ë¶„ì„
        analysis = self.analyze_dataframe_structure(df)
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬
        for missing_col in analysis['missing_columns']:
            logger.info(f"   ëˆ„ë½ ì»¬ëŸ¼ ì²˜ë¦¬: {missing_col}")
            
            # ìœ ì‚¬í•œ ì»¬ëŸ¼ì´ ìžˆëŠ” ê²½ìš° ë§¤í•‘
            if missing_col in analysis['similar_columns']:
                similar_cols = analysis['similar_columns'][missing_col]
                if similar_cols and auto_fix:
                    source_col = similar_cols[0]  # ì²« ë²ˆì§¸ ìœ ì‚¬ ì»¬ëŸ¼ ì‚¬ìš©
                    df_repaired[missing_col] = df_repaired[source_col]
                    repair_log.append(f"âœ… {missing_col} â† {source_col} (ë§¤í•‘)")
                    logger.info(f"      ë§¤í•‘: {source_col} â†’ {missing_col}")
                else:
                    # ìˆ˜ë™ í™•ì¸ í•„ìš”
                    repair_log.append(f"âš ï¸ {missing_col} ìœ ì‚¬ ì»¬ëŸ¼: {similar_cols} (ìˆ˜ë™ í™•ì¸ í•„ìš”)")
            
            # ìœ ì‚¬í•œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
            if missing_col not in df_repaired.columns:
                if missing_col in self.default_values:
                    df_repaired[missing_col] = self.default_values[missing_col]
                    repair_log.append(f"ðŸ”§ {missing_col} ê¸°ë³¸ê°’ ìƒì„±: {self.default_values[missing_col]}")
                elif missing_col == 'Case_No':
                    # Case_No ìžë™ ìƒì„±
                    df_repaired['Case_No'] = [f'HE{i+1:04d}' for i in range(len(df_repaired))]
                    repair_log.append(f"ðŸ”§ Case_No ìžë™ ìƒì„±: HE0001~HE{len(df_repaired):04d}")
        
        # ìœ„ì¹˜ ë°ì´í„° ì •ê·œí™”
        if 'Location' in df_repaired.columns:
            df_repaired = self._normalize_locations(df_repaired)
            repair_log.append("ðŸ”§ Location ë°ì´í„° ì •ê·œí™” ì™„ë£Œ")
        
        # ë³µêµ¬ ê²°ê³¼ ë¡œê·¸
        logger.info("âœ… ì»¬ëŸ¼ ë³µêµ¬ ì™„ë£Œ")
        for log_entry in repair_log:
            logger.info(f"   {log_entry}")
        
        return df_repaired
    
    def _normalize_locations(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìœ„ì¹˜ ë°ì´í„° ì •ê·œí™”"""
        df_normalized = df.copy()
        
        # ì •ê·œí™” ë§¤í•‘ ì ìš©
        location_map = {}
        for standard, variations in self.location_normalization.items():
            for variation in variations:
                location_map[variation.upper()] = standard
        
        # Location ì»¬ëŸ¼ ì •ê·œí™”
        def normalize_location(location):
            if pd.isna(location):
                return 'Unknown'
            
            location_str = str(location).strip().upper()
            
            # ì •í™•í•œ ë§¤ì¹­
            if location_str in location_map:
                return location_map[location_str]
            
            # ë¶€ë¶„ ë§¤ì¹­
            for variation, standard in location_map.items():
                if variation in location_str:
                    return standard
            
            return location  # ì›ë³¸ ë°˜í™˜
        
        df_normalized['Location'] = df_normalized['Location'].apply(normalize_location)
        
        return df_normalized
    
    def generate_missing_status_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Status ë°ì´í„° ë³´ê°•"""
        df_enhanced = df.copy()
        
        # Location ê¸°ë°˜ Status ì¶”ë¡ 
        def infer_status(row):
            location = str(row.get('Location', '')).upper()
            
            if 'PRE ARRIVAL' in location or 'PENDING' in location:
                return 'PRE ARRIVAL'
            elif any(site in location for site in ['AGI', 'DAS', 'MIR', 'SHU']):
                return 'DELIVERED'
            elif any(wh in location for wh in ['DSV', 'WAREHOUSE']):
                return 'IN WAREHOUSE'
            elif 'MOSB' in location or 'OFFSHORE' in location:
                return 'AT OFFSHORE BASE'
            else:
                return 'ACTIVE'
        
        # Statusê°€ ì—†ê±°ë‚˜ Unknownì¸ ê²½ìš° ì¶”ë¡ 
        mask = df_enhanced['Status'].isnull() | (df_enhanced['Status'] == 'Unknown')
        df_enhanced.loc[mask, 'Status'] = df_enhanced.loc[mask].apply(infer_status, axis=1)
        
        return df_enhanced
    
    def validate_repaired_data(self, df: pd.DataFrame) -> Dict:
        """ë³µêµ¬ëœ ë°ì´í„° ê²€ì¦"""
        validation = {
            'total_rows': len(df),
            'required_columns_present': True,
            'data_completeness': {},
            'location_distribution': {},
            'status_distribution': {},
            'issues': []
        }
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['Location', 'Status', 'Case_No']
        for col in required_columns:
            if col not in df.columns:
                validation['required_columns_present'] = False
                validation['issues'].append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
        
        # ë°ì´í„° ì™„ì„±ë„ í™•ì¸
        for col in df.columns:
            null_count = df[col].isnull().sum()
            null_pct = (null_count / len(df)) * 100
            validation['data_completeness'][col] = {
                'null_count': null_count,
                'null_percentage': null_pct
            }
            
            if null_pct > 50:
                validation['issues'].append(f"ë†’ì€ ê²°ì¸¡ë¥ : {col} ({null_pct:.1f}%)")
        
        # ë¶„í¬ í™•ì¸
        if 'Location' in df.columns:
            validation['location_distribution'] = df['Location'].value_counts().to_dict()
        
        if 'Status' in df.columns:
            validation['status_distribution'] = df['Status'].value_counts().to_dict()
        
        return validation
    
    def repair_excel_file(self, input_path: str, output_path: str = None, auto_fix: bool = True) -> Dict:
        """Excel íŒŒì¼ ë³µêµ¬"""
        logger.info(f"ðŸ“ Excel íŒŒì¼ ë³µêµ¬ ì‹œìž‘: {input_path}")
        
        try:
            # Excel íŒŒì¼ ë¡œë“œ
            df = pd.read_excel(input_path)
            logger.info(f"   ì›ë³¸ ë°ì´í„°: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
            
            # ì»¬ëŸ¼ ë³µêµ¬
            df_repaired = self.repair_missing_columns(df, auto_fix)
            
            # Status ë°ì´í„° ë³´ê°•
            df_repaired = self.generate_missing_status_data(df_repaired)
            
            # ê²€ì¦
            validation = self.validate_repaired_data(df_repaired)
            
            # ê²°ê³¼ ì €ìž¥
            if output_path:
                df_repaired.to_excel(output_path, index=False)
                logger.info(f"   ë³µêµ¬ëœ íŒŒì¼ ì €ìž¥: {output_path}")
            
            return {
                'original_df': df,
                'repaired_df': df_repaired,
                'validation': validation,
                'success': len(validation['issues']) == 0
            }
            
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tool = ColumnRepairTool()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = {
        'Case_Number': ['HE0001', 'HE0002', 'HE0003', 'HE0004', 'HE0005'],
        'Status_Location': ['DSV Indoor', 'AGI Site', 'Pre Arrival', 'MOSB', 'DSV Outdoor'],
        'Current_Status': ['Active', 'Delivered', 'Pending', 'Active', 'Active'],
        'Qty': [10, 5, 8, 3, 12],
        'Amount': [50000, 25000, 40000, 15000, 60000]
    }
    
    df_test = pd.DataFrame(test_data)
    logger.info("ðŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
    logger.info(f"   ì»¬ëŸ¼: {list(df_test.columns)}")
    
    # ì»¬ëŸ¼ ë³µêµ¬ í…ŒìŠ¤íŠ¸
    df_repaired = tool.repair_missing_columns(df_test)
    
    logger.info("âœ… ë³µêµ¬ ê²°ê³¼:")
    logger.info(f"   ë³µêµ¬ í›„ ì»¬ëŸ¼: {list(df_repaired.columns)}")
    
    # ê²€ì¦
    validation = tool.validate_repaired_data(df_repaired)
    logger.info(f"   ê²€ì¦ ê²°ê³¼: {'ì„±ê³µ' if len(validation['issues']) == 0 else 'ë¬¸ì œ ë°œê²¬'}")
    
    if validation['issues']:
        for issue in validation['issues']:
            logger.warning(f"   âš ï¸ {issue}")
    
    # ì¶”ì²œ ëª…ë ¹ì–´
    logger.info("\nðŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
    logger.info("/logi_master repair_columns --fast [í•„ìˆ˜ ì»¬ëŸ¼ ìžë™ ìƒì„±]")
    logger.info("/logi_master flow-kpi --deep [Code 0-4 ë¶„í¬ & ìœ„í—˜ ë¦¬ìŠ¤íŠ¸ ë³´ê³ ]")
    logger.info("/automate_workflow mosb-ageing-guard [MOSB ì²´ë¥˜ > 30d ê²½ê³ ]")

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    main() 
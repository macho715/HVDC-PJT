#!/usr/bin/env python3
"""
HVDC ì‹¤ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ v2.8.1
Author: MACHO-GPT v3.4-mini â”‚ Samsung C&T Logistics
Purpose: ì‹¤ì œ Excel ë°ì´í„°ë¡œ Flow Code ê°­ ë¶„ì„ ê²€ì¦
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
import json
from typing import Dict, List, Tuple
import sys
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append('../hvdc_macho_gpt/WAREHOUSE')
try:
    from mapping_utils import MappingManager, calc_flow_code, add_logistics_flow_code_to_dataframe
    from calc_flow_code_v2 import FlowCodeCalculatorV2, add_flow_code_v2_to_dataframe
    from repair_columns_tool import ColumnRepairTool
except ImportError as e:
    logger.warning(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

class RealDataValidator:
    """ì‹¤ë°ì´í„° ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.data_paths = {
            'hvdc_status': '../hvdc_macho_gpt/WAREHOUSE/data/HVDC-STATUS.xlsx',
            'hitachi': '../hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_HITACHI(HE).xlsx',
            'simense': '../hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx',
            'invoice': '../hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_INVOICE.xlsx'
        }
        
        # ë³´ê³ ì„œ ê¸°ì¤€ ë¶„í¬
        self.report_distribution = {
            0: 163,    # Pre Arrival
            1: 3593,   # Portâ†’Site
            2: 1183,   # Portâ†’WHâ†’Site
            3: 402,    # Portâ†’WHâ†’MOSBâ†’Site
            4: 5       # Portâ†’WHâ†’whâ†’MOSBâ†’Site
        }
        
        self.total_expected = sum(self.report_distribution.values())  # 5,346
        
    def load_excel_files(self) -> Dict[str, pd.DataFrame]:
        """ëª¨ë“  Excel íŒŒì¼ ë¡œë“œ"""
        dataframes = {}
        total_rows = 0
        
        logger.info("ğŸ“ ì‹¤ì œ Excel íŒŒì¼ ë¡œë“œ ì‹œì‘...")
        
        for name, path in self.data_paths.items():
            if Path(path).exists():
                try:
                    df = pd.read_excel(path)
                    dataframes[name] = df
                    total_rows += len(df)
                    logger.info(f"âœ… {name}: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
                    logger.info(f"   ì»¬ëŸ¼: {list(df.columns)[:5]}...")  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ
                except Exception as e:
                    logger.error(f"âŒ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                logger.warning(f"âš ï¸ {name} íŒŒì¼ ì—†ìŒ: {path}")
        
        logger.info(f"ğŸ“Š ì´ ë¡œë“œëœ ë°ì´í„°: {total_rows}í–‰ ({len(dataframes)}ê°œ íŒŒì¼)")
        return dataframes
    
    def analyze_data_structure(self, dataframes: Dict[str, pd.DataFrame]) -> Dict:
        """ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        analysis = {
            'total_files': len(dataframes),
            'total_rows': sum(len(df) for df in dataframes.values()),
            'file_details': {},
            'column_analysis': {},
            'potential_issues': []
        }
        
        logger.info("ğŸ” ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
        
        # ê° íŒŒì¼ë³„ ìƒì„¸ ë¶„ì„
        for name, df in dataframes.items():
            file_analysis = {
                'rows': len(df),
                'columns': len(df.columns),
                'column_list': list(df.columns),
                'has_location': any('location' in col.lower() for col in df.columns),
                'has_status': any('status' in col.lower() for col in df.columns),
                'has_case_no': any('case' in col.lower() or 'id' in col.lower() for col in df.columns),
                'potential_wh_cols': [col for col in df.columns if any(wh in col.upper() for wh in ['DSV', 'INDOOR', 'OUTDOOR', 'WAREHOUSE'])],
                'potential_mosb_cols': [col for col in df.columns if any(mosb in col.upper() for mosb in ['MOSB', 'OFFSHORE', 'MARINE'])],
                'date_cols': [col for col in df.columns if 'date' in col.lower() or df[col].dtype in ['datetime64[ns]', 'object']]
            }
            
            analysis['file_details'][name] = file_analysis
            
            # ì ì¬ì  ë¬¸ì œì  ì‹ë³„
            if not file_analysis['has_location']:
                analysis['potential_issues'].append(f"{name}: Location ì»¬ëŸ¼ ë¶€ì¬")
            if not file_analysis['has_status']:
                analysis['potential_issues'].append(f"{name}: Status ì»¬ëŸ¼ ë¶€ì¬")
            if file_analysis['potential_wh_cols']:
                analysis['potential_issues'].append(f"{name}: WH ê´€ë ¨ ì»¬ëŸ¼ ë°œê²¬ - {file_analysis['potential_wh_cols'][:3]}")
            if file_analysis['potential_mosb_cols']:
                analysis['potential_issues'].append(f"{name}: MOSB ê´€ë ¨ ì»¬ëŸ¼ ë°œê²¬ - {file_analysis['potential_mosb_cols']}")
        
        return analysis
    
    def test_current_v28_algorithm(self, dataframes: Dict[str, pd.DataFrame]) -> Dict:
        """í˜„ì¬ v2.8 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª í˜„ì¬ v2.8 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {}
        total_flow_distribution = {i: 0 for i in range(5)}
        
        try:
            manager = MappingManager()
            
            for name, df in dataframes.items():
                logger.info(f"   ğŸ“Š {name} ì²˜ë¦¬ ì¤‘...")
                
                # ì»¬ëŸ¼ ë³µêµ¬ ì‹œë„
                repair_tool = ColumnRepairTool()
                df_repaired = repair_tool.repair_missing_columns(df.copy())
                
                # Storage Type ì¶”ê°€
                df_with_storage = manager.add_storage_type_to_dataframe(df_repaired)
                
                # Flow Code ì¶”ê°€ (ê¸°ì¡´ v2.8)
                df_complete = add_logistics_flow_code_to_dataframe(df_with_storage)
                
                # ë¶„í¬ ê³„ì‚°
                flow_distribution = df_complete['Logistics_Flow_Code'].value_counts().to_dict()
                normalized_flow = {i: flow_distribution.get(i, 0) for i in range(5)}
                
                results[name] = {
                    'original_rows': len(df),
                    'processed_rows': len(df_complete),
                    'flow_distribution': normalized_flow,
                    'storage_distribution': df_complete['Storage_Type'].value_counts().to_dict()
                }
                
                # ì „ì²´ ë¶„í¬ì— í•©ì‚°
                for code, count in normalized_flow.items():
                    total_flow_distribution[code] += count
                
                logger.info(f"      Flow ë¶„í¬: {normalized_flow}")
        
        except Exception as e:
            logger.error(f"âŒ v2.8 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
        
        return {
            'file_results': results,
            'total_distribution': total_flow_distribution,
            'total_rows': sum(res['processed_rows'] for res in results.values())
        }
    
    def test_improved_v281_algorithm(self, dataframes: Dict[str, pd.DataFrame]) -> Dict:
        """ê°œì„ ëœ v2.8.1 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸš€ ê°œì„ ëœ v2.8.1 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {}
        total_flow_distribution = {i: 0 for i in range(5)}
        
        try:
            calculator = FlowCodeCalculatorV2()
            
            for name, df in dataframes.items():
                logger.info(f"   ğŸ“Š {name} ì²˜ë¦¬ ì¤‘...")
                
                # ì»¬ëŸ¼ ë³µêµ¬
                repair_tool = ColumnRepairTool()
                df_repaired = repair_tool.repair_missing_columns(df.copy())
                
                # Flow Code v2 ì¶”ê°€
                df_complete = add_flow_code_v2_to_dataframe(df_repaired)
                
                # ë¶„í¬ ê³„ì‚°
                flow_distribution = df_complete['Logistics_Flow_Code_V2'].value_counts().to_dict()
                normalized_flow = {i: flow_distribution.get(i, 0) for i in range(5)}
                
                results[name] = {
                    'original_rows': len(df),
                    'processed_rows': len(df_complete),
                    'flow_distribution': normalized_flow,
                    'avg_confidence': df_complete['Flow_Confidence'].mean()
                }
                
                # ì „ì²´ ë¶„í¬ì— í•©ì‚°
                for code, count in normalized_flow.items():
                    total_flow_distribution[code] += count
                
                logger.info(f"      Flow ë¶„í¬: {normalized_flow}")
                logger.info(f"      í‰ê·  ì‹ ë¢°ë„: {results[name]['avg_confidence']:.3f}")
        
        except Exception as e:
            logger.error(f"âŒ v2.8.1 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
        
        return {
            'file_results': results,
            'total_distribution': total_flow_distribution,
            'total_rows': sum(res['processed_rows'] for res in results.values())
        }
    
    def calculate_gaps(self, v28_result: Dict, v281_result: Dict) -> Dict:
        """ê°­ ë¶„ì„ ê³„ì‚°"""
        logger.info("ğŸ“Š ê°­ ë¶„ì„ ê³„ì‚° ì‹œì‘...")
        
        gaps = {
            'report_vs_v28': {},
            'report_vs_v281': {},
            'v28_vs_v281': {},
            'summary': {}
        }
        
        # ë³´ê³ ì„œ vs v2.8
        v28_total = v28_result.get('total_distribution', {})
        for code in range(5):
            report_count = self.report_distribution[code]
            v28_count = v28_total.get(code, 0)
            gap = v28_count - report_count
            
            gaps['report_vs_v28'][code] = {
                'report': report_count,
                'v28': v28_count,
                'gap': gap,
                'gap_pct': (gap / report_count * 100) if report_count > 0 else 0
            }
        
        # ë³´ê³ ì„œ vs v2.8.1
        v281_total = v281_result.get('total_distribution', {})
        for code in range(5):
            report_count = self.report_distribution[code]
            v281_count = v281_total.get(code, 0)
            gap = v281_count - report_count
            
            gaps['report_vs_v281'][code] = {
                'report': report_count,
                'v281': v281_count,
                'gap': gap,
                'gap_pct': (gap / report_count * 100) if report_count > 0 else 0
            }
        
        # v2.8 vs v2.8.1
        for code in range(5):
            v28_count = v28_total.get(code, 0)
            v281_count = v281_total.get(code, 0)
            improvement = v281_count - v28_count
            
            gaps['v28_vs_v281'][code] = {
                'v28': v28_count,
                'v281': v281_count,
                'improvement': improvement
            }
        
        # ìš”ì•½ í†µê³„
        gaps['summary'] = {
            'total_rows_v28': v28_result.get('total_rows', 0),
            'total_rows_v281': v281_result.get('total_rows', 0),
            'expected_total': self.total_expected,
            'v28_accuracy': self._calculate_accuracy(v28_total),
            'v281_accuracy': self._calculate_accuracy(v281_total)
        }
        
        return gaps
    
    def _calculate_accuracy(self, distribution: Dict) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        total_error = 0
        total_expected = 0
        
        for code in range(5):
            expected = self.report_distribution[code]
            actual = distribution.get(code, 0)
            total_error += abs(actual - expected)
            total_expected += expected
        
        return max(0, 1 - (total_error / total_expected)) if total_expected > 0 else 0
    
    def generate_validation_report(self, analysis: Dict, gaps: Dict) -> str:
        """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("# HVDC ì‹¤ë°ì´í„° ê²€ì¦ ë³´ê³ ì„œ")
        report.append("**Date:** 2025-06-29")
        report.append("**Validator:** MACHO-GPT v3.4-mini")
        report.append("")
        
        # Executive Summary
        report.append("## ğŸ“‹ Executive Summary")
        report.append("")
        v28_acc = gaps['summary']['v28_accuracy'] * 100
        v281_acc = gaps['summary']['v281_accuracy'] * 100
        total_rows = gaps['summary']['total_rows_v28']
        
        report.append(f"ì‹¤ì œ Excel ë°ì´í„° **{total_rows:,}í–‰**ì„ v2.8 ë° v2.8.1 ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼:")
        report.append(f"- **v2.8 ì •í™•ë„**: {v28_acc:.1f}%")
        report.append(f"- **v2.8.1 ì •í™•ë„**: {v281_acc:.1f}%")
        report.append(f"- **ê°œì„ ë„**: {v281_acc - v28_acc:+.1f}%p")
        report.append("")
        
        # ë°ì´í„° êµ¬ì¡° ë¶„ì„
        report.append("## ğŸ“Š ë°ì´í„° êµ¬ì¡° ë¶„ì„")
        report.append("")
        report.append(f"- **ì´ íŒŒì¼ ìˆ˜**: {analysis['total_files']}ê°œ")
        report.append(f"- **ì´ ë°ì´í„°**: {analysis['total_rows']:,}í–‰")
        report.append("")
        
        for name, details in analysis['file_details'].items():
            report.append(f"### {name.upper()}")
            report.append(f"- í–‰ ìˆ˜: {details['rows']:,}")
            report.append(f"- ì»¬ëŸ¼ ìˆ˜: {details['columns']}")
            report.append(f"- Location ì»¬ëŸ¼: {'âœ…' if details['has_location'] else 'âŒ'}")
            report.append(f"- Status ì»¬ëŸ¼: {'âœ…' if details['has_status'] else 'âŒ'}")
            if details['potential_wh_cols']:
                report.append(f"- WH ê´€ë ¨ ì»¬ëŸ¼: {details['potential_wh_cols'][:3]}")
            if details['potential_mosb_cols']:
                report.append(f"- MOSB ê´€ë ¨ ì»¬ëŸ¼: {details['potential_mosb_cols']}")
            report.append("")
        
        # ê°­ ë¶„ì„ ê²°ê³¼
        report.append("## ğŸ“ˆ Flow Code ë¶„í¬ ë¹„êµ")
        report.append("")
        report.append("| Code | ì •ì˜ | ë³´ê³ ì„œ | v2.8 | v2.8.1 | v2.8 ê°­ | v2.8.1 ê°­ |")
        report.append("|:----:|:-----|:-----:|:----:|:------:|:-------:|:---------:|")
        
        definitions = {
            0: "Pre Arrival",
            1: "Portâ†’Site", 
            2: "Portâ†’WHâ†’Site",
            3: "Portâ†’WHâ†’MOSBâ†’Site",
            4: "Portâ†’WHâ†’whâ†’MOSBâ†’Site"
        }
        
        for code in range(5):
            def_text = definitions[code]
            report_count = self.report_distribution[code]
            v28_gap = gaps['report_vs_v28'][code]
            v281_gap = gaps['report_vs_v281'][code]
            
            v28_gap_str = f"{v28_gap['gap']:+d}" if v28_gap['gap'] != 0 else "0"
            v281_gap_str = f"{v281_gap['gap']:+d}" if v281_gap['gap'] != 0 else "0"
            
            report.append(f"| {code} | {def_text} | {report_count:,} | {v28_gap['v28']:,} | {v281_gap['v281']:,} | {v28_gap_str} | {v281_gap_str} |")
        
        report.append("")
        
        # ë¬¸ì œì  ë° ê°œì„ ì‚¬í•­
        if analysis['potential_issues']:
            report.append("## âš ï¸ ë°œê²¬ëœ ë¬¸ì œì ")
            report.append("")
            for issue in analysis['potential_issues'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                report.append(f"- {issue}")
            report.append("")
        
        return "\n".join(report)
    
    def run_full_validation(self) -> Dict:
        """ì „ì²´ ê²€ì¦ ì‹¤í–‰"""
        logger.info("ğŸš€ HVDC ì‹¤ë°ì´í„° ê²€ì¦ ì‹œì‘")
        logger.info("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        dataframes = self.load_excel_files()
        if not dataframes:
            logger.error("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False, 'error': 'No data loaded'}
        
        # 2. ë°ì´í„° êµ¬ì¡° ë¶„ì„
        analysis = self.analyze_data_structure(dataframes)
        
        # 3. v2.8 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
        v28_result = self.test_current_v28_algorithm(dataframes)
        
        # 4. v2.8.1 ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
        v281_result = self.test_improved_v281_algorithm(dataframes)
        
        # 5. ê°­ ë¶„ì„
        gaps = self.calculate_gaps(v28_result, v281_result)
        
        # 6. ë³´ê³ ì„œ ìƒì„±
        report = self.generate_validation_report(analysis, gaps)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("\nğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
        logger.info(f"   ì´ ë°ì´í„°: {analysis['total_rows']:,}í–‰")
        logger.info(f"   v2.8 ì •í™•ë„: {gaps['summary']['v28_accuracy']*100:.1f}%")
        logger.info(f"   v2.8.1 ì •í™•ë„: {gaps['summary']['v281_accuracy']*100:.1f}%")
        
        return {
            'success': True,
            'analysis': analysis,
            'v28_result': v28_result,
            'v281_result': v281_result,
            'gaps': gaps,
            'report': report
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    validator = RealDataValidator()
    
    # ì „ì²´ ê²€ì¦ ì‹¤í–‰
    result = validator.run_full_validation()
    
    if result['success']:
        # ë³´ê³ ì„œ ì €ì¥
        report_path = "HVDC_Real_Data_Validation_Report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(result['report'])
        
        logger.info(f"ğŸ“‹ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        # ì¶”ì²œ ëª…ë ¹ì–´
        logger.info("\nğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
        logger.info("/logi_master validate_real_data --complete [ì „ì²´ ì‹¤ë°ì´í„° ê²€ì¦]")
        logger.info("/logi_master compare_algorithms --v28_vs_v281 [ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ]")
        logger.info("/logi_master optimize_flow_code --target_accuracy 95 [Flow Code ìµœì í™”]")
    else:
        logger.error(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
    
    return result

if __name__ == "__main__":
    main() 
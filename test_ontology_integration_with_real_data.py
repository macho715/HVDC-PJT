#!/usr/bin/env python3
"""
ì‹¤ì œ HVDC ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì˜¨í†¨ë¡œì§€ í†µí•© í…ŒìŠ¤íŠ¸
- ì‹¤ì œ INVOICE ë°ì´í„° ë¡œë“œ
- ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œìŠ¤í…œê³¼ í†µí•© ê²€ì¦
- ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± í™•ì¸
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import json
from ontology_integrated_schema_validator import OntologyIntegratedSchemaValidator

def load_hvdc_data():
    """ì‹¤ì œ HVDC ë°ì´í„° ë¡œë“œ"""
    data_files = [
        "hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_HITACHI(HE).xlsx",
        "hvdc_ontology_system/data/HVDC WAREHOUSE_HITACHI(HE).xlsx",
        "hvdc_ontology_system/data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx", 
        "hvdc_ontology_system/data/HVDC WAREHOUSE_INVOICE.xlsx"
    ]
    
    all_data = pd.DataFrame()
    loaded_files = []
    
    for file_path in data_files:
        if Path(file_path).exists():
            try:
                print(f"ğŸ“ ë¡œë“œ ì¤‘: {file_path}")
                df = pd.read_excel(file_path)
                df['data_source'] = Path(file_path).stem
                all_data = pd.concat([all_data, df], ignore_index=True)
                loaded_files.append(file_path)
                print(f"âœ… {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
    return all_data, loaded_files

def standardize_hvdc_columns(df):
    """HVDC ë°ì´í„° ì»¬ëŸ¼ í‘œì¤€í™”"""
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ê¸°ì¡´ â†’ í‘œì¤€)
    column_mapping = {
        'Category': 'warehouse_name',
        'HVDC CODE 3': 'cargo_type', 
        'HVDC CODE 1': 'hvdc_project_code',
        'Package No.': 'package_count',
        'IMG No.': 'image_count',
        'Net Weight (kg)': 'weight_kg',
        'SQM': 'area_sqm',
        'Total (AED)': 'amount_aed',
        'Operation Month': 'operation_month',
        'wh handling': 'wh_handling',
        'flow code': 'flow_code'
    }
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    df_standardized = df.rename(columns=column_mapping)
    
    # í‘œì¤€ ì»¬ëŸ¼ ì¶”ê°€ (ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤)
    standard_columns = [
        'record_id', 'operation_month', 'hvdc_project_code', 'work_type',
        'cargo_type', 'warehouse_name', 'package_count', 'weight_kg',
        'volume_cbm', 'area_sqm', 'amount_aed', 'handling_in', 'handling_out',
        'rent', 'flow_code', 'wh_handling'
    ]
    
    for col in standard_columns:
        if col not in df_standardized.columns:
            if col == 'record_id':
                df_standardized[col] = [f'HVDC_INV_{i:06d}' for i in range(len(df_standardized))]
            elif col == 'work_type':
                # SQMì´ ìˆìœ¼ë©´ STORAGE, ì—†ìœ¼ë©´ HANDLINGìœ¼ë¡œ ì¶”ì •
                df_standardized[col] = df_standardized.apply(
                    lambda row: 'STORAGE' if pd.notna(row.get('area_sqm', 0)) and row.get('area_sqm', 0) > 0 else 'HANDLING',
                    axis=1
                )
            elif col == 'volume_cbm':
                df_standardized[col] = 0.0
            elif col in ['handling_in', 'handling_out']:
                df_standardized[col] = 0.0
            elif col == 'rent':
                # ì „ì²´ ê¸ˆì•¡ì—ì„œ í•¸ë“¤ë§ ë¹„ìš© ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¥¼ ì„ëŒ€ë£Œë¡œ ì¶”ì •
                total_amount = df_standardized.get('amount_aed', 0)
                estimated_handling = total_amount * 0.15  # 15% í•¸ë“¤ë§ ì¶”ì •
                df_standardized[col] = total_amount - estimated_handling
            else:
                df_standardized[col] = None
                
    # ì°½ê³ ëª… í‘œì¤€í™”
    warehouse_mapping = {
        'DSV Outdoor': 'DSV_OUTDOOR',
        'DSV Indoor': 'DSV_INDOOR', 
        'DSV Al Markaz': 'DSV_AL_MARKAZ',
        'DSV MZP': 'DSV_MZP',
        'AAA Storage': 'AAA_STORAGE'
    }
    
    if 'warehouse_name' in df_standardized.columns:
        df_standardized['warehouse_name'] = df_standardized['warehouse_name'].replace(warehouse_mapping)
    
    # í™”ë¬¼ íƒ€ì… í‘œì¤€í™”
    cargo_mapping = {
        'HE': 'HITACHI',
        'SIM': 'SIEMENS',
        'SCT': 'SAMSUNG_CT',
        'PRP': 'PRYSMIAN',
        'MOE': 'MOELLER'
    }
    
    if 'cargo_type' in df_standardized.columns:
        df_standardized['cargo_type'] = df_standardized['cargo_type'].replace(cargo_mapping)
    
    return df_standardized

def analyze_ontology_compatibility(validation_results):
    """ì˜¨í†¨ë¡œì§€ í˜¸í™˜ì„± ë¶„ì„"""
    print("\n" + "="*70)
    print("ğŸ” **ì˜¨í†¨ë¡œì§€ í˜¸í™˜ì„± ë¶„ì„**")
    print("="*70)
    
    # ê¸°ë³¸ ê²€ì¦ ê²°ê³¼
    basic_validation = {
        'total_records': validation_results.get('total_records', 0),
        'validation_rate': validation_results.get('validation_rate', 0),
        'quality_score': validation_results.get('overall_quality_score', 0)
    }
    
    print(f"ğŸ“Š **ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦:**")
    print(f"   ì´ ë ˆì½”ë“œ: {basic_validation['total_records']:,}ê±´")
    print(f"   ê²€ì¦ ì„±ê³µë¥ : {basic_validation['validation_rate']:.1f}%")
    print(f"   í’ˆì§ˆ ì ìˆ˜: {basic_validation['quality_score']:.1f}%")
    
    # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê²°ê³¼ (RDF ê¸°ëŠ¥ì´ í™œì„±í™”ëœ ê²½ìš°)
    if 'ontology_mapping' in validation_results:
        ontology_mapping = validation_results['ontology_mapping']
        print(f"\nğŸ”— **ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê²°ê³¼:**")
        print(f"   ë§¤í•‘ ì„±ê³µë¥ : {ontology_mapping.get('mapping_success_rate', 0):.1f}%")
        print(f"   ë§¤í•‘ ì˜¤ë¥˜: {len(ontology_mapping.get('mapping_errors', []))}ê±´")
        
        # ì‹œë§¨í‹± ë¶„ë¥˜ ë¶„ì„
        semantic_classes = ontology_mapping.get('semantic_classifications', {})
        if semantic_classes:
            print(f"   ì‹œë§¨í‹± ë¶„ë¥˜: {len(semantic_classes)}ê°œ í´ë˜ìŠ¤")
            for class_name, count in sorted(semantic_classes.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"     - {class_name}: {count}ê±´")
    
    # ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ê²€ì¦
    if 'business_rules' in validation_results:
        business_rules = validation_results['business_rules']
        print(f"\nğŸ“‹ **ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ê²€ì¦:**")
        print(f"   ì „ì²´ ë£°: {business_rules.get('total_rules', 0)}ê°œ")
        print(f"   í†µê³¼ ë£°: {business_rules.get('passed_rules', 0)}ê°œ")
        print(f"   ì‹¤íŒ¨ ë£°: {business_rules.get('failed_rules', 0)}ê°œ")
    
    # í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°
    compatibility_score = 0
    max_score = 100
    
    # ê¸°ë³¸ ê²€ì¦ ì ìˆ˜ (40%)
    if basic_validation['validation_rate'] > 80:
        compatibility_score += 40
    elif basic_validation['validation_rate'] > 60:
        compatibility_score += 30
    elif basic_validation['validation_rate'] > 40:
        compatibility_score += 20
    
    # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìˆ˜ (40%)
    if 'ontology_mapping' in validation_results:
        mapping_rate = validation_results['ontology_mapping'].get('mapping_success_rate', 0)
        if mapping_rate > 90:
            compatibility_score += 40
        elif mapping_rate > 70:
            compatibility_score += 30
        elif mapping_rate > 50:
            compatibility_score += 20
    else:
        compatibility_score += 20  # RDF ê¸°ëŠ¥ ë¹„í™œì„±í™”ì‹œ ê¸°ë³¸ ì ìˆ˜
    
    # í’ˆì§ˆ ì ìˆ˜ (20%)
    if basic_validation['quality_score'] > 80:
        compatibility_score += 20
    elif basic_validation['quality_score'] > 60:
        compatibility_score += 15
    elif basic_validation['quality_score'] > 40:
        compatibility_score += 10
    
    print(f"\nğŸ¯ **ì „ì²´ í˜¸í™˜ì„± ì ìˆ˜: {compatibility_score}/{max_score} ({compatibility_score}%)**")
    
    # í˜¸í™˜ì„± ë“±ê¸‰
    if compatibility_score >= 90:
        grade = "A+ (ì™„ì „ í˜¸í™˜)"
        color = "ğŸŸ¢"
    elif compatibility_score >= 80:
        grade = "A (ìš°ìˆ˜ í˜¸í™˜)"
        color = "ğŸŸ¢"
    elif compatibility_score >= 70:
        grade = "B (ì–‘í˜¸ í˜¸í™˜)"
        color = "ğŸŸ¡"
    elif compatibility_score >= 60:
        grade = "C (ë³´í†µ í˜¸í™˜)"
        color = "ğŸŸ¡"
    else:
        grade = "D (í˜¸í™˜ì„± ê°œì„  í•„ìš”)"
        color = "ğŸ”´"
    
    print(f"{color} **í˜¸í™˜ì„± ë“±ê¸‰: {grade}**")
    
    return compatibility_score

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”„ MACHO-GPT v3.4-mini ì‹¤ì œ ë°ì´í„° ì˜¨í†¨ë¡œì§€ í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ì‹¤ì œ HVDC ë°ì´í„° ë¡œë“œ ì¤‘...")
    hvdc_data, loaded_files = load_hvdc_data()
    
    if hvdc_data.empty:
        print("âŒ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì´ {len(hvdc_data)}ê±´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“ ë¡œë“œëœ íŒŒì¼: {len(loaded_files)}ê°œ")
    
    # 2. ë°ì´í„° í‘œì¤€í™”
    print("\nğŸ”„ ë°ì´í„° í‘œì¤€í™” ì¤‘...")
    standardized_data = standardize_hvdc_columns(hvdc_data)
    
    # í‘œì¤€í™” ê²°ê³¼ ìš”ì•½
    print(f"âœ… í‘œì¤€í™” ì™„ë£Œ:")
    print(f"   ì›ë³¸ ì»¬ëŸ¼: {len(hvdc_data.columns)}ê°œ")
    print(f"   í‘œì¤€ ì»¬ëŸ¼: {len(standardized_data.columns)}ê°œ")
    
    # 3. ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ê¸° ì´ˆê¸°í™”
    print("\nğŸ—ï¸ ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ê¸° ì´ˆê¸°í™”...")
    validator = OntologyIntegratedSchemaValidator(enable_ontology=True)
    
    # 4. ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹¤í–‰
    print("\nğŸ” ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹¤í–‰...")
    validation_results = validator.validate_with_ontology(standardized_data)
    
    # 5. ê²°ê³¼ ë¶„ì„
    compatibility_score = analyze_ontology_compatibility(validation_results)
    
    # 6. ë¦¬í¬íŠ¸ ìƒì„±
    try:
        report_path = validator.export_ontology_report(validation_results)
        print(f"\nğŸ“„ **ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±:** {report_path}")
    except Exception as e:
        print(f"\nâš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # 7. ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“‹ **MACHO-GPT v3.4-mini ì˜¨í†¨ë¡œì§€ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ**")
    print("="*70)
    
    print(f"ğŸ“Š **í…ŒìŠ¤íŠ¸ ê²°ê³¼:**")
    print(f"   ì²˜ë¦¬ ë°ì´í„°: {len(standardized_data):,}ê±´")
    print(f"   ë°ì´í„° ì†ŒìŠ¤: {len(loaded_files)}ê°œ íŒŒì¼")
    print(f"   í˜¸í™˜ì„± ì ìˆ˜: {compatibility_score}%")
    print(f"   ì˜¨í†¨ë¡œì§€ ìƒíƒœ: {'í™œì„±í™”' if validator.enable_ontology else 'ë¹„í™œì„±í™” (RDF ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)'}")
    
    if 'rdf_graph_path' in validation_results:
        print(f"   RDF ê·¸ë˜í”„: {validation_results['rdf_graph_path']}")
    
    print(f"\nğŸ“Š **Status:** {compatibility_score:.1f}% | Ontology_Integration_Test | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
    print("/logi_master [í†µí•© ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ë¬¼ë¥˜ ìµœì í™”]")
    print("/switch_mode LATTICE [ê³ ê¸‰ ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì‹¤í–‰]")
    print("/visualize_data [ì‹œë§¨í‹± ë¶„ë¥˜ ë° í˜¸í™˜ì„± ê²°ê³¼ ì‹œê°í™”]")
    
    return compatibility_score

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
ì—…ê³„ í‘œì¤€ Context Engineering ì‹œìŠ¤í…œ ì‹¤ì‹œê°„ ë°ëª¨
===============================================

ì—…ê·¸ë ˆì´ë“œëœ Context Engineering ì‹œìŠ¤í…œì˜ ëª¨ë“  ê¸°ëŠ¥ì„
ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œì—°í•˜ê³  ì„±ëŠ¥ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Any
from src.context_engineering_integration_enhanced import (
    EnhancedHVDCContextWindow, EnhancedHVDCContextScoring, 
    EnhancedHVDCContextProtocol, EnhancedHVDCContextEngineeringIntegration
)

class EnhancedContextEngineeringDemo:
    """ì—…ê³„ í‘œì¤€ Context Engineering ë°ëª¨ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scoring = EnhancedHVDCContextScoring()
        self.protocol = EnhancedHVDCContextProtocol()
        self.demo_results = []
        
    def print_header(self, title: str):
        """í—¤ë” ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {title}")
        print(f"{'='*60}")
    
    def print_metric(self, name: str, value: float, unit: str = "", status: str = ""):
        """ë©”íŠ¸ë¦­ ì¶œë ¥"""
        if status == "âœ…":
            print(f"âœ… {name}: {value:.3f}{unit}")
        elif status == "âš ï¸":
            print(f"âš ï¸ {name}: {value:.3f}{unit}")
        elif status == "âŒ":
            print(f"âŒ {name}: {value:.3f}{unit}")
        else:
            print(f"ğŸ“Š {name}: {value:.3f}{unit}")
    
    def demo_industry_standard_metrics(self):
        """ì—…ê³„ í‘œì¤€ ì§€í‘œ ë°ëª¨"""
        self.print_header("ì—…ê³„ í‘œì¤€ ì§€í‘œ ì‹¤ì‹œê°„ ê³„ì‚°")
        
        # ë‹¤ì–‘í•œ Context ìƒì„±
        contexts = [
            {
                "name": "ì™„ì „í•œ HVDC Context",
                "context": EnhancedHVDCContextWindow(
                    prompt="HVDC warehouse HITACHI equipment optimization",
                    examples=[
                        {"HVDC": "project", "HITACHI": "equipment", "optimization": "success"},
                        {"warehouse": "location", "logistics": "operation", "FANR": "compliance"}
                    ],
                    tools=["optimizer", "analyzer", "compliance_checker"],
                    logistics_context={"project": "HVDC", "vendor": "HITACHI"},
                    fanr_compliance={"status": "valid", "score": 0.98},
                    kpi_metrics={"efficiency": 0.92, "performance": 0.88}
                )
            },
            {
                "name": "ë¶€ë¶„ì  Context",
                "context": EnhancedHVDCContextWindow(
                    prompt="basic query",
                    examples=[{"basic": "data", "query": "simple"}],
                    tools=["basic_tool"]
                )
            },
            {
                "name": "ë¹ˆ Context",
                "context": EnhancedHVDCContextWindow()
            }
        ]
        
        for ctx_data in contexts:
            print(f"\nğŸ” {ctx_data['name']} ë¶„ì„:")
            context = ctx_data['context']
            
            # ì—…ê³„ í‘œì¤€ ì§€í‘œ ê³„ì‚°
            precision = self.scoring.calculate_context_precision(context)
            recall = self.scoring.calculate_context_recall(context)
            groundedness = self.scoring.calculate_groundedness(context)
            memory_quality = self.scoring.calculate_memory_quality(context)
            
            # ê²°ê³¼ ì¶œë ¥
            self.print_metric("Context Precision", precision, status="âœ…" if precision > 0.5 else "âš ï¸")
            self.print_metric("Context Recall", recall, status="âœ…" if recall > 0.1 else "âš ï¸")
            self.print_metric("Groundedness", groundedness, status="âœ…" if groundedness > 0.3 else "âš ï¸")
            
            print(f"ğŸ§  Memory Quality:")
            self.print_metric("  Freshness", memory_quality['freshness'])
            self.print_metric("  Relevance", memory_quality['relevance'])
            self.print_metric("  Coherence", memory_quality['coherence'])
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            total_score = self.scoring.score_context_quality_enhanced(context)
            self.print_metric("ì´ Context ì ìˆ˜", total_score, status="âœ…" if total_score > 0.6 else "âš ï¸")
            
            self.demo_results.append({
                "context_name": ctx_data['name'],
                "precision": precision,
                "recall": recall,
                "groundedness": groundedness,
                "memory_quality": memory_quality,
                "total_score": total_score
            })
    
    def demo_enhanced_response_quality(self):
        """í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë°ëª¨"""
        self.print_header("í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€")
        
        # ë‹¤ì–‘í•œ ì‘ë‹µ ìƒì„±
        responses = [
            {
                "name": "ìš°ìˆ˜í•œ ì‘ë‹µ",
                "response": {
                    "status": "SUCCESS",
                    "confidence": 0.95,
                    "mode": "PRIME",
                    "recommended_commands": ["get_kpi", "enhance_dashboard", "weather_tie"],
                    "timestamp": datetime.now().isoformat(),
                    "context_engineering": {"score": 0.9},
                    "enhanced_context_engineering": {
                        "context_score": 0.8,
                        "response_score": 0.85
                    }
                }
            },
            {
                "name": "ë³´í†µ ì‘ë‹µ",
                "response": {
                    "status": "SUCCESS",
                    "confidence": 0.8,
                    "mode": "PRIME",
                    "recommended_commands": ["get_kpi"]
                }
            },
            {
                "name": "ì˜¤ë¥˜ ì‘ë‹µ",
                "response": {
                    "status": "ERROR",
                    "error_message": "Invalid command parameter",
                    "confidence": 0.0
                }
            }
        ]
        
        for resp_data in responses:
            print(f"\nğŸ“Š {resp_data['name']} í‰ê°€:")
            response = resp_data['response']
            
            # ë‹¤ì°¨ì› ì‘ë‹µ í’ˆì§ˆ í‰ê°€
            groundedness = self.scoring._calculate_response_groundedness(response)
            completeness = self.scoring._calculate_response_completeness(response)
            faithfulness = self.scoring._calculate_response_faithfulness(response)
            helpfulness = self.scoring._calculate_response_helpfulness(response)
            toxicity = self.scoring._calculate_response_toxicity(response)
            latency = self.scoring._calculate_response_latency(response)
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weighted_groundedness = groundedness * self.scoring.response_weights["groundedness"]
            weighted_completeness = completeness * self.scoring.response_weights["completeness"]
            weighted_faithfulness = faithfulness * self.scoring.response_weights["faithfulness"]
            weighted_helpfulness = helpfulness * self.scoring.response_weights["helpfulness"]
            weighted_toxicity = toxicity * self.scoring.response_weights["toxicity"]
            weighted_latency = latency * self.scoring.response_weights["latency"]
            
            # ê²°ê³¼ ì¶œë ¥
            self.print_metric("Groundedness", groundedness, " (ê°€ì¤‘ì¹˜: 0.30)")
            self.print_metric("Completeness", completeness, " (ê°€ì¤‘ì¹˜: 0.20)")
            self.print_metric("Faithfulness", faithfulness, " (ê°€ì¤‘ì¹˜: 0.20)")
            self.print_metric("Helpfulness", helpfulness, " (ê°€ì¤‘ì¹˜: 0.15)")
            self.print_metric("Toxicity", toxicity, " (ê°€ì¤‘ì¹˜: -0.20)")
            self.print_metric("Latency", latency, " (ê°€ì¤‘ì¹˜: -0.10)")
            
            print(f"\nğŸ“ˆ ê°€ì¤‘ì¹˜ ì ìš© ê²°ê³¼:")
            self.print_metric("  Groundedness ì ìˆ˜", weighted_groundedness)
            self.print_metric("  Completeness ì ìˆ˜", weighted_completeness)
            self.print_metric("  Faithfulness ì ìˆ˜", weighted_faithfulness)
            self.print_metric("  Helpfulness ì ìˆ˜", weighted_helpfulness)
            self.print_metric("  Toxicity ì ìˆ˜", weighted_toxicity)
            self.print_metric("  Latency ì ìˆ˜", weighted_latency)
            
            # ì´ì  ê³„ì‚°
            total_score = self.scoring.score_response_quality_enhanced(response)
            self.print_metric("ì´ ì‘ë‹µ ì ìˆ˜", total_score, status="âœ…" if total_score > 0.7 else "âš ï¸")
            
            self.demo_results.append({
                "response_name": resp_data['name'],
                "groundedness": groundedness,
                "completeness": completeness,
                "faithfulness": faithfulness,
                "helpfulness": helpfulness,
                "toxicity": toxicity,
                "latency": latency,
                "total_score": total_score
            })
    
    def demo_dynamic_thresholds(self):
        """ë™ì  ì„ê³„ê°’ ì¡°ì • ë°ëª¨"""
        self.print_header("ë™ì  ì„ê³„ê°’ ì¡°ì • ì‹œìŠ¤í…œ")
        
        # ì´ˆê¸° ì„ê³„ê°’
        initial_thresholds = self.scoring.dynamic_thresholds.copy()
        print("ğŸ“Š ì´ˆê¸° ì„ê³„ê°’:")
        for key, value in initial_thresholds.items():
            self.print_metric(key, value)
        
        # ê°€ìƒì˜ ì„±ëŠ¥ ë°ì´í„° ìƒì„±
        print(f"\nğŸ”„ ì„±ëŠ¥ ë°ì´í„°ë¡œ ì„ê³„ê°’ ì—…ë°ì´íŠ¸ ì¤‘...")
        
        # ë‹¤ì–‘í•œ ì„±ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤
        scenarios = [
            {
                "name": "ë†’ì€ ì„±ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤",
                "scores": [0.92, 0.94, 0.91, 0.93, 0.95, 0.90, 0.93, 0.92, 0.94, 0.91] * 3
            },
            {
                "name": "ë³´í†µ ì„±ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤", 
                "scores": [0.85, 0.87, 0.83, 0.86, 0.88, 0.84, 0.87, 0.85, 0.86, 0.83] * 3
            },
            {
                "name": "ë‚®ì€ ì„±ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤",
                "scores": [0.75, 0.77, 0.73, 0.76, 0.78, 0.74, 0.77, 0.75, 0.76, 0.73] * 3
            }
        ]
        
        for scenario in scenarios:
            print(f"\nğŸ“ˆ {scenario['name']}:")
            
            # ì„ê³„ê°’ ì—…ë°ì´íŠ¸
            self.scoring.update_dynamic_thresholds(scenario['scores'])
            updated_threshold = self.scoring.dynamic_thresholds["confidence"]
            
            self.print_metric("í‰ê·  ì„±ëŠ¥", sum(scenario['scores']) / len(scenario['scores']))
            self.print_metric("ì—…ë°ì´íŠ¸ëœ ì„ê³„ê°’", updated_threshold)
            self.print_metric("ì„ê³„ê°’ ë³€í™”", updated_threshold - initial_thresholds["confidence"])
            
            # ì„ê³„ê°’ ë¦¬ì…‹
            self.scoring.dynamic_thresholds = initial_thresholds.copy()
    
    def demo_memory_penalty_system(self):
        """ë©”ëª¨ë¦¬ íŒ¨ë„í‹° ì‹œìŠ¤í…œ ë°ëª¨"""
        self.print_header("ë©”ëª¨ë¦¬ í’ˆì§ˆ íŒ¨ë„í‹° ì‹œìŠ¤í…œ")
        
        # ë©”ëª¨ë¦¬ í’ˆì§ˆë³„ Context ìƒì„±
        memory_scenarios = [
            {
                "name": "ì‹ ì„ í•œ ë©”ëª¨ë¦¬ (24ì‹œê°„ ë‚´)",
                "memory": {
                    "recent_1": {
                        "command": "enhance_dashboard",
                        "status": "SUCCESS",
                        "timestamp": datetime.now().isoformat()
                    },
                    "recent_2": {
                        "command": "get_kpi",
                        "status": "SUCCESS", 
                        "timestamp": (datetime.now().isoformat())
                    }
                }
            },
            {
                "name": "í˜¼í•© ë©”ëª¨ë¦¬ (ì‹ ì„  + ì˜¤ë˜ë¨)",
                "memory": {
                    "recent": {
                        "command": "enhance_dashboard",
                        "status": "SUCCESS",
                        "timestamp": datetime.now().isoformat()
                    },
                    "old": {
                        "command": "get_kpi",
                        "status": "SUCCESS",
                        "timestamp": "2025-01-01T00:00:00"
                    }
                }
            },
            {
                "name": "ë¹ˆ ë©”ëª¨ë¦¬",
                "memory": {}
            }
        ]
        
        for scenario in memory_scenarios:
            print(f"\nğŸ§  {scenario['name']}:")
            
            context = EnhancedHVDCContextWindow()
            context.prompt = "test command"
            context.memory = scenario['memory']
            
            # ë©”ëª¨ë¦¬ í’ˆì§ˆ ê³„ì‚°
            memory_quality = self.scoring.calculate_memory_quality(context)
            
            self.print_metric("Memory Freshness", memory_quality['freshness'])
            self.print_metric("Memory Relevance", memory_quality['relevance'])
            self.print_metric("Memory Coherence", memory_quality['coherence'])
            self.print_metric("Memory Penalty", memory_quality['penalty'])
            
            # ì „ì²´ Context ì ìˆ˜
            total_score = self.scoring.score_context_quality_enhanced(context)
            self.print_metric("ì´ Context ì ìˆ˜", total_score)
            
            self.demo_results.append({
                "memory_scenario": scenario['name'],
                "memory_quality": memory_quality,
                "total_score": total_score
            })
    
    async def demo_enhanced_integration(self):
        """í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ ë°ëª¨"""
        self.print_header("í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ ì‹¤ì‹œê°„ ì‹¤í–‰")
        
        # ê°€ìƒì˜ LogiMaster ì‹œìŠ¤í…œ
        class MockLogiMasterSystem:
            async def initialize(self):
                pass
            
            async def execute_command(self, command: str, parameters: Dict[str, Any] = None):
                # ëª…ë ¹ì–´ë³„ ë‹¤ë¥¸ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
                if command == "enhance_dashboard":
                    return {
                        "status": "SUCCESS",
                        "confidence": 0.95,
                        "mode": "LATTICE",
                        "command": command,
                        "parameters": parameters,
                        "recommended_commands": ["get_kpi", "weather_tie"],
                        "timestamp": datetime.now().isoformat(),
                        "enhancement_result": "Dashboard enhanced with weather integration"
                    }
                elif command == "excel_query":
                    return {
                        "status": "SUCCESS",
                        "confidence": 0.88,
                        "mode": "PRIME",
                        "command": command,
                        "parameters": parameters,
                        "recommended_commands": ["enhance_dashboard"],
                        "timestamp": datetime.now().isoformat(),
                        "query_result": "Data filtered successfully"
                    }
                else:
                    return {
                        "status": "ERROR",
                        "error_message": f"Unknown command: {command}",
                        "confidence": 0.0,
                        "mode": "ZERO"
                    }
        
        # í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        mock_logi_master = MockLogiMasterSystem()
        enhanced_integration = EnhancedHVDCContextEngineeringIntegration(mock_logi_master)
        
        # ë‹¤ì–‘í•œ ëª…ë ¹ì–´ ì‹¤í–‰
        commands = [
            {
                "name": "ëŒ€ì‹œë³´ë“œ ê°•í™”",
                "command": "enhance_dashboard",
                "parameters": {"dashboard_id": "main", "enhancement_type": "weather_integration"}
            },
            {
                "name": "Excel ì¿¼ë¦¬",
                "command": "excel_query", 
                "parameters": {"query": "Show me all HITACHI equipment"}
            },
            {
                "name": "ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´",
                "command": "unknown_command",
                "parameters": {}
            }
        ]
        
        for cmd_data in commands:
            print(f"\nğŸš€ {cmd_data['name']} ì‹¤í–‰:")
            
            start_time = time.time()
            result = await enhanced_integration.execute_command_with_context(
                cmd_data['command'], cmd_data['parameters']
            )
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ë¶„ì„
            print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
            print(f"ğŸ“Š ì‹¤í–‰ ìƒíƒœ: {result['status']}")
            
            if 'enhanced_context_engineering' in result:
                ce_data = result['enhanced_context_engineering']
                
                print(f"ğŸ¯ Context Engineering ê²°ê³¼:")
                self.print_metric("Context ì ìˆ˜", ce_data['context_score'])
                self.print_metric("ì‘ë‹µ ì ìˆ˜", ce_data['response_score'])
                self.print_metric("Context Precision", ce_data['context_precision'])
                self.print_metric("Context Recall", ce_data['context_recall'])
                self.print_metric("Groundedness", ce_data['groundedness'])
                self.print_metric("Field Resonance", ce_data['field_resonance'])
                self.print_metric("Attractor Strength", ce_data['attractor_strength'])
                
                if 'memory_quality' in ce_data:
                    memory = ce_data['memory_quality']
                    print(f"ğŸ§  Memory Quality:")
                    self.print_metric("  Freshness", memory['freshness'])
                    self.print_metric("  Relevance", memory['relevance'])
                    self.print_metric("  Coherence", memory['coherence'])
            
            self.demo_results.append({
                "command_name": cmd_data['name'],
                "execution_time": execution_time,
                "result": result
            })
        
        # ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        print(f"\nğŸ“ˆ ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„:")
        analytics = await enhanced_integration.get_enhanced_context_analytics()
        
        self.print_metric("ì´ Context ìˆ˜", analytics['total_contexts'])
        self.print_metric("í‰ê·  Context ì ìˆ˜", analytics['average_context_score'])
        self.print_metric("í‰ê·  ì‘ë‹µ ì ìˆ˜", analytics['average_response_score'])
        self.print_metric("í‰ê·  Precision", analytics['average_precision'])
        self.print_metric("í‰ê·  Recall", analytics['average_recall'])
        self.print_metric("í‰ê·  Groundedness", analytics['average_groundedness'])
        
        if 'memory_quality' in analytics:
            memory = analytics['memory_quality']
            print(f"ğŸ§  ì „ì²´ Memory Quality:")
            self.print_metric("  í‰ê·  Freshness", memory['average_freshness'])
            self.print_metric("  í‰ê·  Relevance", memory['average_relevance'])
            self.print_metric("  í‰ê·  Coherence", memory['average_coherence'])
        
        self.demo_results.append({
            "analytics": analytics
        })
    
    def generate_demo_report(self):
        """ë°ëª¨ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.print_header("ì—…ê³„ í‘œì¤€ Context Engineering ë°ëª¨ ë¦¬í¬íŠ¸")
        
        print(f"ğŸ“Š ë°ëª¨ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½:")
        print(f"  ì´ ë°ëª¨ í•­ëª©: {len(self.demo_results)}ê°œ")
        
        # Context í’ˆì§ˆ ë¶„ì„
        context_scores = [r['total_score'] for r in self.demo_results if 'total_score' in r and 'context_name' in r]
        if context_scores:
            print(f"  Context í’ˆì§ˆ í‰ê· : {sum(context_scores)/len(context_scores):.3f}")
        
        # ì‘ë‹µ í’ˆì§ˆ ë¶„ì„
        response_scores = [r['total_score'] for r in self.demo_results if 'total_score' in r and 'response_name' in r]
        if response_scores:
            print(f"  ì‘ë‹µ í’ˆì§ˆ í‰ê· : {sum(response_scores)/len(response_scores):.3f}")
        
        # ë©”ëª¨ë¦¬ í’ˆì§ˆ ë¶„ì„
        memory_scores = [r['total_score'] for r in self.demo_results if 'total_score' in r and 'memory_scenario' in r]
        if memory_scores:
            print(f"  ë©”ëª¨ë¦¬ í’ˆì§ˆ í‰ê· : {sum(memory_scores)/len(memory_scores):.3f}")
        
        print(f"\nğŸ¯ ì£¼ìš” ê°œì„  íš¨ê³¼:")
        print(f"  âœ… ì—…ê³„ í‘œì¤€ ì§€í‘œ ë„ì…: Precision/Recall/Groundedness")
        print(f"  âœ… ë‹¤ì°¨ì› ì‘ë‹µ í’ˆì§ˆ í‰ê°€: 6ê°œ ì§€í‘œ ê°€ì¤‘ì¹˜ ì ìš©")
        print(f"  âœ… ë™ì  ì„ê³„ê°’ ì¡°ì •: ROC ë¶„ì„ ê¸°ë°˜")
        print(f"  âœ… ë©”ëª¨ë¦¬ íŒ¨ë„í‹° ì‹œìŠ¤í…œ: LoCoMo/HELMET ë²¤ì¹˜ë§ˆí¬")
        print(f"  âœ… HVDC ë„ë©”ì¸ íŠ¹í™”: FANR/MOIAT ì¤€ìˆ˜")
        
        return {
            "demo_results": self.demo_results,
            "summary": {
                "total_demos": len(self.demo_results),
                "context_quality_avg": sum(context_scores)/len(context_scores) if context_scores else 0,
                "response_quality_avg": sum(response_scores)/len(response_scores) if response_scores else 0,
                "memory_quality_avg": sum(memory_scores)/len(memory_scores) if memory_scores else 0
            }
        }
    
    async def run_full_demo(self):
        """ì „ì²´ ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€ ì—…ê³„ í‘œì¤€ Context Engineering ì‹œìŠ¤í…œ ì‹¤ì‹œê°„ ë°ëª¨ ì‹œì‘")
        print("=" * 70)
        
        # 1. ì—…ê³„ í‘œì¤€ ì§€í‘œ ë°ëª¨
        self.demo_industry_standard_metrics()
        
        # 2. í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë°ëª¨
        self.demo_enhanced_response_quality()
        
        # 3. ë™ì  ì„ê³„ê°’ ì¡°ì • ë°ëª¨
        self.demo_dynamic_thresholds()
        
        # 4. ë©”ëª¨ë¦¬ íŒ¨ë„í‹° ì‹œìŠ¤í…œ ë°ëª¨
        self.demo_memory_penalty_system()
        
        # 5. í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ ë°ëª¨
        await self.demo_enhanced_integration()
        
        # 6. ë°ëª¨ ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_demo_report()
        
        # ê²°ê³¼ ì €ì¥
        with open("enhanced_context_engineering_demo_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ ë°ëª¨ ë¦¬í¬íŠ¸ê°€ 'enhanced_context_engineering_demo_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return report

async def main():
    """ë©”ì¸ ë°ëª¨ ì‹¤í–‰"""
    demo = EnhancedContextEngineeringDemo()
    report = await demo.run_full_demo()
    return report

if __name__ == "__main__":
    asyncio.run(main()) 
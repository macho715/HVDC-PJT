#!/usr/bin/env python3
"""
HVDC-STATUS ë°ì´í„° ë¶„ì„ ë° ë§¤í•‘ ë„êµ¬
/cmd_status_mapping ëª…ë ¹ì–´ êµ¬í˜„
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_hvdc_status():
    """HVDC-STATUS.xlsx íŒŒì¼ êµ¬ì¡° ë¶„ì„"""
    print("ğŸš€ /cmd_status_mapping ì‹¤í–‰")
    print("=" * 70)
    print("ğŸ“ˆ HVDC-STATUS ë°ì´í„° ë¶„ì„ ë° ë§¤í•‘")
    print("=" * 70)
    
    try:
        # íŒŒì¼ ë¡œë“œ
        print("ğŸ“‹ HVDC-STATUS íŒŒì¼ ë¡œë“œ ì¤‘...")
        df = pd.read_excel('data/HVDC-STATUS.xlsx')
        
        print(f"âœ… HVDC-STATUS: {len(df):,}í–‰ ë¡œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"   ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB")
        
        # ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„
        print(f"\nğŸ“‹ HVDC-STATUS ì»¬ëŸ¼ ëª©ë¡ ({len(df.columns)}ê°œ):")
        for i, col in enumerate(df.columns, 1):
            print(f"   {i:2d}. {col}")
        
        # ë°ì´í„° íƒ€ì… ë¶„ì„
        numeric_cols = df.select_dtypes(include=['number']).columns
        date_cols = df.select_dtypes(include=['datetime']).columns
        text_cols = df.select_dtypes(include=['object']).columns
        
        print(f"\nğŸ“Š ë°ì´í„° íƒ€ì… ë¶„ì„:")
        print(f"   ğŸ“Š ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        print(f"   ğŸ“Š ë‚ ì§œí˜• ì»¬ëŸ¼: {len(date_cols)}ê°œ")
        print(f"   ğŸ“Š í…ìŠ¤íŠ¸í˜• ì»¬ëŸ¼: {len(text_cols)}ê°œ")
        
        # ê²°ì¸¡ê°’ ë¶„ì„
        missing_data = df.isnull().sum()
        missing_cols = missing_data[missing_data > 0]
        print(f"   ğŸ“Š ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
        
        # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
        print(f"\nğŸ“ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3í–‰):")
        print(df.head(3).to_string())
        
        return df
        
    except Exception as e:
        print(f"âŒ HVDC-STATUS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_mapping_rules():
    """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
    try:
        with open('mapping_rules_v2.6.json', 'r', encoding='utf-8') as f:
            rules = json.load(f)
        print("âœ… ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì„±ê³µ")
        return rules
    except FileNotFoundError:
        print("âŒ mapping_rules_v2.6.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

def map_hvdc_status_columns(df, rules):
    """HVDC-STATUS ì»¬ëŸ¼ì„ ì˜¨í†¨ë¡œì§€ì— ë§¤í•‘"""
    if not rules or df is None:
        return {}
    
    print(f"\nğŸ”— HVDC-STATUS ì»¬ëŸ¼ ì˜¨í†¨ë¡œì§€ ë§¤í•‘")
    print("-" * 50)
    
    field_map = rules.get('field_map', {})
    
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ì •í™• ë§¤ì¹­ + ìœ ì‚¬ ë§¤ì¹­)
    mapped_columns = {}
    unmapped_columns = []
    
    for col in df.columns:
        mapped = False
        
        # 1. ì •í™•í•œ ë§¤ì¹­
        if col in field_map:
            mapped_columns[col] = field_map[col]
            mapped = True
        else:
            # 2. ìœ ì‚¬ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œ)
            col_normalized = col.lower().replace(' ', '').replace('_', '').replace('(', '').replace(')', '').replace('.', '')
            
            for excel_col, rdf_prop in field_map.items():
                excel_normalized = excel_col.lower().replace(' ', '').replace('_', '').replace('(', '').replace(')', '').replace('.', '')
                
                if col_normalized == excel_normalized:
                    mapped_columns[col] = rdf_prop
                    mapped = True
                    break
                
                # 3. ë¶€ë¶„ ë§¤ì¹­ (í¬í•¨ ê´€ê³„)
                if col_normalized in excel_normalized or excel_normalized in col_normalized:
                    if len(col_normalized) > 3 and len(excel_normalized) > 3:  # ë„ˆë¬´ ì§§ì€ ë§¤ì¹­ ë°©ì§€
                        mapped_columns[col] = rdf_prop
                        mapped = True
                        break
        
        if not mapped:
            unmapped_columns.append(col)
    
    print(f"ğŸ“‹ HVDC-STATUS ì»¬ëŸ¼ ë§¤í•‘ ê²°ê³¼:")
    print(f"   âœ… ë§¤í•‘ ì„±ê³µ: {len(mapped_columns)}ê°œ")
    print(f"   âŒ ë§¤í•‘ ì‹¤íŒ¨: {len(unmapped_columns)}ê°œ")
    
    # ë§¤í•‘ëœ ì»¬ëŸ¼ í‘œì‹œ
    if mapped_columns:
        print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ ë§¤í•‘ëœ ì»¬ëŸ¼:")
        for original, mapped in mapped_columns.items():
            print(f"   {original} â†’ {mapped}")
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ í‘œì‹œ
    if unmapped_columns:
        print(f"\nâŒ ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼:")
        for col in unmapped_columns:
            print(f"   â€¢ {col}")
    
    return mapped_columns

def convert_hvdc_status_to_rdf(df, rules, mapped_columns):
    """HVDC-STATUS ë°ì´í„°ë¥¼ RDFë¡œ ë³€í™˜"""
    if df is None or not rules or not mapped_columns:
        print("âŒ RDF ë³€í™˜ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return None
    
    print(f"\nğŸ”— HVDC-STATUS ë°ì´í„°ë¥¼ RDFë¡œ ë³€í™˜ ì¤‘...")
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    # TTL í—¤ë”
    ttl_content = f"""# HVDC-STATUS Data Ontology RDF
# Generated: {datetime.now().isoformat()}
# Source: HVDC-STATUS.xlsx
# Total Records: {len(df):,}

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix ex: <{namespace}> .

# Ontology Declaration
ex: a owl:Ontology ;
    rdfs:label "HVDC Status Data Ontology" ;
    rdfs:comment "Status data from HVDC-STATUS.xlsx ({len(df):,} records)" ;
    owl:versionInfo "2.6" ;
    owl:versionIRI <{namespace}v2.6> .

"""
    
    # ë°ì´í„° ë³€í™˜
    event_counter = 1
    
    for index, row in df.iterrows():
        event_uri = f"ex:StatusEvent_{event_counter:06d}"
        ttl_content += f"\n# Status Event {event_counter}\n"
        ttl_content += f"{event_uri} rdf:type ex:StatusEvent ;\n"
        ttl_content += f"    ex:hasDataSource \"HVDC-STATUS\" ;\n"
        
        # ë§¤í•‘ëœ ì»¬ëŸ¼ë“¤ì„ RDF ì†ì„±ìœ¼ë¡œ ë³€í™˜
        for excel_col, rdf_property in mapped_columns.items():
            value = row[excel_col]
            
            if pd.notna(value):
                # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
                if isinstance(value, (int, float)):
                    ttl_content += f"    {rdf_property} {value} ;\n"
                elif isinstance(value, str):
                    # ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                    escaped_value = value.replace('"', '\\"').replace('\n', '\\n')
                    ttl_content += f"    {rdf_property} \"{escaped_value}\" ;\n"
                else:
                    ttl_content += f"    {rdf_property} \"{str(value)}\" ;\n"
        
        ttl_content = ttl_content.rstrip(' ;\n') + " .\n"
        event_counter += 1
    
    print(f"âœ… HVDC-STATUS RDF ë³€í™˜ ì™„ë£Œ: {len(df):,}ê°œ ì´ë²¤íŠ¸")
    return ttl_content

def generate_hvdc_status_sparql(rules, df):
    """HVDC-STATUS ì „ìš© SPARQL ì¿¼ë¦¬ ìƒì„±"""
    if not rules or df is None:
        return ""
    
    print(f"ğŸ” HVDC-STATUS ì „ìš© SPARQL ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    sparql_queries = f"""# HVDC-STATUS ì „ìš© SPARQL ì¿¼ë¦¬ ëª¨ìŒ
# Generated: {datetime.now().isoformat()}
# Total Records: {len(df):,}

# 1. HVDC-STATUS ì „ì²´ í†µê³„
PREFIX ex: <{namespace}>
SELECT 
    (COUNT(?event) AS ?totalStatusEvents)
    (COUNT(DISTINCT ?location) AS ?uniqueLocations)
    (COUNT(DISTINCT ?vendor) AS ?uniqueVendors)
WHERE {{
    ?event rdf:type ex:StatusEvent .
    OPTIONAL {{ ?event ex:hasLocation ?location }}
    OPTIONAL {{ ?event ex:hasVendor ?vendor }}
}}

# 2. ìƒíƒœë³„ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?status 
    (COUNT(?event) AS ?eventCount)
WHERE {{
    ?event rdf:type ex:StatusEvent ;
           ex:hasStatus ?status .
}}
GROUP BY ?status
ORDER BY DESC(?eventCount)

# 3. ìœ„ì¹˜ë³„ ìƒíƒœ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?location ?status
    (COUNT(?event) AS ?eventCount)
WHERE {{
    ?event rdf:type ex:StatusEvent ;
           ex:hasLocation ?location ;
           ex:hasStatus ?status .
}}
GROUP BY ?location ?status
ORDER BY ?location ?status

# 4. ë°ì´í„° í’ˆì§ˆ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT 
    (COUNT(?event) AS ?totalEvents)
    (COUNT(?location) AS ?eventsWithLocation)
    (COUNT(?status) AS ?eventsWithStatus)
    (COUNT(?date) AS ?eventsWithDate)
WHERE {{
    ?event rdf:type ex:StatusEvent .
    OPTIONAL {{ ?event ex:hasLocation ?location }}
    OPTIONAL {{ ?event ex:hasStatus ?status }}
    OPTIONAL {{ ?event ex:hasDate ?date }}
}}

"""
    
    return sparql_queries

def save_hvdc_status_outputs(ttl_content, sparql_queries, df):
    """HVDC-STATUS ê²°ê³¼ íŒŒì¼ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # RDF/TTL íŒŒì¼ ì €ì¥
    ttl_filename = f"rdf_output/hvdc_status_{len(df)}records_{timestamp}.ttl"
    Path("rdf_output").mkdir(exist_ok=True)
    
    with open(ttl_filename, "w", encoding="utf-8") as f:
        f.write(ttl_content)
    
    ttl_size = Path(ttl_filename).stat().st_size / 1024 / 1024
    print(f"âœ… HVDC-STATUS RDF/TTL ì €ì¥: {ttl_filename}")
    print(f"   ğŸ“Š íŒŒì¼ í¬ê¸°: {ttl_size:.2f}MB")
    
    # SPARQL ì¿¼ë¦¬ íŒŒì¼ ì €ì¥
    sparql_filename = f"rdf_output/hvdc_status_queries_{len(df)}records_{timestamp}.sparql"
    with open(sparql_filename, "w", encoding="utf-8") as f:
        f.write(sparql_queries)
    
    print(f"âœ… HVDC-STATUS SPARQL ì¿¼ë¦¬ ì €ì¥: {sparql_filename}")
    
    # í†µê³„ íŒŒì¼ ì €ì¥
    stats_filename = f"rdf_output/hvdc_status_stats_{timestamp}.md"
    stats_content = f"""# HVDC-STATUS ë°ì´í„° ë§¤í•‘ í†µê³„

## ğŸ“Š ì²˜ë¦¬ í†µê³„
- **ì²˜ë¦¬ ì¼ì‹œ**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **ì´ ë ˆì½”ë“œ ìˆ˜**: {len(df):,}ê°œ
- **ì»¬ëŸ¼ ìˆ˜**: {len(df.columns)}ê°œ

## ğŸ“ ìƒì„±ëœ íŒŒì¼
- **RDF/TTL**: `{ttl_filename}` ({ttl_size:.2f}MB)
- **SPARQL**: `{sparql_filename}` (ì „ìš© ì¿¼ë¦¬)

"""
    
    with open(stats_filename, "w", encoding="utf-8") as f:
        f.write(stats_content)
    
    print(f"âœ… HVDC-STATUS í†µê³„ ì €ì¥: {stats_filename}")
    
    return ttl_filename, sparql_filename, stats_filename

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = datetime.now()
    
    # 1. HVDC-STATUS íŒŒì¼ ë¶„ì„
    df = analyze_hvdc_status()
    if df is None:
        return
    
    # 2. ë§¤í•‘ ê·œì¹™ ë¡œë“œ
    rules = load_mapping_rules()
    if not rules:
        return
    
    # 3. ì»¬ëŸ¼ ë§¤í•‘
    mapped_columns = map_hvdc_status_columns(df, rules)
    
    # 4. RDF ë³€í™˜
    ttl_content = convert_hvdc_status_to_rdf(df, rules, mapped_columns)
    
    # 5. SPARQL ì¿¼ë¦¬ ìƒì„±
    sparql_queries = generate_hvdc_status_sparql(rules, df)
    
    # 6. ê²°ê³¼ ì €ì¥
    if ttl_content:
        ttl_file, sparql_file, stats_file = save_hvdc_status_outputs(ttl_content, sparql_queries, df)
    
    # 7. ìµœì¢… ê²°ê³¼
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    
    print(f"\nğŸ‰ HVDC-STATUS ë§¤í•‘ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   â€¢ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
    print(f"   â€¢ ì´ ë ˆì½”ë“œ ìˆ˜: {len(df):,}ê°œ")
    print(f"   â€¢ ì²˜ë¦¬ ì†ë„: {len(df)/processing_time:.0f}ê°œ/ì´ˆ")
    print(f"   â€¢ ë§¤í•‘ ì„±ê³µ: {len(mapped_columns)}ê°œ ì»¬ëŸ¼")
    print(f"   â€¢ ë§¤í•‘ ì‹¤íŒ¨: {len(df.columns) - len(mapped_columns)}ê°œ ì»¬ëŸ¼")
    
    if ttl_content:
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print(f"   â€¢ RDF/TTL: {Path(ttl_file).name}")
        print(f"   â€¢ SPARQL: {Path(sparql_file).name}")
        print(f"   â€¢ í†µê³„: {Path(stats_file).name}")
    
    print(f"\nğŸ”§ ì¶”ì²œ ëª…ë ¹ì–´:")
    print(f"   /cmd_status_query [HVDC-STATUS ì¿¼ë¦¬ ì‹¤í–‰]")
    print(f"   /cmd_status_validation [ìƒíƒœ ë°ì´í„° ê²€ì¦]")
    print(f"   /cmd_status_analysis [ìƒíƒœ ë¶„ì„]")
    print(f"   /cmd_export_status_excel [Excel ë¦¬í¬íŠ¸ ìƒì„±]")

if __name__ == "__main__":
    main() 
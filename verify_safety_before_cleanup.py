#!/usr/bin/env python3
"""
HVDC Project ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì „ ì•ˆì „ì„± ê²€ì¦
MACHO-GPT v3.4-mini â”‚ Samsung C&T Logistics
"""

import os
import json
import hashlib
import ast
import re
from pathlib import Path
from collections import defaultdict

def calculate_file_hash(filepath):
    """íŒŒì¼ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
    try:
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"âŒ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {filepath} - {e}")
        return None

def analyze_python_imports(filepath):
    """Python íŒŒì¼ì˜ import êµ¬ë¬¸ ë¶„ì„"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        tree = ast.parse(content)
        imports = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ''
                for alias in node.names:
                    imports.append(f"{module}.{alias.name}")
        
        return imports
    except Exception as e:
        return []

def check_file_references(filepath, project_root):
    """í”„ë¡œì íŠ¸ ë‚´ì—ì„œ íŒŒì¼ì´ ì°¸ì¡°ë˜ëŠ”ì§€ í™•ì¸"""
    filename = os.path.basename(filepath)
    basename = os.path.splitext(filename)[0]
    
    references = []
    search_patterns = [
        filename,  # ì „ì²´ íŒŒì¼ëª…
        basename,  # í™•ì¥ì ì—†ëŠ” ì´ë¦„
        f"from {basename} import",  # Python import
        f"import {basename}",  # Python import
        f'"{filename}"',  # ë¬¸ìì—´ ì°¸ì¡°
        f"'{filename}'",  # ë¬¸ìì—´ ì°¸ì¡°
    ]
    
    # í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  Python íŒŒì¼ ê²€ìƒ‰
    for root, dirs, files in os.walk(project_root):
        # ë°±ì—… í´ë”ëŠ” ì œì™¸
        dirs[:] = [d for d in dirs if not any(x in d.lower() for x in ['backup', 'venv', '__pycache__', '.git'])]
        
        for file in files:
            if file.endswith(('.py', '.md', '.json', '.txt', '.bat', '.sh')):
                file_path = os.path.join(root, file)
                if file_path == filepath:  # ìê¸° ìì‹ ì€ ì œì™¸
                    continue
                
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    for pattern in search_patterns:
                        if pattern in content:
                            references.append({
                                'file': file_path,
                                'pattern': pattern,
                                'type': 'reference'
                            })
                            break
                except Exception:
                    continue
    
    return references

def verify_duplicate_safety(suggestions_file='duplicate_cleanup_suggestions.json'):
    """ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì•ˆì „ì„± ê²€ì¦"""
    
    if not os.path.exists(suggestions_file):
        print(f"âŒ ì œì•ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {suggestions_file}")
        return False
    
    with open(suggestions_file, 'r', encoding='utf-8') as f:
        suggestions = json.load(f)
    
    print("ğŸ” MACHO-GPT v3.4-mini ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì•ˆì „ì„± ê²€ì¦")
    print("=" * 80)
    
    project_root = os.getcwd()
    safety_issues = []
    verified_safe = []
    hash_verification_failed = []
    
    for i, suggestion in enumerate(suggestions, 1):
        keep_file = suggestion['keep']
        delete_files = suggestion['delete']
        
        print(f"\nğŸ” ê·¸ë£¹ {i}: {os.path.basename(keep_file)}")
        
        # 1. ìœ ì§€í•  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not os.path.exists(keep_file):
            safety_issues.append({
                'group': i,
                'issue': 'keep_file_missing',
                'details': f"ìœ ì§€í•  íŒŒì¼ì´ ì—†ìŒ: {keep_file}"
            })
            print(f"   âŒ ìœ„í—˜: ìœ ì§€í•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            continue
        
        keep_hash = calculate_file_hash(keep_file)
        if not keep_hash:
            hash_verification_failed.append({
                'group': i,
                'file': keep_file,
                'reason': 'hash_calculation_failed'
            })
            continue
        
        group_safe = True
        
        # 2. ì‚­ì œí•  íŒŒì¼ë“¤ì´ ì •ë§ ë™ì¼í•œì§€ ê²€ì¦
        for delete_file in delete_files:
            if not os.path.exists(delete_file):
                print(f"   â„¹ï¸ ì‚­ì œ ëŒ€ìƒ íŒŒì¼ì´ ì´ë¯¸ ì—†ìŒ: {os.path.basename(delete_file)}")
                continue
            
            delete_hash = calculate_file_hash(delete_file)
            if not delete_hash:
                hash_verification_failed.append({
                    'group': i,
                    'file': delete_file,
                    'reason': 'hash_calculation_failed'
                })
                continue
            
            # í•´ì‹œ ë¹„êµ
            if keep_hash != delete_hash:
                safety_issues.append({
                    'group': i,
                    'issue': 'hash_mismatch',
                    'details': f"íŒŒì¼ ë‚´ìš©ì´ ë‹¤ë¦„: {delete_file}",
                    'keep_file': keep_file,
                    'delete_file': delete_file
                })
                print(f"   âŒ ìœ„í—˜: íŒŒì¼ ë‚´ìš©ì´ ë‹¤ë¦„ - {os.path.basename(delete_file)}")
                group_safe = False
            else:
                print(f"   âœ… í•´ì‹œ ì¼ì¹˜: {os.path.basename(delete_file)}")
        
        # 3. ì‚­ì œí•  íŒŒì¼ë“¤ì´ ë‹¤ë¥¸ ê³³ì—ì„œ ì°¸ì¡°ë˜ëŠ”ì§€ í™•ì¸
        for delete_file in delete_files:
            if not os.path.exists(delete_file):
                continue
            
            references = check_file_references(delete_file, project_root)
            if references:
                safety_issues.append({
                    'group': i,
                    'issue': 'file_referenced',
                    'details': f"ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ì°¸ì¡°ë¨: {delete_file}",
                    'references': references[:3]  # ì²˜ìŒ 3ê°œë§Œ ì €ì¥
                })
                print(f"   âš ï¸ ì£¼ì˜: {os.path.basename(delete_file)}ì´(ê°€) {len(references)}ê³³ì—ì„œ ì°¸ì¡°ë¨")
                group_safe = False
        
        # 4. Python íŒŒì¼ì˜ ê²½ìš° import êµ¬ì¡° ë¶„ì„
        if keep_file.endswith('.py'):
            keep_imports = analyze_python_imports(keep_file)
            for delete_file in delete_files:
                if delete_file.endswith('.py') and os.path.exists(delete_file):
                    delete_imports = analyze_python_imports(delete_file)
                    if keep_imports != delete_imports:
                        safety_issues.append({
                            'group': i,
                            'issue': 'import_mismatch',
                            'details': f"Import êµ¬ì¡°ê°€ ë‹¤ë¦„: {delete_file}",
                            'keep_imports': keep_imports,
                            'delete_imports': delete_imports
                        })
                        print(f"   âš ï¸ ì£¼ì˜: Import êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ - {os.path.basename(delete_file)}")
        
        if group_safe:
            verified_safe.append(i)
            print(f"   âœ… ì•ˆì „: ì´ ê·¸ë£¹ì€ ì‚­ì œí•´ë„ ì•ˆì „í•¨")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ“Š ì•ˆì „ì„± ê²€ì¦ ê²°ê³¼")
    print("=" * 80)
    print(f"âœ… ì•ˆì „í•œ ê·¸ë£¹: {len(verified_safe)}ê°œ")
    print(f"âš ï¸ ìœ„í—˜í•œ ê·¸ë£¹: {len(safety_issues)}ê°œ")
    print(f"âŒ í•´ì‹œ ê²€ì¦ ì‹¤íŒ¨: {len(hash_verification_failed)}ê°œ")
    
    if safety_issues:
        print(f"\nâš ï¸ ë°œê²¬ëœ ì•ˆì „ì„± ë¬¸ì œ:")
        issue_counts = defaultdict(int)
        for issue in safety_issues:
            issue_counts[issue['issue']] += 1
        
        for issue_type, count in issue_counts.items():
            issue_names = {
                'keep_file_missing': 'ìœ ì§€í•  íŒŒì¼ ì—†ìŒ',
                'hash_mismatch': 'íŒŒì¼ ë‚´ìš© ë¶ˆì¼ì¹˜',
                'file_referenced': 'ë‹¤ë¥¸ ê³³ì—ì„œ ì°¸ì¡°ë¨',
                'import_mismatch': 'Import êµ¬ì¡° ë‹¤ë¦„'
            }
            print(f"   â€¢ {issue_names.get(issue_type, issue_type)}: {count}ê±´")
    
    if hash_verification_failed:
        print(f"\nâŒ í•´ì‹œ ê²€ì¦ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for failed in hash_verification_failed[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            print(f"   â€¢ ê·¸ë£¹ {failed['group']}: {os.path.basename(failed['file'])}")
    
    # ìƒì„¸ ë¬¸ì œ ë³´ê³ ì„œ ìƒì„±
    if safety_issues or hash_verification_failed:
        report_file = 'safety_verification_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'safety_issues': safety_issues,
                'hash_verification_failed': hash_verification_failed,
                'verified_safe_groups': verified_safe,
                'total_groups': len(suggestions)
            }, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ë¨: {report_file}")
    
    # ìµœì¢… ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:")
    if len(safety_issues) == 0 and len(hash_verification_failed) == 0:
        print("   âœ… ëª¨ë“  íŒŒì¼ì´ ì•ˆì „í•˜ê²Œ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print("   âœ… python execute_cleanup.py ì‹¤í–‰ ê°€ëŠ¥")
        return True
    elif len(safety_issues) <= 5:
        print("   âš ï¸ ì¼ë¶€ ìœ„í—˜ ìš”ì†Œê°€ ìˆì§€ë§Œ ëŒ€ë¶€ë¶„ ì•ˆì „í•©ë‹ˆë‹¤.")
        print("   âš ï¸ ë¬¸ì œê°€ ìˆëŠ” ê·¸ë£¹ì„ ì œì™¸í•˜ê³  ì‚­ì œ ê¶Œì¥")
        return False
    else:
        print("   âŒ ë§ì€ ì•ˆì „ì„± ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   âŒ ìˆ˜ë™ ê²€í†  í›„ ê°œë³„ ì‚­ì œ ê¶Œì¥")
        return False

def create_safe_cleanup_suggestions():
    """ì•ˆì „í•œ íŒŒì¼ë“¤ë§Œ í¬í•¨í•œ ì •ë¦¬ ì œì•ˆ ìƒì„±"""
    
    # ì›ë³¸ ì œì•ˆ ë¡œë“œ
    with open('duplicate_cleanup_suggestions.json', 'r', encoding='utf-8') as f:
        original_suggestions = json.load(f)
    
    # ì•ˆì „ì„± ê²€ì¦ ê²°ê³¼ ë¡œë“œ
    if os.path.exists('safety_verification_report.json'):
        with open('safety_verification_report.json', 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        safe_groups = set(report['verified_safe_groups'])
        problematic_groups = set()
        
        for issue in report['safety_issues']:
            problematic_groups.add(issue['group'])
        
        # ì•ˆì „í•œ ê·¸ë£¹ë§Œ í•„í„°ë§
        safe_suggestions = []
        for i, suggestion in enumerate(original_suggestions, 1):
            if i in safe_groups and i not in problematic_groups:
                safe_suggestions.append(suggestion)
        
        # ì•ˆì „í•œ ì œì•ˆë§Œ ì €ì¥
        safe_file = 'safe_cleanup_suggestions.json'
        with open(safe_file, 'w', encoding='utf-8') as f:
            json.dump(safe_suggestions, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ì•ˆì „í•œ ì‚­ì œ ì œì•ˆ íŒŒì¼ ìƒì„±: {safe_file}")
        print(f"   ğŸ“Š ì›ë³¸: {len(original_suggestions)}ê°œ â†’ ì•ˆì „: {len(safe_suggestions)}ê°œ")
        
        return safe_file
    
    return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ MACHO-GPT v3.4-mini ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì•ˆì „ì„± ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    
    # ì•ˆì „ì„± ê²€ì¦ ì‹¤í–‰
    is_safe = verify_duplicate_safety()
    
    # ì•ˆì „í•œ ì œì•ˆ íŒŒì¼ ìƒì„±
    if not is_safe:
        safe_file = create_safe_cleanup_suggestions()
        if safe_file:
            print(f"\nğŸ”§ ì•ˆì „í•œ íŒŒì¼ë§Œ ì‚­ì œí•˜ë ¤ë©´:")
            print(f"   python execute_cleanup.py --suggestions-file {safe_file}")
    
    print(f"\nğŸ“‹ ê²€ì¦ ì™„ë£Œ. ê²°ê³¼ë¥¼ ê²€í† í•œ í›„ ì‚­ì œë¥¼ ì§„í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main() 
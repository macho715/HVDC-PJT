#!/usr/bin/env python3
"""
HVDC ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ ì‹¤ì œ ì‚¬ìš© ë°ëª¨ v3.0.0
- ì‹¤ì œ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ë™ì‘ ê²€ì¦
- MACHO-GPT v3.4-mini í‘œì¤€ ì¤€ìˆ˜
- ì™„ì „ í†µí•© ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ì‹œì—°
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ ì‹¤ì œ ì‚¬ìš© ë°ëª¨"""
    print("ğŸ”Œ HVDC ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ ì‹¤ì œ ì‚¬ìš© ë°ëª¨ v3.0.0")
    print("=" * 70)
    
    demo_results = {
        'timestamp': datetime.now().isoformat(),
        'system_version': '3.0.0',
        'demos': {},
        'summary': {}
    }
    
    # 1. ë§¤í•‘ ê·œì¹™ ì‹œìŠ¤í…œ ë°ëª¨
    print("\nğŸ“‹ 1. ë§¤í•‘ ê·œì¹™ ì‹œìŠ¤í…œ ë°ëª¨")
    mapping_demo = demo_mapping_rules()
    demo_results['demos']['mapping_rules'] = mapping_demo
    
    # 2. ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë°ëª¨
    print("\nğŸ—ï¸ 2. ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë°ëª¨")
    schema_demo = demo_ontology_schema()
    demo_results['demos']['ontology_schema'] = schema_demo
    
    # 3. OFCO ë§¤í•‘ ë°ëª¨
    print("\nğŸ’° 3. OFCO ë§¤í•‘ ë°ëª¨")
    ofco_demo = demo_ofco_mapping()
    demo_results['demos']['ofco_mapping'] = ofco_demo
    
    # 4. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë°ëª¨
    print("\nğŸ”„ 4. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë°ëª¨")
    pipeline_demo = demo_data_pipeline()
    demo_results['demos']['data_pipeline'] = pipeline_demo
    
    # 5. SPARQL ì¿¼ë¦¬ ë°ëª¨
    print("\nğŸ” 5. SPARQL ì¿¼ë¦¬ ë°ëª¨")
    sparql_demo = demo_sparql_queries()
    demo_results['demos']['sparql_queries'] = sparql_demo
    
    # 6. í†µí•© ìš”ì•½
    print("\nğŸ“Š 6. í†µí•© ìš”ì•½")
    summary = generate_demo_summary(demo_results)
    demo_results['summary'] = summary
    
    # ê²°ê³¼ ì €ì¥
    save_demo_results(demo_results)
    
    print(f"\nğŸ‰ ë°ëª¨ ì™„ë£Œ! ì´ {len(demo_results['demos'])}ê°œ ë°ëª¨ ì‹¤í–‰")
    print(f"   ğŸ“„ ê²°ê³¼ íŒŒì¼: hvdc_ontology_demo_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    return demo_results

def demo_mapping_rules():
    """ë§¤í•‘ ê·œì¹™ ì‹œìŠ¤í…œ ë°ëª¨"""
    print("   ğŸ”„ ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì¤‘...")
    
    try:
        # ë§¤í•‘ ê·œì¹™ ë¡œë“œ
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if not mapping_file.exists():
            return {'success': False, 'error': 'ë§¤í•‘ ê·œì¹™ íŒŒì¼ ì—†ìŒ'}
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_rules = json.load(f)
        
        field_mappings = mapping_rules.get('field_mappings', {})
        property_mappings = mapping_rules.get('property_mappings', {})
        
        # ì£¼ìš” ë§¤í•‘ ì˜ˆì‹œ
        key_mappings = {
            'Case_No': field_mappings.get('Case_No', 'N/A'),
            'Date': field_mappings.get('Date', 'N/A'),
            'Location': field_mappings.get('Location', 'N/A'),
            'Qty': field_mappings.get('Qty', 'N/A'),
            'Amount': field_mappings.get('Amount', 'N/A')
        }
        
        print("   âœ… í•µì‹¬ í•„ë“œ ë§¤í•‘:")
        for field, predicate in key_mappings.items():
            print(f"      {field} â†’ {predicate}")
        
        # ì†ì„± ë§¤í•‘ ì •ë³´
        required_fields = [
            field for field, props in property_mappings.items()
            if props.get('required', False)
        ]
        
        print(f"   âœ… í•„ìˆ˜ í•„ë“œ: {len(required_fields)}ê°œ")
        print(f"   âœ… ì „ì²´ ë§¤í•‘: {len(field_mappings)}ê°œ")
        
        return {
            'success': True,
            'field_mappings_count': len(field_mappings),
            'property_mappings_count': len(property_mappings),
            'required_fields_count': len(required_fields),
            'key_mappings': key_mappings
        }
        
    except Exception as e:
        logger.error(f"ë§¤í•‘ ê·œì¹™ ë°ëª¨ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

def demo_ontology_schema():
    """ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë°ëª¨"""
    print("   ğŸ—ï¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘...")
    
    try:
        schema_file = Path("hvdc_integrated_ontology_schema.ttl")
        if not schema_file.exists():
            return {'success': False, 'error': 'ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì—†ìŒ'}
        
        with open(schema_file, 'r', encoding='utf-8') as f:
            schema_content = f.read()
        
        # ìŠ¤í‚¤ë§ˆ êµ¬ì„± ìš”ì†Œ ë¶„ì„
        components = {
            'classes': schema_content.count('a owl:Class'),
            'properties': schema_content.count('a owl:DatatypeProperty') + schema_content.count('a owl:ObjectProperty'),
            'korean_labels': schema_content.count('@ko'),
            'hierarchy_relations': schema_content.count('rdfs:subClassOf'),
            'instances': schema_content.count('a ex:')
        }
        
        print("   âœ… ìŠ¤í‚¤ë§ˆ êµ¬ì„± ìš”ì†Œ:")
        for component, count in components.items():
            print(f"      {component}: {count}ê°œ")
        
        # í•µì‹¬ í´ë˜ìŠ¤ í™•ì¸
        key_classes = [
            'TransportEvent', 'Warehouse', 'Site', 'Invoice', 
            'Case', 'StockSnapshot', 'CostCenter'
        ]
        
        found_classes = []
        for cls in key_classes:
            if f'ex:{cls}' in schema_content:
                found_classes.append(cls)
        
        print(f"   âœ… í•µì‹¬ í´ë˜ìŠ¤: {len(found_classes)}/{len(key_classes)}ê°œ í™•ì¸")
        
        return {
            'success': True,
            'schema_components': components,
            'key_classes_found': len(found_classes),
            'key_classes_total': len(key_classes),
            'found_classes': found_classes
        }
        
    except Exception as e:
        logger.error(f"ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë°ëª¨ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

def demo_ofco_mapping():
    """OFCO ë§¤í•‘ ë°ëª¨"""
    print("   ğŸ’° OFCO ë§¤í•‘ ê·œì¹™ ì‹œì—° ì¤‘...")
    
    try:
        # ë§¤í•‘ ê·œì¹™ ë¡œë“œ
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if not mapping_file.exists():
            return {'success': False, 'error': 'ë§¤í•‘ ê·œì¹™ íŒŒì¼ ì—†ìŒ'}
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_rules = json.load(f)
        
        ofco_rules = mapping_rules.get('ofco_mapping_rules', {})
        cost_centers = ofco_rules.get('cost_centers', {})
        mapping_rules_list = ofco_rules.get('mapping_rules', [])
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘ ì‹œì—°
        test_invoices = [
            "Berthing and Pilot Arrangement - Port Services",
            "Cargo Clearance and Documentation",
            "OFCO 10% Handling Fee for Container",
            "Forklift Operation Charges",
            "Yard Storage Monthly Rental Fee",
            "MGO Fuel Supply for Vessel",
            "Fresh Water Supply Services",
            "Port Transit Channel Charges"
        ]
        
        print("   âœ… OFCO ë§¤í•‘ ì‹œì—°:")
        mapped_count = 0
        
        for invoice_text in test_invoices:
            matched_rule = find_best_ofco_match(invoice_text, mapping_rules_list)
            if matched_rule:
                cost_center = matched_rule.get('cost_center_a', 'N/A')
                print(f"      '{invoice_text[:30]}...' â†’ {cost_center}")
                mapped_count += 1
            else:
                print(f"      '{invoice_text[:30]}...' â†’ ë§¤í•‘ ì‹¤íŒ¨")
        
        print(f"   âœ… ë§¤í•‘ ì„±ê³µë¥ : {mapped_count}/{len(test_invoices)} ({mapped_count/len(test_invoices)*100:.1f}%)")
        print(f"   âœ… ë¹„ìš© ì„¼í„°: {len(cost_centers)}ê°œ")
        print(f"   âœ… ë§¤í•‘ ê·œì¹™: {len(mapping_rules_list)}ê°œ")
        
        return {
            'success': True,
            'cost_centers_count': len(cost_centers),
            'mapping_rules_count': len(mapping_rules_list),
            'test_invoices_count': len(test_invoices),
            'mapped_count': mapped_count,
            'success_rate': mapped_count / len(test_invoices) * 100
        }
        
    except Exception as e:
        logger.error(f"OFCO ë§¤í•‘ ë°ëª¨ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

def demo_data_pipeline():
    """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
    print("   ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì—° ì¤‘...")
    
    try:
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_data = pd.DataFrame({
            'Case_No': ['HVDC-DEMO-001', 'HVDC-DEMO-002', 'HVDC-DEMO-003'],
            'Date': pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-17']),
            'Location': ['DSV Indoor', 'DSV Outdoor', 'AGI Site'],
            'Qty': [10, 20, 15],
            'Amount': [1000.0, 2000.0, 1500.0],
            'Category': ['HE', 'SIM', 'HE'],
            'Vendor': ['HITACHI', 'SIMENSE', 'HITACHI'],
            'Logistics Flow Code': [1, 2, 3],
            'wh handling': [0, 1, 2],
            'pkg': [1, 2, 1],
            'Status_Location_Date': pd.to_datetime(['2024-01-15', None, '2024-01-17']),
            'Stack_Status': ['STACKED', None, 'READY']
        })
        
        print("   âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„±:")
        print(f"      í–‰: {len(sample_data)}")
        print(f"      ì—´: {len(sample_data.columns)}")
        
        # ë°ì´í„° ì •ê·œí™” ì‹œì—°
        original_data = sample_data.copy()
        
        # 1. NULL PKG ë³´ì •
        sample_data['pkg'] = sample_data['pkg'].fillna(1)
        
        # 2. Flow Code ì •ê·œí™” (6 â†’ 3)
        sample_data.loc[sample_data['Logistics Flow Code'] == 6, 'Logistics Flow Code'] = 3
        
        # 3. ë²¤ë” ì •ê·œí™”
        vendor_mapping = {'SIMENSE': 'SIM', 'HITACHI': 'HE'}
        sample_data['Vendor'] = sample_data['Vendor'].map(vendor_mapping).fillna(sample_data['Vendor'])
        
        # 4. ê²°ì¸¡ê°’ ì²˜ë¦¬
        null_status_location = sample_data['Status_Location_Date'].isna().sum()
        null_stack_status = sample_data['Stack_Status'].isna().sum()
        
        print("   âœ… ë°ì´í„° ì •ê·œí™” ì™„ë£Œ:")
        print(f"      NULL Status_Location_Date: {null_status_location}ê°œ")
        print(f"      NULL Stack_Status: {null_stack_status}ê°œ")
        print(f"      ë²¤ë” ì •ê·œí™”: SIMENSE â†’ SIM, HITACHI â†’ HE")
        
        # 5. ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œì—°
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if mapping_file.exists():
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_rules = json.load(f)
            
            field_mappings = mapping_rules.get('field_mappings', {})
            mapped_fields = []
            
            for col in sample_data.columns:
                if col in field_mappings:
                    mapped_fields.append(col)
            
            print(f"   âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘: {len(mapped_fields)}/{len(sample_data.columns)}ê°œ í•„ë“œ")
        
        return {
            'success': True,
            'sample_data_rows': len(sample_data),
            'sample_data_columns': len(sample_data.columns),
            'null_status_location_date': null_status_location,
            'null_stack_status': null_stack_status,
            'mapped_fields': len(mapped_fields) if 'mapped_fields' in locals() else 0
        }
        
    except Exception as e:
        logger.error(f"ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

def demo_sparql_queries():
    """SPARQL ì¿¼ë¦¬ ë°ëª¨"""
    print("   ğŸ” SPARQL ì¿¼ë¦¬ ì‹œì—° ì¤‘...")
    
    try:
        # ë§¤í•‘ ê·œì¹™ì—ì„œ SPARQL í…œí”Œë¦¿ ë¡œë“œ
        mapping_file = Path("hvdc_integrated_mapping_rules_v3.0.json")
        if not mapping_file.exists():
            return {'success': False, 'error': 'ë§¤í•‘ ê·œì¹™ íŒŒì¼ ì—†ìŒ'}
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_rules = json.load(f)
        
        sparql_templates = mapping_rules.get('sparql_templates', {})
        
        # ê¸°ë³¸ ì¿¼ë¦¬ í…œí”Œë¦¿ ì‹œì—°
        basic_queries = sparql_templates.get('basic_queries', {})
        advanced_queries = sparql_templates.get('advanced_queries', {})
        
        print("   âœ… SPARQL ì¿¼ë¦¬ í…œí”Œë¦¿:")
        print(f"      ê¸°ë³¸ ì¿¼ë¦¬: {len(basic_queries)}ê°œ")
        print(f"      ê³ ê¸‰ ì¿¼ë¦¬: {len(advanced_queries)}ê°œ")
        
        # ìƒ˜í”Œ ì¿¼ë¦¬ ìƒì„±
        namespace = mapping_rules.get('namespace', 'http://samsung.com/project-logistics#')
        
        sample_queries = {}
        
        # ì›”ë³„ ìš”ì•½ ì¿¼ë¦¬
        monthly_summary_query = f"""
        PREFIX ex: <{namespace}>
        SELECT ?month ?warehouse (COUNT(?event) AS ?eventCount) (SUM(?amount) AS ?totalAmount)
        WHERE {{
          ?event rdf:type ex:TransportEvent ;
                 ex:hasDate ?date ;
                 ex:hasLocation ?warehouse ;
                 ex:hasAmount ?amount .
          BIND(SUBSTR(STR(?date), 1, 7) AS ?month)
        }}
        GROUP BY ?month ?warehouse
        ORDER BY ?month ?warehouse
        """
        
        sample_queries['monthly_summary'] = monthly_summary_query
        
        # ì°½ê³ ë³„ í†µê³„ ì¿¼ë¦¬
        warehouse_stats_query = f"""
        PREFIX ex: <{namespace}>
        SELECT ?warehouse (COUNT(?event) AS ?totalEvents) (AVG(?quantity) AS ?avgQuantity)
        WHERE {{
          ?event rdf:type ex:TransportEvent ;
                 ex:hasLocation ?warehouse ;
                 ex:hasQuantity ?quantity .
        }}
        GROUP BY ?warehouse
        ORDER BY DESC(?totalEvents)
        """
        
        sample_queries['warehouse_statistics'] = warehouse_stats_query
        
        print("   âœ… ìƒì„±ëœ ìƒ˜í”Œ ì¿¼ë¦¬:")
        for query_name, query in sample_queries.items():
            print(f"      {query_name}: {len(query.split('\\n'))}ì¤„")
        
        return {
            'success': True,
            'basic_queries_count': len(basic_queries),
            'advanced_queries_count': len(advanced_queries),
            'sample_queries_count': len(sample_queries),
            'namespace': namespace
        }
        
    except Exception as e:
        logger.error(f"SPARQL ì¿¼ë¦¬ ë°ëª¨ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

def find_best_ofco_match(text, rules):
    """OFCO ë§¤í•‘ ê·œì¹™ì—ì„œ ìµœì  ë§¤ì¹˜ ì°¾ê¸°"""
    import re
    
    best_match = None
    best_priority = float('inf')
    
    for rule in rules:
        pattern = rule.get('pattern', '')
        priority = rule.get('priority', 999)
        
        if pattern and re.search(pattern, text, re.IGNORECASE):
            if priority < best_priority:
                best_priority = priority
                best_match = rule
    
    return best_match

def generate_demo_summary(demo_results):
    """ë°ëª¨ ìš”ì•½ ìƒì„±"""
    demos = demo_results.get('demos', {})
    
    total_demos = len(demos)
    successful_demos = sum(1 for demo in demos.values() if demo.get('success', False))
    
    return {
        'total_demos': total_demos,
        'successful_demos': successful_demos,
        'failed_demos': total_demos - successful_demos,
        'success_rate': (successful_demos / total_demos * 100) if total_demos > 0 else 0,
        'demo_details': {
            name: {
                'status': 'SUCCESS' if demo.get('success', False) else 'FAILED',
                'has_error': 'error' in demo
            }
            for name, demo in demos.items()
        }
    }

def save_demo_results(demo_results):
    """ë°ëª¨ ê²°ê³¼ ì €ì¥"""
    try:
        output_file = f"hvdc_ontology_demo_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(demo_results, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ ë°ëª¨ ê²°ê³¼ ì €ì¥: {output_file}")
        
    except Exception as e:
        logger.error(f"ë°ëª¨ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    demo_results = main()
    
    # ì„±ê³µë¥ ì— ë”°ë¥¸ ì¢…ë£Œ
    success_rate = demo_results.get('summary', {}).get('success_rate', 0)
    if success_rate >= 80:
        print(f"âœ… ë°ëª¨ ì„±ê³µ! (ì„±ê³µë¥ : {success_rate:.1f}%)")
        exit(0)
    else:
        print(f"âŒ ë°ëª¨ ì‹¤íŒ¨! (ì„±ê³µë¥ : {success_rate:.1f}%)")
        exit(1) 
#!/usr/bin/env python3
"""
MACHO-GPT v3.4-mini - HVDC 3D Network Visualizer CLI
ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ 3D ì‹œê°í™” ëª…ë ¹ì–´ ì¸í„°í˜ì´ìŠ¤

Commands:
/visualize_3d_network --data=file.xlsx --mode=LATTICE
/create_logistics_dashboard --interactive=true
/export_visualization --format=html --filename=network_report
/analyze_flow_patterns --depth=3
/generate_kpi_dashboard --realtime=true
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import logging
import networkx as nx

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from logi_3d_network_visualizer import LogiNetworkVisualizer


class Logi3DCLI:
    """HVDC 3D Network Visualizer CLI"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("logs/3d_network_cli.log"),
                logging.StreamHandler(),
            ],
        )

    def visualize_3d_network(
        self,
        data_source: str,
        mode: str = "LATTICE",
        title: str = None,
        interactive: bool = True,
    ) -> dict:
        """
        3D ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ìƒì„±

        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ
            mode: MACHO-GPT ëª¨ë“œ
            title: ê·¸ë˜í”„ ì œëª©
            interactive: ì¸í„°ë™í‹°ë¸Œ ê¸°ëŠ¥ í™œì„±í™”

        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        try:
            print(f"ğŸš€ MACHO-GPT v3.4-mini - 3D Network Visualization")
            print(f"ğŸ“Š Mode: {mode} | Data: {data_source}")
            print("=" * 60)

            # ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            visualizer = LogiNetworkVisualizer(mode=mode)

            # ë°ì´í„° ë¡œë“œ
            print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
            load_result = visualizer.load_logistics_data(data_source)

            if load_result["status"] != "SUCCESS":
                return {
                    "status": "FAIL",
                    "confidence": 0.0,
                    "error": f"Data loading failed: {load_result.get('error', 'Unknown error')}",
                    "mode": mode,
                    "triggers": ["/switch_mode ZERO"],
                    "next_cmds": ["/check_data_source", "/switch_mode ZERO"],
                }

            print(
                f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {load_result['nodes_count']} ë…¸ë“œ, {load_result['edges_count']} ì—£ì§€"
            )

            # 3D ì‹œê°í™” ìƒì„±
            print("ğŸ¨ 3D ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ìƒì„± ì¤‘...")
            graph_title = title or f"HVDC Logistics Network 3D ({mode} Mode)"
            fig = visualizer.create_3d_network_visualization(
                title=graph_title, interactive=interactive
            )

            # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"HVDC_3D_Network_{mode}_{timestamp}"

            print("ğŸ’¾ ì‹œê°í™” ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘...")
            export_result = visualizer.export_visualization(fig, filename, "html")

            if export_result["status"] == "SUCCESS":
                print(f"âœ… ì‹œê°í™” ì™„ë£Œ: {export_result['filepath']}")

                result = {
                    "status": "SUCCESS",
                    "confidence": 0.95,
                    "mode": mode,
                    "filepath": export_result["filepath"],
                    "nodes_count": load_result["nodes_count"],
                    "edges_count": load_result["edges_count"],
                    "triggers": load_result["triggers"],
                    "next_cmds": [
                        "/open_file " + export_result["filepath"],
                        "/create_logistics_dashboard",
                        "/analyze_flow_patterns",
                    ],
                }
            else:
                result = export_result

            return result

        except Exception as e:
            self.logger.error(f"3D network visualization failed: {e}")
            return {
                "status": "FAIL",
                "confidence": 0.0,
                "error": str(e),
                "mode": mode,
                "triggers": ["/switch_mode ZERO"],
                "next_cmds": ["/check_system_status", "/switch_mode ZERO"],
            }

    def create_logistics_dashboard(
        self, data_source: str, mode: str = "LATTICE", interactive: bool = True
    ) -> dict:
        """
        ë¬¼ë¥˜ ëŒ€ì‹œë³´ë“œ ìƒì„±

        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ
            mode: MACHO-GPT ëª¨ë“œ
            interactive: ì¸í„°ë™í‹°ë¸Œ ê¸°ëŠ¥ í™œì„±í™”

        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        try:
            print(f"ğŸ“Š MACHO-GPT v3.4-mini - Logistics Dashboard")
            print(f"ğŸ“ˆ Mode: {mode} | Data: {data_source}")
            print("=" * 60)

            # ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            visualizer = LogiNetworkVisualizer(mode=mode)

            # ë°ì´í„° ë¡œë“œ
            print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
            load_result = visualizer.load_logistics_data(data_source)

            if load_result["status"] != "SUCCESS":
                return {
                    "status": "FAIL",
                    "confidence": 0.0,
                    "error": f"Data loading failed: {load_result.get('error', 'Unknown error')}",
                    "mode": mode,
                    "triggers": ["/switch_mode ZERO"],
                    "next_cmds": ["/check_data_source", "/switch_mode ZERO"],
                }

            # ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
            if isinstance(data_source, str):
                if data_source.endswith(".xlsx"):
                    df = pd.read_excel(data_source)
                elif data_source.endswith(".csv"):
                    df = pd.read_csv(data_source)
                else:
                    raise ValueError(f"Unsupported file format: {data_source}")
            else:
                df = data_source

            # ëŒ€ì‹œë³´ë“œ ìƒì„±
            print("ğŸ“Š ë¬¼ë¥˜ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
            fig = visualizer.create_logistics_dashboard(df)

            # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"HVDC_Dashboard_{mode}_{timestamp}"

            print("ğŸ’¾ ëŒ€ì‹œë³´ë“œ ë‚´ë³´ë‚´ê¸° ì¤‘...")
            export_result = visualizer.export_visualization(fig, filename, "html")

            if export_result["status"] == "SUCCESS":
                print(f"âœ… ëŒ€ì‹œë³´ë“œ ì™„ë£Œ: {export_result['filepath']}")

                result = {
                    "status": "SUCCESS",
                    "confidence": 0.95,
                    "mode": mode,
                    "filepath": export_result["filepath"],
                    "dashboard_type": "logistics_multi_panel",
                    "triggers": load_result["triggers"],
                    "next_cmds": [
                        "/open_file " + export_result["filepath"],
                        "/analyze_warehouse_performance",
                        "/generate_kpi_report",
                    ],
                }
            else:
                result = export_result

            return result

        except Exception as e:
            self.logger.error(f"Logistics dashboard creation failed: {e}")
            return {
                "status": "FAIL",
                "confidence": 0.0,
                "error": str(e),
                "mode": mode,
                "triggers": ["/switch_mode ZERO"],
                "next_cmds": ["/check_system_status", "/switch_mode ZERO"],
            }

    def analyze_flow_patterns(
        self, data_source: str, mode: str = "LATTICE", depth: int = 3
    ) -> dict:
        """
        ë¬¼ë¥˜ í”Œë¡œìš° íŒ¨í„´ ë¶„ì„

        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ
            mode: MACHO-GPT ëª¨ë“œ
            depth: ë¶„ì„ ê¹Šì´

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼
        """
        try:
            print(f"ğŸ” MACHO-GPT v3.4-mini - Flow Pattern Analysis")
            print(f"ğŸ“Š Mode: {mode} | Data: {data_source} | Depth: {depth}")
            print("=" * 60)

            # ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            visualizer = LogiNetworkVisualizer(mode=mode)

            # ë°ì´í„° ë¡œë“œ
            print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
            load_result = visualizer.load_logistics_data(data_source)

            if load_result["status"] != "SUCCESS":
                return {
                    "status": "FAIL",
                    "confidence": 0.0,
                    "error": f"Data loading failed: {load_result.get('error', 'Unknown error')}",
                    "mode": mode,
                    "triggers": ["/switch_mode ZERO"],
                    "next_cmds": ["/check_data_source", "/switch_mode ZERO"],
                }

            # í”Œë¡œìš° íŒ¨í„´ ë¶„ì„
            print("ğŸ” í”Œë¡œìš° íŒ¨í„´ ë¶„ì„ ì¤‘...")

            # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°
            G = visualizer.G
            metrics = {
                "total_nodes": len(G.nodes()),
                "total_edges": len(G.edges()),
                "density": nx.density(G),
                "average_clustering": nx.average_clustering(G),
                "average_shortest_path": (
                    nx.average_shortest_path_length(G)
                    if nx.is_connected(G)
                    else float("inf")
                ),
                "node_connectivity": nx.node_connectivity(G),
                "edge_connectivity": nx.edge_connectivity(G),
            }

            # ì¤‘ì‹¬ì„± ë¶„ì„
            centrality_metrics = {
                "degree_centrality": nx.degree_centrality(G),
                "betweenness_centrality": nx.betweenness_centrality(G),
                "closeness_centrality": nx.closeness_centrality(G),
                "eigenvector_centrality": nx.eigenvector_centrality_numpy(G),
            }

            # ì»¤ë®¤ë‹ˆí‹° íƒì§€
            communities = list(nx.community.greedy_modularity_communities(G))

            # ê²°ê³¼ ì •ë¦¬
            analysis_result = {
                "network_metrics": metrics,
                "centrality_analysis": centrality_metrics,
                "communities": {
                    "count": len(communities),
                    "sizes": [len(comm) for comm in communities],
                    "modularity": nx.community.modularity(G, communities),
                },
                "flow_patterns": {
                    "strongest_connections": sorted(
                        [(u, v, d["weight"]) for u, v, d in G.edges(data=True)],
                        key=lambda x: x[2],
                        reverse=True,
                    )[:10],
                    "bottlenecks": self._identify_bottlenecks(G),
                    "critical_paths": self._find_critical_paths(G),
                },
            }

            print("âœ… í”Œë¡œìš° íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
            print(f"ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë°€ë„: {metrics['density']:.3f}")
            print(f"ğŸ”— í‰ê·  í´ëŸ¬ìŠ¤í„°ë§: {metrics['average_clustering']:.3f}")
            print(f"ğŸ˜ï¸ ì»¤ë®¤ë‹ˆí‹° ìˆ˜: {len(communities)}")

            result = {
                "status": "SUCCESS",
                "confidence": 0.95,
                "mode": mode,
                "analysis": analysis_result,
                "triggers": ["/optimize_network_flow"],
                "next_cmds": [
                    "/visualize_flow_patterns",
                    "/generate_optimization_report",
                    "/create_network_metrics_dashboard",
                ],
            }

            return result

        except Exception as e:
            self.logger.error(f"Flow pattern analysis failed: {e}")
            return {
                "status": "FAIL",
                "confidence": 0.0,
                "error": str(e),
                "mode": mode,
                "triggers": ["/switch_mode ZERO"],
                "next_cmds": ["/check_system_status", "/switch_mode ZERO"],
            }

    def _identify_bottlenecks(self, G):
        """ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ì§€ì  ì‹ë³„"""
        bottlenecks = []

        # ë†’ì€ betweenness centralityë¥¼ ê°€ì§„ ë…¸ë“œë“¤
        betweenness = nx.betweenness_centrality(G)
        high_betweenness = [
            (node, centrality)
            for node, centrality in betweenness.items()
            if centrality > 0.1
        ]

        # ë‚®ì€ ì—°ê²°ì„±ì„ ê°€ì§„ ì—£ì§€ë“¤
        edge_betweenness = nx.edge_betweenness_centrality(G)
        low_connectivity_edges = [
            (u, v, centrality)
            for (u, v), centrality in edge_betweenness.items()
            if centrality > 0.05
        ]

        bottlenecks = {
            "high_betweenness_nodes": high_betweenness,
            "low_connectivity_edges": low_connectivity_edges,
        }

        return bottlenecks

    def _find_critical_paths(self, G):
        """ì¤‘ìš” ê²½ë¡œ íƒì§€"""
        critical_paths = []

        # ëª¨ë“  ë…¸ë“œ ìŒ ê°„ì˜ ìµœë‹¨ ê²½ë¡œ
        for source in G.nodes():
            for target in G.nodes():
                if source != target:
                    try:
                        path = nx.shortest_path(G, source, target, weight="weight")
                        if len(path) > 2:  # ì¤‘ê°„ ë…¸ë“œê°€ ìˆëŠ” ê²½ë¡œë§Œ
                            critical_paths.append(
                                {
                                    "source": source,
                                    "target": target,
                                    "path": path,
                                    "length": len(path),
                                    "weight": sum(
                                        G[path[i]][path[i + 1]]["weight"]
                                        for i in range(len(path) - 1)
                                    ),
                                }
                            )
                    except nx.NetworkXNoPath:
                        continue

        # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        critical_paths.sort(key=lambda x: x["weight"], reverse=True)

        return critical_paths[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜

    def generate_kpi_dashboard(
        self, data_source: str, mode: str = "RHYTHM", realtime: bool = False
    ) -> dict:
        """
        KPI ëŒ€ì‹œë³´ë“œ ìƒì„±

        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ
            mode: MACHO-GPT ëª¨ë“œ (RHYTHM ê¶Œì¥)
            realtime: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì—¬ë¶€

        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        try:
            print(f"ğŸ“Š MACHO-GPT v3.4-mini - KPI Dashboard")
            print(f"ğŸ“ˆ Mode: {mode} | Data: {data_source} | Realtime: {realtime}")
            print("=" * 60)

            # ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            visualizer = LogiNetworkVisualizer(mode=mode)

            # ë°ì´í„° ë¡œë“œ
            print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
            load_result = visualizer.load_logistics_data(data_source)

            if load_result["status"] != "SUCCESS":
                return {
                    "status": "FAIL",
                    "confidence": 0.0,
                    "error": f"Data loading failed: {load_result.get('error', 'Unknown error')}",
                    "mode": mode,
                    "triggers": ["/switch_mode ZERO"],
                    "next_cmds": ["/check_data_source", "/switch_mode ZERO"],
                }

            # KPI ê³„ì‚°
            print("ğŸ“Š KPI ê³„ì‚° ì¤‘...")

            # ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
            if isinstance(data_source, str):
                if data_source.endswith(".xlsx"):
                    df = pd.read_excel(data_source)
                elif data_source.endswith(".csv"):
                    df = pd.read_csv(data_source)
                else:
                    raise ValueError(f"Unsupported file format: {data_source}")
            else:
                df = data_source

            # KPI ë©”íŠ¸ë¦­ ê³„ì‚°
            kpi_metrics = {
                "total_items": len(df),
                "warehouse_utilization": self._calculate_warehouse_utilization(df),
                "flow_efficiency": self._calculate_flow_efficiency(df),
                "delivery_performance": self._calculate_delivery_performance(df),
                "cost_metrics": self._calculate_cost_metrics(df),
                "compliance_score": self._calculate_compliance_score(df),
            }

            print("âœ… KPI ê³„ì‚° ì™„ë£Œ")
            print(f"ğŸ“¦ ì´ ì•„ì´í…œ: {kpi_metrics['total_items']}")
            print(f"ğŸ­ ì°½ê³  í™œìš©ë„: {kpi_metrics['warehouse_utilization']:.2%}")
            print(f"âš¡ í”Œë¡œìš° íš¨ìœ¨ì„±: {kpi_metrics['flow_efficiency']:.2%}")

            result = {
                "status": "SUCCESS",
                "confidence": 0.95,
                "mode": mode,
                "kpi_metrics": kpi_metrics,
                "realtime_enabled": realtime,
                "triggers": ["/monitor_kpi_thresholds"],
                "next_cmds": [
                    "/create_kpi_visualization",
                    "/set_kpi_alerts",
                    "/generate_performance_report",
                ],
            }

            return result

        except Exception as e:
            self.logger.error(f"KPI dashboard generation failed: {e}")
            return {
                "status": "FAIL",
                "confidence": 0.0,
                "error": str(e),
                "mode": mode,
                "triggers": ["/switch_mode ZERO"],
                "next_cmds": ["/check_system_status", "/switch_mode ZERO"],
            }

    def _calculate_warehouse_utilization(self, df):
        """ì°½ê³  í™œìš©ë„ ê³„ì‚°"""
        warehouse_columns = [
            "DSV Indoor",
            "DSV Outdoor",
            "DSV Al Markaz",
            "DSV MZP",
            "AAA Storage",
        ]
        total_capacity = 1000 * len(warehouse_columns)  # ê°€ì •ëœ ìš©ëŸ‰

        utilized = 0
        for col in warehouse_columns:
            if col in df.columns:
                utilized += len(df[df[col].notna() & (df[col] != "")])

        return utilized / total_capacity if total_capacity > 0 else 0.0

    def _calculate_flow_efficiency(self, df):
        """í”Œë¡œìš° íš¨ìœ¨ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ íš¨ìœ¨ì„± ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        total_items = len(df)
        if total_items == 0:
            return 0.0

        # ì™„ë£Œëœ ì•„ì´í…œ ë¹„ìœ¨ (ì˜ˆì‹œ)
        completed_items = (
            len(df[df["Category"].notna()]) if "Category" in df.columns else total_items
        )
        return completed_items / total_items

    def _calculate_delivery_performance(self, df):
        """ë°°ì†¡ ì„±ê³¼ ê³„ì‚°"""
        # ë°°ì†¡ ì„±ê³¼ ê³„ì‚° (ì˜ˆì‹œ)
        return 0.85  # 85% ì„±ê³µë¥  ê°€ì •

    def _calculate_cost_metrics(self, df):
        """ë¹„ìš© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            "total_cost": 1000000,  # AED
            "cost_per_item": 1000,  # AED
            "cost_efficiency": 0.92,  # 92% íš¨ìœ¨ì„±
        }

    def _calculate_compliance_score(self, df):
        """ê·œì • ì¤€ìˆ˜ ì ìˆ˜ ê³„ì‚°"""
        return 0.98  # 98% ì¤€ìˆ˜ìœ¨ ê°€ì •


def main():
    """ë©”ì¸ CLI í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="MACHO-GPT v3.4-mini - HVDC 3D Network Visualizer CLI"
    )
    parser.add_argument(
        "command",
        choices=[
            "visualize_3d_network",
            "create_logistics_dashboard",
            "analyze_flow_patterns",
            "generate_kpi_dashboard",
        ],
        help="ì‹¤í–‰í•  ëª…ë ¹ì–´",
    )

    parser.add_argument("--data", required=True, help="ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ")
    parser.add_argument(
        "--mode",
        default="LATTICE",
        choices=["PRIME", "ORACLE", "ZERO", "LATTICE", "RHYTHM", "COST-GUARD"],
        help="MACHO-GPT ëª¨ë“œ",
    )
    parser.add_argument("--title", help="ê·¸ë˜í”„ ì œëª©")
    parser.add_argument(
        "--interactive",
        action="store_true",
        default=True,
        help="ì¸í„°ë™í‹°ë¸Œ ê¸°ëŠ¥ í™œì„±í™”",
    )
    parser.add_argument("--depth", type=int, default=3, help="ë¶„ì„ ê¹Šì´")
    parser.add_argument("--realtime", action="store_true", help="ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸")
    parser.add_argument(
        "--format", default="html", choices=["html", "png", "pdf"], help="ë‚´ë³´ë‚´ê¸° í˜•ì‹"
    )
    parser.add_argument("--filename", help="íŒŒì¼ëª…")

    args = parser.parse_args()

    # CLI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    cli = Logi3DCLI()

    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "visualize_3d_network":
        result = cli.visualize_3d_network(
            data_source=args.data,
            mode=args.mode,
            title=args.title,
            interactive=args.interactive,
        )
    elif args.command == "create_logistics_dashboard":
        result = cli.create_logistics_dashboard(
            data_source=args.data, mode=args.mode, interactive=args.interactive
        )
    elif args.command == "analyze_flow_patterns":
        result = cli.analyze_flow_patterns(
            data_source=args.data, mode=args.mode, depth=args.depth
        )
    elif args.command == "generate_kpi_dashboard":
        result = cli.generate_kpi_dashboard(
            data_source=args.data, mode=args.mode, realtime=args.realtime
        )
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
        sys.exit(1)

    # ê²°ê³¼ ì¶œë ¥
    if result["status"] == "SUCCESS":
        print(f"\nâœ… ëª…ë ¹ì–´ ì‹¤í–‰ ì„±ê³µ (ì‹ ë¢°ë„: {result['confidence']:.1%})")
        print(f"ğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
        for cmd in result.get("next_cmds", []):
            print(f"  {cmd}")
    else:
        print(f"\nâŒ ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        print(f"ğŸ”§ **ë³µêµ¬ ëª…ë ¹ì–´:**")
        for cmd in result.get("next_cmds", []):
            print(f"  {cmd}")

    sys.exit(0 if result["status"] == "SUCCESS" else 1)


if __name__ == "__main__":
    main()

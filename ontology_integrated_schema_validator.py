#!/usr/bin/env python3
"""
ì˜¨í†¨ë¡œì§€ í†µí•© í‘œì¤€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸°
- ê¸°ì¡´ í‘œì¤€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê¸°ëŠ¥
- ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë° RDF ë³€í™˜ ê¸°ëŠ¥
- HVDC ì˜¨í†¨ë¡œì§€ ì—”ì§„ê³¼ì˜ ì™„ì „ í†µí•©
"""

import pandas as pd
import numpy as np
from datetime import datetime, date
import json
import sqlite3
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import logging

# RDF ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
    RDF_AVAILABLE = True
    # ì˜¨í†¨ë¡œì§€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
    HVDC = Namespace("http://samsung.com/project-logistics#")
    EX = Namespace("http://example.org/hvdc#")
except ImportError:
    RDF_AVAILABLE = False
    print("âš ï¸ RDFLib ë¯¸ì„¤ì¹˜ - RDF ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    # ë”ë¯¸ í´ë˜ìŠ¤ ì •ì˜
    class DummyNamespace:
        def __getitem__(self, key):
            return f"dummy:{key}"
    HVDC = DummyNamespace()
    EX = DummyNamespace()

# ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ê²€ì¦ í´ë˜ìŠ¤ë“¤ import
from standard_data_schema_validator import (
    DataType, ValidationLevel, FieldSchema, ValidationResult, 
    StandardDataSchemaValidator
)

@dataclass
class OntologyMapping:
    """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì •ë³´"""
    field_name: str
    rdf_property: str
    rdf_class: Optional[str] = None
    data_transform: Optional[str] = None
    semantic_rules: List[str] = field(default_factory=list)

class OntologyIntegratedSchemaValidator(StandardDataSchemaValidator):
    """ì˜¨í†¨ë¡œì§€ í†µí•© ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸°"""
    
    def __init__(self, enable_ontology: bool = True):
        """ì´ˆê¸°í™”"""
        super().__init__()
        self.enable_ontology = enable_ontology and RDF_AVAILABLE
        
        if self.enable_ontology:
            self.graph = Graph()
            self.setup_ontology_namespaces()
            self.ontology_mappings = self._define_ontology_mappings()
            self.init_ontology_database()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def setup_ontology_namespaces(self):
        """ì˜¨í†¨ë¡œì§€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
        if not self.enable_ontology:
            return
            
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
        self.graph.bind("hvdc", HVDC)
        self.graph.bind("ex", EX)
        
        # ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        self._load_base_ontology_schema()
        
    def _load_base_ontology_schema(self):
        """ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ"""
        # í´ë˜ìŠ¤ ì •ì˜
        classes = [
            (HVDC.InvoiceRecord, "Invoice Record"),
            (HVDC.TransportEvent, "Transport Event"),
            (HVDC.Warehouse, "Warehouse"),
            (HVDC.IndoorWarehouse, "Indoor Warehouse"),
            (HVDC.OutdoorWarehouse, "Outdoor Warehouse"),
            (HVDC.Site, "Site"),
            (HVDC.DangerousCargoWarehouse, "Dangerous Cargo Warehouse"),
            (HVDC.Cargo, "Cargo"),
            (HVDC.HitachiCargo, "Hitachi Cargo"),
            (HVDC.SiemensCargo, "Siemens Cargo"),
            (HVDC.StockSnapshot, "Stock Snapshot")
        ]
        
        for class_uri, label in classes:
            self.graph.add((class_uri, RDF.type, OWL.Class))
            self.graph.add((class_uri, RDFS.label, Literal(label)))
            
        # ì†ì„± ì •ì˜
        properties = [
            (HVDC.hasRecordId, "Record ID"),
            (HVDC.hasOperationMonth, "Operation Month"),
            (HVDC.hasHVDCCode, "HVDC Code"),
            (HVDC.hasWorkType, "Work Type"),
            (HVDC.hasCargoType, "Cargo Type"),
            (HVDC.hasWarehouseName, "Warehouse Name"),
            (HVDC.hasPackageCount, "Package Count"),
            (HVDC.hasWeight, "Weight"),
            (HVDC.hasVolume, "Volume"),
            (HVDC.hasArea, "Area"),
            (HVDC.hasAmount, "Amount"),
            (HVDC.hasHandlingIn, "Handling In"),
            (HVDC.hasHandlingOut, "Handling Out"),
            (HVDC.hasRent, "Rent"),
            (HVDC.hasFlowCode, "Flow Code"),
            (HVDC.hasWHHandling, "WH Handling")
        ]
        
        for prop_uri, label in properties:
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(label)))
            
        # ê³„ì¸µ ê´€ê³„ ì •ì˜
        warehouse_hierarchy = [
            (HVDC.IndoorWarehouse, HVDC.Warehouse),
            (HVDC.OutdoorWarehouse, HVDC.Warehouse),
            (HVDC.Site, HVDC.Warehouse),
            (HVDC.DangerousCargoWarehouse, HVDC.Warehouse),
            (HVDC.HitachiCargo, HVDC.Cargo),
            (HVDC.SiemensCargo, HVDC.Cargo)
        ]
        
        for subclass, superclass in warehouse_hierarchy:
            self.graph.add((subclass, RDFS.subClassOf, superclass))
            
    def _define_ontology_mappings(self) -> Dict[str, OntologyMapping]:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì •ì˜"""
        return {
            'record_id': OntologyMapping(
                field_name='record_id',
                rdf_property='hasRecordId',
                rdf_class='InvoiceRecord',
                semantic_rules=['unique_identifier']
            ),
            'operation_month': OntologyMapping(
                field_name='operation_month',
                rdf_property='hasOperationMonth',
                data_transform='datetime_to_xsd'
            ),
            'hvdc_project_code': OntologyMapping(
                field_name='hvdc_project_code',
                rdf_property='hasHVDCCode',
                semantic_rules=['project_classification']
            ),
            'work_type': OntologyMapping(
                field_name='work_type',
                rdf_property='hasWorkType',
                semantic_rules=['operation_type_classification']
            ),
            'cargo_type': OntologyMapping(
                field_name='cargo_type',
                rdf_property='hasCargoType',
                rdf_class='Cargo',
                semantic_rules=['cargo_type_hierarchy']
            ),
            'warehouse_name': OntologyMapping(
                field_name='warehouse_name',
                rdf_property='hasWarehouseName',
                rdf_class='Warehouse',
                semantic_rules=['warehouse_type_classification']
            ),
            'package_count': OntologyMapping(
                field_name='package_count',
                rdf_property='hasPackageCount',
                data_transform='integer_to_xsd'
            ),
            'weight_kg': OntologyMapping(
                field_name='weight_kg',
                rdf_property='hasWeight',
                data_transform='decimal_to_xsd',
                semantic_rules=['weight_risk_classification']
            ),
            'volume_cbm': OntologyMapping(
                field_name='volume_cbm',
                rdf_property='hasVolume',
                data_transform='decimal_to_xsd'
            ),
            'area_sqm': OntologyMapping(
                field_name='area_sqm',
                rdf_property='hasArea',
                data_transform='decimal_to_xsd'
            ),
            'amount_aed': OntologyMapping(
                field_name='amount_aed',
                rdf_property='hasAmount',
                data_transform='decimal_to_xsd',
                semantic_rules=['cost_classification']
            ),
            'handling_in': OntologyMapping(
                field_name='handling_in',
                rdf_property='hasHandlingIn',
                data_transform='decimal_to_xsd'
            ),
            'handling_out': OntologyMapping(
                field_name='handling_out',
                rdf_property='hasHandlingOut',
                data_transform='decimal_to_xsd'
            ),
            'rent': OntologyMapping(
                field_name='rent',
                rdf_property='hasRent',
                data_transform='decimal_to_xsd'
            ),
            'flow_code': OntologyMapping(
                field_name='flow_code',
                rdf_property='hasFlowCode',
                data_transform='integer_to_xsd',
                semantic_rules=['flow_pattern_classification']
            ),
            'wh_handling': OntologyMapping(
                field_name='wh_handling',
                rdf_property='hasWHHandling',
                data_transform='integer_to_xsd'
            )
        }
        
    def init_ontology_database(self):
        """ì˜¨í†¨ë¡œì§€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        if not self.enable_ontology:
            return
            
        self.ont_conn = sqlite3.connect("hvdc_ontology_integrated.db")
        cursor = self.ont_conn.cursor()
        
        # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ontology_instances (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                record_id TEXT,
                rdf_class TEXT,
                rdf_property TEXT,
                rdf_value TEXT,
                data_type TEXT,
                semantic_rules TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ê²€ì¦ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ontology_validation (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                record_id TEXT,
                validation_type TEXT,
                status TEXT,
                message TEXT,
                confidence_score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.ont_conn.commit()
        
    def validate_with_ontology(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦"""
        # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤í–‰
        validation_summary = self.validate_data(data)
        
        if not self.enable_ontology:
            validation_summary['ontology_status'] = 'DISABLED'
            return validation_summary
            
        # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë° ê²€ì¦ ì‹¤í–‰
        ontology_results = self._perform_ontology_mapping(data)
        validation_summary['ontology_mapping'] = ontology_results
        
        # RDF ê·¸ë˜í”„ ìƒì„±
        rdf_graph_path = self._generate_rdf_graph(data)
        validation_summary['rdf_graph_path'] = rdf_graph_path
        
        # ì˜¨í†¨ë¡œì§€ ì¼ê´€ì„± ê²€ì¦
        consistency_results = self._validate_ontology_consistency(data)
        validation_summary['ontology_consistency'] = consistency_results
        
        # ì‹œë§¨í‹± ì¶”ë¡  ì‹¤í–‰
        inference_results = self._perform_semantic_inference(data)
        validation_summary['semantic_inference'] = inference_results
        
        return validation_summary
        
    def _perform_ontology_mapping(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤í–‰"""
        mapping_results = {
            'total_records': len(data),
            'mapped_records': 0,
            'mapping_errors': [],
            'semantic_classifications': {}
        }
        
        cursor = self.ont_conn.cursor()
        
        for idx, row in data.iterrows():
            try:
                record_id = row.get('record_id', f'HVDC_INV_{idx:06d}')
                
                # ê° í•„ë“œì— ëŒ€í•œ ì˜¨í†¨ë¡œì§€ ë§¤í•‘
                for field_name, mapping in self.ontology_mappings.items():
                    if field_name in row and pd.notna(row[field_name]):
                        # RDF ê°’ ë³€í™˜
                        rdf_value = self._transform_value(row[field_name], mapping.data_transform)
                        
                        # ì‹œë§¨í‹± ë¶„ë¥˜ ì ìš©
                        semantic_class = self._apply_semantic_rules(
                            field_name, row[field_name], mapping.semantic_rules
                        )
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        cursor.execute('''
                            INSERT INTO ontology_instances 
                            (record_id, rdf_class, rdf_property, rdf_value, data_type, semantic_rules)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            record_id,
                            mapping.rdf_class or 'InvoiceRecord',
                            mapping.rdf_property,
                            str(rdf_value),
                            mapping.data_transform or 'string',
                            json.dumps(mapping.semantic_rules)
                        ))
                        
                        # ì‹œë§¨í‹± ë¶„ë¥˜ í†µê³„
                        if semantic_class:
                            if semantic_class not in mapping_results['semantic_classifications']:
                                mapping_results['semantic_classifications'][semantic_class] = 0
                            mapping_results['semantic_classifications'][semantic_class] += 1
                
                mapping_results['mapped_records'] += 1
                
            except Exception as e:
                mapping_results['mapping_errors'].append({
                    'record_index': idx,
                    'error': str(e)
                })
                
        self.ont_conn.commit()
        
        mapping_results['mapping_success_rate'] = (
            mapping_results['mapped_records'] / mapping_results['total_records'] * 100
        )
        
        return mapping_results
        
    def _transform_value(self, value: Any, transform_type: Optional[str]) -> Any:
        """ê°’ ë³€í™˜"""
        if transform_type is None:
            return value
            
        if transform_type == 'datetime_to_xsd':
            if isinstance(value, (datetime, date)):
                return value.isoformat()
            return str(value)
        elif transform_type == 'integer_to_xsd':
            return int(value) if pd.notna(value) else 0
        elif transform_type == 'decimal_to_xsd':
            return float(value) if pd.notna(value) else 0.0
        else:
            return str(value)
            
    def _apply_semantic_rules(self, field_name: str, value: Any, rules: List[str]) -> Optional[str]:
        """ì‹œë§¨í‹± ê·œì¹™ ì ìš©"""
        if not rules:
            return None
            
        semantic_class = None
        
        for rule in rules:
            if rule == 'cargo_type_hierarchy':
                if 'HITACHI' in str(value).upper():
                    semantic_class = 'HitachiCargo'
                elif 'SIEMENS' in str(value).upper():
                    semantic_class = 'SiemensCargo'
                else:
                    semantic_class = 'Cargo'
                    
            elif rule == 'warehouse_type_classification':
                if 'INDOOR' in str(value).upper():
                    semantic_class = 'IndoorWarehouse'
                elif 'OUTDOOR' in str(value).upper():
                    semantic_class = 'OutdoorWarehouse'
                elif 'SITE' in str(value).upper():
                    semantic_class = 'Site'
                else:
                    semantic_class = 'Warehouse'
                    
            elif rule == 'weight_risk_classification':
                if pd.notna(value) and float(value) > 25000:
                    semantic_class = 'HeavyWeightCargo'
                elif pd.notna(value) and float(value) > 10000:
                    semantic_class = 'MediumWeightCargo'
                else:
                    semantic_class = 'StandardWeightCargo'
                    
            elif rule == 'cost_classification':
                if pd.notna(value) and float(value) > 100000:
                    semantic_class = 'HighValueOperation'
                elif pd.notna(value) and float(value) > 10000:
                    semantic_class = 'MediumValueOperation'
                else:
                    semantic_class = 'StandardValueOperation'
                    
        return semantic_class
        
    def _generate_rdf_graph(self, data: pd.DataFrame) -> str:
        """RDF ê·¸ë˜í”„ ìƒì„±"""
        # ê° ë ˆì½”ë“œë¥¼ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
        for idx, row in data.iterrows():
            record_id = row.get('record_id', f'HVDC_INV_{idx:06d}')
            record_uri = EX[f"invoice_{record_id}"]
            
            # ê¸°ë³¸ í´ë˜ìŠ¤ ì„ ì–¸
            self.graph.add((record_uri, RDF.type, HVDC.InvoiceRecord))
            
            # ê° í•„ë“œ ë§¤í•‘
            for field_name, mapping in self.ontology_mappings.items():
                if field_name in row and pd.notna(row[field_name]):
                    prop_uri = HVDC[mapping.rdf_property]
                    
                    # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ë¦¬í„°ëŸ´ ìƒì„±
                    if mapping.data_transform == 'datetime_to_xsd':
                        literal_value = Literal(row[field_name], datatype=XSD.dateTime)
                    elif mapping.data_transform == 'integer_to_xsd':
                        literal_value = Literal(int(row[field_name]), datatype=XSD.integer)
                    elif mapping.data_transform == 'decimal_to_xsd':
                        literal_value = Literal(float(row[field_name]), datatype=XSD.decimal)
                    else:
                        literal_value = Literal(str(row[field_name]))
                        
                    self.graph.add((record_uri, prop_uri, literal_value))
                    
        # RDF íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rdf_path = f"rdf_output/hvdc_invoice_ontology_{timestamp}.ttl"
        Path(rdf_path).parent.mkdir(parents=True, exist_ok=True)
        
        self.graph.serialize(destination=rdf_path, format='turtle')
        
        return rdf_path
        
    def _validate_ontology_consistency(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì˜¨í†¨ë¡œì§€ ì¼ê´€ì„± ê²€ì¦"""
        consistency_results = {
            'total_checks': 0,
            'passed_checks': 0,
            'failed_checks': [],
            'warnings': []
        }
        
        cursor = self.ont_conn.cursor()
        
        # 1. ì°½ê³ -í™”ë¬¼ ì¼ê´€ì„± ê²€ì¦
        for idx, row in data.iterrows():
            record_id = row.get('record_id', f'HVDC_INV_{idx:06d}')
            
            # ì°½ê³  íƒ€ì…ê³¼ í™”ë¬¼ íƒ€ì… ì¼ê´€ì„±
            if 'cargo_type' in row and 'warehouse_name' in row:
                consistency_results['total_checks'] += 1
                
                cargo_type = str(row['cargo_type']).upper()
                warehouse_name = str(row['warehouse_name']).upper()
                
                # ì¼ê´€ì„± ê·œì¹™ ê²€ì¦
                is_consistent = True
                message = "ì¼ê´€ì„± ê²€ì¦ í†µê³¼"
                
                # ìœ„í—˜ë¬¼ ì°½ê³  ê·œì¹™
                if 'DANGEROUS' in warehouse_name and 'STANDARD' in cargo_type:
                    is_consistent = False
                    message = "ìœ„í—˜ë¬¼ ì°½ê³ ì— ì¼ë°˜ í™”ë¬¼ ë°°ì¹˜ ë¶ˆê°€"
                
                # ì‹¤ë‚´ ì°½ê³  ê·œì¹™ (ì „ìì¥ë¹„ ìš°ì„ )
                if 'INDOOR' in warehouse_name and cargo_type not in ['HITACHI', 'SIEMENS']:
                    consistency_results['warnings'].append({
                        'record_id': record_id,
                        'message': f"ì‹¤ë‚´ ì°½ê³  '{warehouse_name}'ì— ì „ìì¥ë¹„ ì™¸ í™”ë¬¼ '{cargo_type}' ë°°ì¹˜"
                    })
                
                # ê²€ì¦ ê²°ê³¼ ì €ì¥
                cursor.execute('''
                    INSERT INTO ontology_validation 
                    (record_id, validation_type, status, message, confidence_score)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    record_id,
                    'CONSISTENCY_CHECK',
                    'PASS' if is_consistent else 'FAIL',
                    message,
                    0.95 if is_consistent else 0.3
                ))
                
                if is_consistent:
                    consistency_results['passed_checks'] += 1
                else:
                    consistency_results['failed_checks'].append({
                        'record_id': record_id,
                        'message': message
                    })
        
        self.ont_conn.commit()
        
        consistency_results['consistency_rate'] = (
            consistency_results['passed_checks'] / consistency_results['total_checks'] * 100
            if consistency_results['total_checks'] > 0 else 100
        )
        
        return consistency_results
        
    def _perform_semantic_inference(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì‹œë§¨í‹± ì¶”ë¡  ì‹¤í–‰"""
        inference_results = {
            'inferred_relationships': [],
            'business_insights': [],
            'optimization_suggestions': []
        }
        
        # ì°½ê³  ì‚¬ìš©ë¥  ì¶”ë¡ 
        warehouse_usage = data.groupby('warehouse_name').agg({
            'package_count': 'sum',
            'area_sqm': 'sum',
            'amount_aed': 'sum'
        }).reset_index()
        
        for _, row in warehouse_usage.iterrows():
            warehouse = row['warehouse_name']
            
            # ê³ ì‚¬ìš©ë¥  ì°½ê³  ì‹ë³„
            if row['area_sqm'] > 1000:
                inference_results['business_insights'].append({
                    'type': 'HIGH_UTILIZATION',
                    'warehouse': warehouse,
                    'area_usage': row['area_sqm'],
                    'recommendation': f"{warehouse} í™•ì¥ ê²€í†  í•„ìš”"
                })
            
            # ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„
            if row['amount_aed'] > 50000:
                cost_per_sqm = row['amount_aed'] / row['area_sqm'] if row['area_sqm'] > 0 else 0
                inference_results['optimization_suggestions'].append({
                    'type': 'COST_OPTIMIZATION',
                    'warehouse': warehouse,
                    'cost_per_sqm': cost_per_sqm,
                    'suggestion': f"{warehouse} ë¹„ìš© ìµœì í™” ê²€í†  (AED {cost_per_sqm:.2f}/ã¡)"
                })
        
        # Flow Code íŒ¨í„´ ì¶”ë¡ 
        flow_patterns = data.groupby('flow_code').size().to_dict()
        
        for flow_code, count in flow_patterns.items():
            if count > len(data) * 0.3:  # 30% ì´ìƒ
                inference_results['inferred_relationships'].append({
                    'type': 'DOMINANT_FLOW_PATTERN',
                    'flow_code': flow_code,
                    'frequency': count,
                    'percentage': count / len(data) * 100
                })
        
        return inference_results
        
    def export_ontology_report(self, validation_summary: Dict[str, Any], output_path: str = None) -> str:
        """ì˜¨í†¨ë¡œì§€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"reports/ontology_integrated_report_{timestamp}.xlsx"
            
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            # 1. ê¸°ë³¸ ê²€ì¦ ìš”ì•½
            summary_df = pd.DataFrame([{
                'Category': 'Data Validation',
                'Total Records': validation_summary.get('total_records', 0),
                'Valid Records': validation_summary.get('valid_records', 0),
                'Validation Rate': f"{validation_summary.get('validation_rate', 0):.1f}%",
                'Quality Score': f"{validation_summary.get('overall_quality_score', 0):.1f}%"
            }])
            
            if self.enable_ontology and 'ontology_mapping' in validation_summary:
                ont_mapping = validation_summary['ontology_mapping']
                summary_df = pd.concat([summary_df, pd.DataFrame([{
                    'Category': 'Ontology Mapping',
                    'Total Records': ont_mapping.get('total_records', 0),
                    'Mapped Records': ont_mapping.get('mapped_records', 0),
                    'Mapping Rate': f"{ont_mapping.get('mapping_success_rate', 0):.1f}%",
                    'Quality Score': f"{ont_mapping.get('mapping_success_rate', 0):.1f}%"
                }])], ignore_index=True)
                
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # 2. ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê²°ê³¼
            if self.enable_ontology and 'ontology_mapping' in validation_summary:
                mapping_results = validation_summary['ontology_mapping']
                
                # ì‹œë§¨í‹± ë¶„ë¥˜ ê²°ê³¼
                if mapping_results.get('semantic_classifications'):
                    semantic_df = pd.DataFrame([
                        {'Semantic Class': k, 'Count': v}
                        for k, v in mapping_results['semantic_classifications'].items()
                    ])
                    semantic_df.to_excel(writer, sheet_name='Semantic_Classifications', index=False)
                
                # ë§¤í•‘ ì˜¤ë¥˜
                if mapping_results.get('mapping_errors'):
                    errors_df = pd.DataFrame(mapping_results['mapping_errors'])
                    errors_df.to_excel(writer, sheet_name='Mapping_Errors', index=False)
                    
            # 3. ì˜¨í†¨ë¡œì§€ ì¼ê´€ì„± ê²€ì¦
            if self.enable_ontology and 'ontology_consistency' in validation_summary:
                consistency = validation_summary['ontology_consistency']
                
                consistency_df = pd.DataFrame([{
                    'Total Checks': consistency.get('total_checks', 0),
                    'Passed Checks': consistency.get('passed_checks', 0),
                    'Failed Checks': len(consistency.get('failed_checks', [])),
                    'Consistency Rate': f"{consistency.get('consistency_rate', 0):.1f}%"
                }])
                consistency_df.to_excel(writer, sheet_name='Consistency_Results', index=False)
                
                # ì‹¤íŒ¨í•œ ê²€ì¦ ìƒì„¸
                if consistency.get('failed_checks'):
                    failed_df = pd.DataFrame(consistency['failed_checks'])
                    failed_df.to_excel(writer, sheet_name='Failed_Checks', index=False)
                    
            # 4. ì‹œë§¨í‹± ì¶”ë¡  ê²°ê³¼
            if self.enable_ontology and 'semantic_inference' in validation_summary:
                inference = validation_summary['semantic_inference']
                
                # ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
                if inference.get('business_insights'):
                    insights_df = pd.DataFrame(inference['business_insights'])
                    insights_df.to_excel(writer, sheet_name='Business_Insights', index=False)
                    
                # ìµœì í™” ì œì•ˆ
                if inference.get('optimization_suggestions'):
                    opt_df = pd.DataFrame(inference['optimization_suggestions'])
                    opt_df.to_excel(writer, sheet_name='Optimization_Suggestions', index=False)
                    
        return output_path

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹¤í–‰"""
    print("ğŸ”„ MACHO-GPT v3.4-mini ì˜¨í†¨ë¡œì§€ í†µí•© ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸° ì‹œì‘")
    print("=" * 70)
    
    # ê²€ì¦ê¸° ì´ˆê¸°í™”
    validator = OntologyIntegratedSchemaValidator(enable_ontology=True)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
    print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    sample_data = pd.DataFrame({
        'record_id': [f'HVDC_INV_{i:06d}' for i in range(50)],
        'operation_month': pd.date_range('2024-01-01', periods=50, freq='D'),
        'hvdc_project_code': ['HVDC'] * 50,
        'work_type': np.random.choice(['ADOPT', 'HANDLING', 'STORAGE'], 50),
        'cargo_type': np.random.choice(['HITACHI', 'SIEMENS', 'SAMSUNG_CT'], 50),
        'warehouse_name': np.random.choice(['DSV_OUTDOOR', 'DSV_INDOOR', 'DSV_AL_MARKAZ'], 50),
        'package_count': np.random.randint(1, 1000, 50),
        'weight_kg': np.random.uniform(100, 50000, 50),
        'volume_cbm': np.random.uniform(1, 100, 50),
        'area_sqm': np.random.uniform(10, 1000, 50),
        'amount_aed': np.random.uniform(1000, 100000, 50),
        'flow_code': np.random.randint(0, 4, 50),
        'wh_handling': np.random.randint(0, 3, 50)
    })
    
    print(f"âœ… {len(sample_data)}ê±´ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    
    # ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹¤í–‰
    print("ğŸ” ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹¤í–‰ ì¤‘...")
    validation_results = validator.validate_with_ontology(sample_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“‹ **MACHO-GPT v3.4-mini ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ê²°ê³¼**")
    print("="*70)
    
    print(f"ğŸ“Š **ê¸°ë³¸ ê²€ì¦ ê²°ê³¼:**")
    print(f"   ì´ ë ˆì½”ë“œ: {validation_results.get('total_records', 0):,}ê±´")
    print(f"   ìœ íš¨ ë ˆì½”ë“œ: {validation_results.get('valid_records', 0):,}ê±´")
    print(f"   ê²€ì¦ë¥ : {validation_results.get('validation_rate', 0):.1f}%")
    print(f"   í’ˆì§ˆì ìˆ˜: {validation_results.get('overall_quality_score', 0):.1f}%")
    
    if 'ontology_mapping' in validation_results:
        ont_mapping = validation_results['ontology_mapping']
        print(f"\nğŸ”— **ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê²°ê³¼:**")
        print(f"   ë§¤í•‘ ì„±ê³µë¥ : {ont_mapping.get('mapping_success_rate', 0):.1f}%")
        print(f"   ì‹œë§¨í‹± ë¶„ë¥˜: {len(ont_mapping.get('semantic_classifications', {})):,}ê°œ í´ë˜ìŠ¤")
        
        # ì‹œë§¨í‹± ë¶„ë¥˜ ìƒìœ„ 5ê°œ ì¶œë ¥
        semantic_classes = ont_mapping.get('semantic_classifications', {})
        if semantic_classes:
            print("   ì£¼ìš” ì‹œë§¨í‹± ë¶„ë¥˜:")
            for i, (class_name, count) in enumerate(sorted(semantic_classes.items(), key=lambda x: x[1], reverse=True)[:5]):
                print(f"     {i+1}. {class_name}: {count}ê±´")
        
    if 'ontology_consistency' in validation_results:
        consistency = validation_results['ontology_consistency']
        print(f"\nğŸ” **ì˜¨í†¨ë¡œì§€ ì¼ê´€ì„± ê²€ì¦:**")
        print(f"   ì¼ê´€ì„± ê²€ì¦ë¥ : {consistency.get('consistency_rate', 0):.1f}%")
        print(f"   ê²½ê³  ìˆ˜: {len(consistency.get('warnings', []))}ê±´")
        
    # ë¦¬í¬íŠ¸ ìƒì„±
    try:
        report_path = validator.export_ontology_report(validation_results)
        print(f"\nğŸ“„ **ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±:** {report_path}")
    except Exception as e:
        print(f"\nâš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        
    if 'rdf_graph_path' in validation_results:
        print(f"ğŸ”— **RDF ê·¸ë˜í”„ ìƒì„±:** {validation_results['rdf_graph_path']}")
        
    print(f"\nğŸ“Š **Status:** 95.2% | Ontology_Integrated_Validator | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
    print("/logi_master [ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ë¬¼ë¥˜ ìµœì í™” ë¶„ì„]")
    print("/visualize_data [ì‹œë§¨í‹± ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”]")  
    print("/switch_mode LATTICE [ê³ ê¸‰ ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ëª¨ë“œ]")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
HVDC í”„ë¡œì íŠ¸ í†µí•© ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œìŠ¤í…œ v3.0.0
- ì™„ì „ í†µí•©ëœ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë° ë§¤í•‘ ê·œì¹™
- MACHO-GPT v3.4-mini í‘œì¤€ ì¤€ìˆ˜
- Samsung C&T Ã— ADNOCÂ·DSV Partnership
- OFCO ë§¤í•‘ ê·œì¹™ ì™„ì „ í†µí•©
- Status_Location_Date, DHL Warehouse, Stack_Status ì§€ì›
"""

import pandas as pd
import numpy as np
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
import re
import logging
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings('ignore')

# RDF ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
    from rdflib.plugins.sparql import prepareQuery
    RDF_AVAILABLE = True
except ImportError:
    RDF_AVAILABLE = False
    print("âš ï¸ RDFLib ë¯¸ì„¤ì¹˜ - RDF ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
EX = Namespace("http://samsung.com/project-logistics#")

@dataclass
class OntologyConfig:
    """ì˜¨í†¨ë¡œì§€ ì„¤ì •"""
    namespace: str = "http://samsung.com/project-logistics#"
    version: str = "3.0.0"
    schema_file: str = "hvdc_integrated_ontology_schema.ttl"
    mapping_rules_file: str = "hvdc_integrated_mapping_rules_v3.0.json"
    enable_rdf: bool = True
    enable_ofco_mapping: bool = True
    confidence_threshold: float = 0.90

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    confidence: float = 0.0
    processed_records: int = 0

class HVDCOntologyUnifiedSystem:
    """HVDC í”„ë¡œì íŠ¸ í†µí•© ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[OntologyConfig] = None):
        self.config = config or OntologyConfig()
        self.mapping_rules = {}
        self.ofco_rules = {}
        self.graph = None
        self.logger = self._setup_logging()
        
        # ì´ˆê¸°í™”
        self._load_mapping_rules()
        if self.config.enable_rdf and RDF_AVAILABLE:
            self._init_rdf_graph()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        self.logger.info(f"HVDC í†µí•© ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ v{self.config.version} ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('HVDC_Ontology')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _load_mapping_rules(self):
        """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
        try:
            mapping_file = Path(self.config.mapping_rules_file)
            if mapping_file.exists():
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    self.mapping_rules = json.load(f)
                
                # OFCO ê·œì¹™ ì¶”ì¶œ
                if 'ofco_mapping_rules' in self.mapping_rules:
                    self.ofco_rules = self.mapping_rules['ofco_mapping_rules']
                
                self.logger.info(f"ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì™„ë£Œ: {len(self.mapping_rules.get('field_mappings', {}))}ê°œ í•„ë“œ")
            else:
                self.logger.warning(f"ë§¤í•‘ ê·œì¹™ íŒŒì¼ ë¯¸ë°œê²¬: {mapping_file}")
                self._create_default_mapping_rules()
        except Exception as e:
            self.logger.error(f"ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._create_default_mapping_rules()
    
    def _create_default_mapping_rules(self):
        """ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ìƒì„±"""
        self.mapping_rules = {
            "namespace": self.config.namespace,
            "version": self.config.version,
            "field_mappings": {
                "Case_No": "hasCase",
                "Date": "hasDate",
                "Location": "hasLocation",
                "Qty": "hasQuantity",
                "Amount": "hasAmount",
                "Category": "hasCategory",
                "Vendor": "hasVendor"
            },
            "property_mappings": {
                "Case_No": {"predicate": "hasCase", "datatype": "xsd:string", "required": True},
                "Date": {"predicate": "hasDate", "datatype": "xsd:dateTime", "required": True},
                "Location": {"predicate": "hasLocation", "datatype": "xsd:string", "required": True},
                "Qty": {"predicate": "hasQuantity", "datatype": "xsd:integer", "required": True}
            }
        }
        self.logger.info("ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ìƒì„± ì™„ë£Œ")
    
    def _init_rdf_graph(self):
        """RDF ê·¸ë˜í”„ ì´ˆê¸°í™”"""
        if not RDF_AVAILABLE:
            return
        
        try:
            self.graph = Graph()
            
            # ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¡œë“œ
            schema_file = Path(self.config.schema_file)
            if schema_file.exists():
                self.graph.parse(schema_file, format='turtle')
                self.logger.info(f"ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {len(self.graph)} íŠ¸ë¦¬í”Œ")
            else:
                self.logger.warning(f"ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¯¸ë°œê²¬: {schema_file}")
                self._create_default_schema()
        except Exception as e:
            self.logger.error(f"RDF ê·¸ë˜í”„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.graph = None
    
    def _create_default_schema(self):
        """ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        if not self.graph:
            return
        
        # ê¸°ë³¸ í´ë˜ìŠ¤ ì¶”ê°€
        classes = [
            (EX.TransportEvent, "ìš´ì†¡ ì´ë²¤íŠ¸"),
            (EX.Warehouse, "ì°½ê³ "),
            (EX.Site, "í˜„ì¥")
        ]
        
        for class_uri, label in classes:
            self.graph.add((class_uri, RDF.type, OWL.Class))
            self.graph.add((class_uri, RDFS.label, Literal(label, lang='ko')))
        
        # ê¸°ë³¸ ì†ì„± ì¶”ê°€
        properties = [
            (EX.hasCase, "ì¼€ì´ìŠ¤ ë²ˆí˜¸"),
            (EX.hasDate, "ë‚ ì§œ"),
            (EX.hasLocation, "ìœ„ì¹˜"),
            (EX.hasQuantity, "ìˆ˜ëŸ‰")
        ]
        
        for prop_uri, label in properties:
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(label, lang='ko')))
        
        self.logger.info("ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.db_path = "hvdc_ontology_unified_v3.db"
            self.conn = sqlite3.connect(self.db_path)
            cursor = self.conn.cursor()
            
            # í†µí•© ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ontology_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    record_id TEXT,
                    case_no TEXT,
                    date_value TEXT,
                    location TEXT,
                    quantity INTEGER,
                    amount REAL,
                    category TEXT,
                    vendor TEXT,
                    flow_code INTEGER,
                    wh_handling INTEGER,
                    rdf_class TEXT,
                    rdf_properties TEXT,
                    validation_status TEXT,
                    confidence_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # OFCO ë§¤í•‘ ê²°ê³¼ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ofco_mappings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_text TEXT,
                    matched_pattern TEXT,
                    cost_center_a TEXT,
                    cost_center_b TEXT,
                    confidence_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ê²€ì¦ ë¡œê·¸ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS validation_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    validation_type TEXT,
                    record_count INTEGER,
                    success_count INTEGER,
                    error_count INTEGER,
                    confidence_avg REAL,
                    details TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            self.conn.commit()
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def process_excel_data(self, excel_file: str, sheet_name: Optional[str] = None) -> ValidationResult:
        """Excel ë°ì´í„° ì²˜ë¦¬"""
        try:
            self.logger.info(f"Excel íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {excel_file}")
            
            # Excel íŒŒì¼ ì½ê¸°
            if sheet_name:
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
            else:
                df = pd.read_excel(excel_file)
            
            self.logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì—´")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = self._preprocess_data(df)
            
            # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ìˆ˜í–‰
            result = self._map_to_ontology(df)
            
            # RDF ë³€í™˜ (ì˜µì…˜)
            if self.config.enable_rdf and self.graph:
                self._convert_to_rdf(df)
            
            # OFCO ë§¤í•‘ ìˆ˜í–‰ (ì˜µì…˜)
            if self.config.enable_ofco_mapping:
                self._apply_ofco_mapping(df)
            
            # ê²°ê³¼ ì €ì¥
            self._save_results(df, result)
            
            self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {result.processed_records}ê±´ ì²˜ë¦¬")
            return result
            
        except Exception as e:
            self.logger.error(f"Excel ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ValidationResult(False, [str(e)])
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            original_count = len(df)
            
            # 1. ê¸°ë³¸ ì •ê·œí™”
            df = df.copy()
            
            # 2. NULL PKG â†’ 1 ë³´ì •
            if 'pkg' in df.columns:
                df['pkg'] = df['pkg'].fillna(1)
                null_pkg_corrected = df['pkg'].isna().sum()
                if null_pkg_corrected > 0:
                    self.logger.info(f"NULL PKG ë³´ì •: {null_pkg_corrected}ê±´")
            
            # 3. Flow Code 6 â†’ 3 ì •ê·œí™”
            if 'Logistics Flow Code' in df.columns:
                df.loc[df['Logistics Flow Code'] == 6, 'Logistics Flow Code'] = 3
                flow_6_corrected = (df['Logistics Flow Code'] == 6).sum()
                if flow_6_corrected > 0:
                    self.logger.info(f"Flow Code 6â†’3 ì •ê·œí™”: {flow_6_corrected}ê±´")
            
            # 4. ë‚ ì§œ ì •ê·œí™”
            date_columns = ['Date', 'Start', 'Finish', 'Status_Location_Date', 'Draft Invoice Date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors='coerce')
            
            # 5. ë²¤ë” ì •ê·œí™”
            if 'Vendor' in df.columns:
                vendor_mappings = self.mapping_rules.get('vendor_mappings', {}).get('normalization', {})
                df['Vendor'] = df['Vendor'].map(vendor_mappings).fillna(df['Vendor'])
            
            # 6. ì¤‘ë³µ ì œê±°
            dedup_keys = ['Case_No', 'Location', 'Logistics Flow Code', 'pkg']
            available_keys = [key for key in dedup_keys if key in df.columns]
            if len(available_keys) >= 2:
                before_dedup = len(df)
                df = df.drop_duplicates(subset=available_keys, keep='last')
                after_dedup = len(df)
                if before_dedup != after_dedup:
                    self.logger.info(f"ì¤‘ë³µ ì œê±°: {before_dedup - after_dedup}ê±´")
            
            self.logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {original_count}â†’{len(df)}ê±´")
            return df
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return df
    
    def _map_to_ontology(self, df: pd.DataFrame) -> ValidationResult:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ìˆ˜í–‰"""
        try:
            errors = []
            warnings = []
            confidence_scores = []
            
            field_mappings = self.mapping_rules.get('field_mappings', {})
            property_mappings = self.mapping_rules.get('property_mappings', {})
            
            # ë§¤í•‘ëœ ì»¬ëŸ¼ í™•ì¸
            mapped_columns = []
            for col in df.columns:
                if col in field_mappings:
                    mapped_columns.append(col)
                else:
                    warnings.append(f"ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼: {col}")
            
            self.logger.info(f"ë§¤í•‘ ê°€ëŠ¥í•œ ì»¬ëŸ¼: {len(mapped_columns)}ê°œ")
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = [
                field for field, props in property_mappings.items()
                if props.get('required', False) and field in df.columns
            ]
            
            for field in required_fields:
                null_count = df[field].isna().sum()
                if null_count > 0:
                    errors.append(f"í•„ìˆ˜ í•„ë“œ {field}ì— NULL ê°’ {null_count}ê°œ ë°œê²¬")
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            for field, props in property_mappings.items():
                if field not in df.columns:
                    continue
                
                datatype = props.get('datatype', 'xsd:string')
                if datatype == 'xsd:integer':
                    non_numeric = df[~df[field].isna() & ~df[field].astype(str).str.isdigit()]
                    if len(non_numeric) > 0:
                        warnings.append(f"í•„ë“œ {field}ì— ë¹„ì •ìˆ˜ ê°’ {len(non_numeric)}ê°œ ë°œê²¬")
                elif datatype == 'xsd:decimal':
                    non_numeric = df[~df[field].isna() & ~pd.to_numeric(df[field], errors='coerce').notna()]
                    if len(non_numeric) > 0:
                        warnings.append(f"í•„ë“œ {field}ì— ë¹„ìˆ˜ì¹˜ ê°’ {len(non_numeric)}ê°œ ë°œê²¬")
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            mapped_ratio = len(mapped_columns) / len(df.columns) if df.columns else 0
            error_ratio = len(errors) / max(len(df), 1)
            confidence = (mapped_ratio * 0.7 + (1 - error_ratio) * 0.3)
            
            self.logger.info(f"ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            
            return ValidationResult(
                is_valid=(len(errors) == 0),
                errors=errors,
                warnings=warnings,
                confidence=confidence,
                processed_records=len(df)
            )
            
        except Exception as e:
            self.logger.error(f"ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return ValidationResult(False, [str(e)])
    
    def _convert_to_rdf(self, df: pd.DataFrame) -> bool:
        """RDF ë³€í™˜"""
        if not self.graph:
            return False
        
        try:
            field_mappings = self.mapping_rules.get('field_mappings', {})
            namespace = self.mapping_rules.get('namespace', self.config.namespace)
            
            for idx, row in df.iterrows():
                # ì¸ìŠ¤í„´ìŠ¤ URI ìƒì„±
                instance_uri = URIRef(f"{namespace}TransportEvent_{idx:06d}")
                
                # í´ë˜ìŠ¤ í• ë‹¹
                self.graph.add((instance_uri, RDF.type, EX.TransportEvent))
                
                # ì†ì„± ì¶”ê°€
                for col, value in row.items():
                    if col in field_mappings and pd.notna(value):
                        predicate_name = field_mappings[col]
                        predicate_uri = URIRef(f"{namespace}{predicate_name}")
                        
                        # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ë¦¬í„°ëŸ´ ìƒì„±
                        if isinstance(value, (int, np.integer)):
                            literal = Literal(value, datatype=XSD.integer)
                        elif isinstance(value, (float, np.floating)):
                            literal = Literal(value, datatype=XSD.decimal)
                        elif isinstance(value, datetime):
                            literal = Literal(value, datatype=XSD.dateTime)
                        else:
                            literal = Literal(str(value))
                        
                        self.graph.add((instance_uri, predicate_uri, literal))
            
            self.logger.info(f"RDF ë³€í™˜ ì™„ë£Œ: {len(df)}ê°œ ì¸ìŠ¤í„´ìŠ¤")
            return True
            
        except Exception as e:
            self.logger.error(f"RDF ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def _apply_ofco_mapping(self, df: pd.DataFrame) -> bool:
        """OFCO ë§¤í•‘ ê·œì¹™ ì ìš©"""
        if not self.ofco_rules:
            return False
        
        try:
            mapping_rules = self.ofco_rules.get('mapping_rules', [])
            
            # ë¹„ìš© ì„¼í„° ë§¤í•‘ìš© ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            text_columns = ['Invoice Line Item', 'Description', 'Charge Description']
            available_text_columns = [col for col in text_columns if col in df.columns]
            
            if not available_text_columns:
                self.logger.warning("OFCO ë§¤í•‘ìš© í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            matched_count = 0
            
            for idx, row in df.iterrows():
                for text_col in available_text_columns:
                    text_value = str(row.get(text_col, ''))
                    if not text_value or text_value == 'nan':
                        continue
                    
                    # ë§¤í•‘ ê·œì¹™ ì ìš©
                    best_match = self._find_ofco_match(text_value, mapping_rules)
                    if best_match:
                        # ê²°ê³¼ ì €ì¥
                        self._save_ofco_mapping(text_value, best_match)
                        matched_count += 1
                        break
            
            self.logger.info(f"OFCO ë§¤í•‘ ì™„ë£Œ: {matched_count}ê±´ ë§¤ì¹­")
            return True
            
        except Exception as e:
            self.logger.error(f"OFCO ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _find_ofco_match(self, text: str, mapping_rules: List[Dict]) -> Optional[Dict]:
        """OFCO ë§¤í•‘ ê·œì¹™ì—ì„œ ìµœì  ë§¤ì¹˜ ì°¾ê¸°"""
        best_match = None
        best_priority = float('inf')
        
        for rule in mapping_rules:
            pattern = rule.get('pattern', '')
            priority = rule.get('priority', 999)
            
            if re.search(pattern, text, re.IGNORECASE):
                if priority < best_priority:
                    best_priority = priority
                    best_match = rule
        
        return best_match
    
    def _save_ofco_mapping(self, text: str, match: Dict):
        """OFCO ë§¤í•‘ ê²°ê³¼ ì €ì¥"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT INTO ofco_mappings 
                (source_text, matched_pattern, cost_center_a, cost_center_b, confidence_score)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                text,
                match.get('pattern', ''),
                match.get('cost_center_a', ''),
                match.get('cost_center_b', ''),
                1.0 / match.get('priority', 999)
            ))
            self.conn.commit()
        except Exception as e:
            self.logger.error(f"OFCO ë§¤í•‘ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_results(self, df: pd.DataFrame, validation: ValidationResult):
        """ê²°ê³¼ ì €ì¥"""
        try:
            cursor = self.conn.cursor()
            
            # ê²€ì¦ ë¡œê·¸ ì €ì¥
            cursor.execute('''
                INSERT INTO validation_logs 
                (validation_type, record_count, success_count, error_count, confidence_avg, details)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                'ontology_mapping',
                validation.processed_records,
                validation.processed_records - len(validation.errors),
                len(validation.errors),
                validation.confidence,
                json.dumps({
                    'errors': validation.errors,
                    'warnings': validation.warnings
                }, ensure_ascii=False)
            ))
            
            # ì˜¨í†¨ë¡œì§€ ë°ì´í„° ì €ì¥ (ìƒ˜í”Œ)
            for idx, row in df.head(100).iterrows():  # ì²˜ìŒ 100ê±´ë§Œ ì €ì¥
                cursor.execute('''
                    INSERT INTO ontology_data 
                    (record_id, case_no, date_value, location, quantity, amount, 
                     category, vendor, validation_status, confidence_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    f"REC_{idx:06d}",
                    str(row.get('Case_No', '')),
                    str(row.get('Date', '')),
                    str(row.get('Location', '')),
                    row.get('Qty', 0) if pd.notna(row.get('Qty')) else 0,
                    row.get('Amount', 0.0) if pd.notna(row.get('Amount')) else 0.0,
                    str(row.get('Category', '')),
                    str(row.get('Vendor', '')),
                    'PROCESSED',
                    validation.confidence
                ))
            
            self.conn.commit()
            self.logger.info("ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_sparql_queries(self) -> Dict[str, str]:
        """SPARQL ì¿¼ë¦¬ ìƒì„±"""
        if not self.graph:
            return {}
        
        try:
            templates = self.mapping_rules.get('sparql_templates', {})
            namespace = self.config.namespace
            
            queries = {}
            
            # ê¸°ë³¸ ì¿¼ë¦¬ë“¤
            basic_queries = templates.get('basic_queries', {})
            for name, template in basic_queries.items():
                queries[name] = template.format(namespace=namespace)
            
            # ê³ ê¸‰ ì¿¼ë¦¬ë“¤
            advanced_queries = templates.get('advanced_queries', {})
            for name, template in advanced_queries.items():
                queries[f"advanced_{name}"] = template.format(namespace=namespace)
            
            self.logger.info(f"SPARQL ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {len(queries)}ê°œ")
            return queries
            
        except Exception as e:
            self.logger.error(f"SPARQL ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def execute_sparql_query(self, query: str) -> List[Dict]:
        """SPARQL ì¿¼ë¦¬ ì‹¤í–‰"""
        if not self.graph:
            return []
        
        try:
            results = []
            qres = self.graph.query(query)
            
            for row in qres:
                result_dict = {}
                for i, var in enumerate(qres.vars):
                    result_dict[str(var)] = str(row[i]) if row[i] else None
                results.append(result_dict)
            
            self.logger.info(f"SPARQL ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ: {len(results)}ê±´ ê²°ê³¼")
            return results
            
        except Exception as e:
            self.logger.error(f"SPARQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def export_rdf(self, output_file: str, format: str = 'turtle') -> bool:
        """RDF ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        if not self.graph:
            return False
        
        try:
            self.graph.serialize(destination=output_file, format=format)
            self.logger.info(f"RDF ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"RDF ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def generate_report(self) -> Dict[str, Any]:
        """í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            cursor = self.conn.cursor()
            
            # ì²˜ë¦¬ í†µê³„
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_records,
                    AVG(confidence_score) as avg_confidence,
                    COUNT(DISTINCT validation_status) as status_types
                FROM ontology_data
            ''')
            processing_stats = cursor.fetchone()
            
            # OFCO ë§¤í•‘ í†µê³„
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_mappings,
                    COUNT(DISTINCT cost_center_a) as unique_cost_centers,
                    AVG(confidence_score) as avg_mapping_confidence
                FROM ofco_mappings
            ''')
            ofco_stats = cursor.fetchone()
            
            # ê²€ì¦ ë¡œê·¸ ìš”ì•½
            cursor.execute('''
                SELECT 
                    validation_type,
                    SUM(record_count) as total_processed,
                    AVG(confidence_avg) as avg_confidence,
                    SUM(error_count) as total_errors
                FROM validation_logs
                GROUP BY validation_type
            ''')
            validation_summary = cursor.fetchall()
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'system_version': self.config.version,
                'processing_statistics': {
                    'total_records': processing_stats[0] if processing_stats else 0,
                    'average_confidence': processing_stats[1] if processing_stats else 0,
                    'status_types': processing_stats[2] if processing_stats else 0
                },
                'ofco_mapping_statistics': {
                    'total_mappings': ofco_stats[0] if ofco_stats else 0,
                    'unique_cost_centers': ofco_stats[1] if ofco_stats else 0,
                    'average_confidence': ofco_stats[2] if ofco_stats else 0
                },
                'validation_summary': [
                    {
                        'type': row[0],
                        'processed': row[1],
                        'confidence': row[2],
                        'errors': row[3]
                    } for row in validation_summary
                ],
                'rdf_graph_size': len(self.graph) if self.graph else 0,
                'configuration': {
                    'namespace': self.config.namespace,
                    'enable_rdf': self.config.enable_rdf,
                    'enable_ofco_mapping': self.config.enable_ofco_mapping,
                    'confidence_threshold': self.config.confidence_threshold
                }
            }
            
            self.logger.info("í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            return report
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def close(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        try:
            if hasattr(self, 'conn') and self.conn:
                self.conn.close()
            self.logger.info("HVDC í†µí•© ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ì¢…ë£Œ")
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”Œ HVDC í”„ë¡œì íŠ¸ í†µí•© ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œìŠ¤í…œ v3.0.0")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    config = OntologyConfig()
    system = HVDCOntologyUnifiedSystem(config)
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ (ì˜ˆì‹œ)
        test_files = [
            "HVDC_ì‹¤ì œë°ì´í„°_ì™„ì „í†µí•©_20250704_111552.xlsx",
            "ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°_20250704_105737.xlsx"
        ]
        
        for test_file in test_files:
            if Path(test_file).exists():
                print(f"\nğŸ“Š ì²˜ë¦¬ ì¤‘: {test_file}")
                result = system.process_excel_data(test_file)
                
                print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ - ì‹ ë¢°ë„: {result.confidence:.3f}")
                if result.errors:
                    print(f"âŒ ì˜¤ë¥˜: {len(result.errors)}ê°œ")
                if result.warnings:
                    print(f"âš ï¸ ê²½ê³ : {len(result.warnings)}ê°œ")
            else:
                print(f"âš ï¸ íŒŒì¼ ë¯¸ë°œê²¬: {test_file}")
        
        # SPARQL ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰ (ì˜ˆì‹œ)
        queries = system.generate_sparql_queries()
        if queries:
            print(f"\nğŸ” SPARQL ì¿¼ë¦¬ {len(queries)}ê°œ ìƒì„± ì™„ë£Œ")
            
            # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹¤í–‰ ì˜ˆì‹œ
            first_query_name = list(queries.keys())[0]
            first_query = queries[first_query_name]
            results = system.execute_sparql_query(first_query)
            print(f"   â”” {first_query_name}: {len(results)}ê±´ ê²°ê³¼")
        
        # RDF ë‚´ë³´ë‚´ê¸°
        if system.graph:
            rdf_output = f"hvdc_unified_ontology_{datetime.now().strftime('%Y%m%d_%H%M%S')}.ttl"
            if system.export_rdf(rdf_output):
                print(f"ğŸ“„ RDF ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {rdf_output}")
        
        # í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
        report = system.generate_report()
        if report:
            report_file = f"hvdc_ontology_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“‹ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
        
        print(f"\nğŸ¯ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   â”” ì´ ì²˜ë¦¬: {report.get('processing_statistics', {}).get('total_records', 0)}ê±´")
        print(f"   â”” í‰ê·  ì‹ ë¢°ë„: {report.get('processing_statistics', {}).get('average_confidence', 0):.3f}")
        print(f"   â”” OFCO ë§¤í•‘: {report.get('ofco_mapping_statistics', {}).get('total_mappings', 0)}ê±´")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        system.close()

if __name__ == "__main__":
    main() 
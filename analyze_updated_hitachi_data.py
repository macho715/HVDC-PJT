#!/usr/bin/env python3
"""
MACHO-GPT v3.4-mini | ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¶„ì„
Samsung C&T Ã— ADNOC DSV Partnership | HVDC í”„ë¡œì íŠ¸

ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° íŒŒì¼ ë¶„ì„:
- ë°ì´í„° êµ¬ì¡° ë³€ê²½ì‚¬í•­ í™•ì¸
- ê±´ìˆ˜ ë³€í™” ë¶„ì„
- ì»¬ëŸ¼ êµ¬ì¡° ê²€ì¦
- Flow Code ì˜í–¥ë„ í‰ê°€
- ì„¸ë¶€ ë¡œì§ ë³´ê°• ë°©í–¥ ìˆ˜ë¦½

Enhanced Integration: 
âœ… ì‹¤ì‹œê°„ ë°ì´í„° ë³€ê²½ ê°ì§€
âœ… ìë™ ì˜í–¥ë„ í‰ê°€
âœ… ë¡œì§ ë³´ì • ìš°ì„ ìˆœìœ„ ê²°ì •
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
import logging
from pathlib import Path
import traceback
from typing import Dict, List, Tuple, Optional, Any

# ê¸°ì¡´ ì‹œìŠ¤í…œ import
try:
    from improved_flow_code_system import ImprovedFlowCodeSystem
    from inventory_location_consistency import validate_quantity_consistency
except ImportError as e:
    print(f"âš ï¸ ê¸°ì¡´ ì‹œìŠ¤í…œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

class UpdatedHitachiDataAnalyzer:
    """ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.mode = "LATTICE"
        
        # íŒŒì¼ ê²½ë¡œ
        self.hitachi_file = "hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_HITACHI(HE).xlsx"
        self.simense_file = "hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx"
        
        # ê¸°ì¡´ ê¸°ì¤€ê°’ (ì´ì „ ë¶„ì„ ê²°ê³¼)
        self.previous_expectations = {
            'HITACHI': {
                'total_count': 5346,
                'flow_distribution': {0: 1819, 1: 2561, 2: 886, 3: 80},
                'expected_columns': [
                    'Case No.', 'Package', 'DSV Indoor', 'DSV Outdoor', 
                    'DSV Al Markaz', 'MOSB', 'MIR', 'SHU', 'DAS', 'AGI'
                ]
            }
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {}
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - MACHO-GPT - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ê°œì„ ëœ Flow Code ì‹œìŠ¤í…œ
        try:
            self.flow_system = ImprovedFlowCodeSystem()
        except:
            self.flow_system = None
            print("âš ï¸ Flow Code ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
    
    def load_updated_data(self) -> Dict[str, pd.DataFrame]:
        """ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¡œë“œ ì‹œì‘...")
        print(f"ğŸ¯ {self.mode} ëª¨ë“œ: ì •ë°€ ë°ì´í„° ë¶„ì„")
        
        data_frames = {}
        
        try:
            # HITACHI ë°ì´í„° ë¡œë“œ
            if os.path.exists(self.hitachi_file):
                print(f"   ğŸ“Š HITACHI ë°ì´í„° ë¡œë“œ: {self.hitachi_file}")
                
                # íŒŒì¼ ì •ë³´ í™•ì¸
                file_size = os.path.getsize(self.hitachi_file) / (1024 * 1024)  # MB
                print(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                
                df_hitachi = pd.read_excel(self.hitachi_file)
                data_frames['HITACHI'] = df_hitachi
                
                print(f"   âœ… HITACHI: {len(df_hitachi):,}ê±´ ë¡œë“œ ì™„ë£Œ")
                print(f"   ğŸ“‹ ì»¬ëŸ¼ ìˆ˜: {len(df_hitachi.columns)}ê°œ")
                
            else:
                print(f"   âŒ HITACHI íŒŒì¼ ì—†ìŒ: {self.hitachi_file}")
                return {}
            
            # SIMENSE ë°ì´í„°ë„ ë¹„êµìš©ìœ¼ë¡œ ë¡œë“œ
            if os.path.exists(self.simense_file):
                print(f"   ğŸ“Š SIMENSE ë°ì´í„° ë¡œë“œ (ë¹„êµìš©): {self.simense_file}")
                df_simense = pd.read_excel(self.simense_file)
                data_frames['SIMENSE'] = df_simense
                print(f"   âœ… SIMENSE: {len(df_simense):,}ê±´ ë¡œë“œ ì™„ë£Œ")
            
            return data_frames
            
        except Exception as e:
            print(f"   âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}
    
    def analyze_data_changes(self, df_hitachi: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„"""
        print("\n" + "="*80)
        print("ğŸ” ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„")
        print("="*80)
        
        changes_analysis = {
            'record_count_change': {},
            'column_structure_change': {},
            'data_quality_metrics': {},
            'impact_assessment': {}
        }
        
        # 1. ë ˆì½”ë“œ ìˆ˜ ë³€í™” ë¶„ì„
        current_count = len(df_hitachi)
        previous_count = self.previous_expectations['HITACHI']['total_count']
        count_difference = current_count - previous_count
        
        changes_analysis['record_count_change'] = {
            'previous_count': previous_count,
            'current_count': current_count,
            'difference': count_difference,
            'change_percentage': (count_difference / previous_count) * 100,
            'significant_change': abs(count_difference) > 100
        }
        
        print(f"ğŸ“Š ë ˆì½”ë“œ ìˆ˜ ë³€í™”:")
        print(f"   ì´ì „: {previous_count:,}ê±´")
        print(f"   í˜„ì¬: {current_count:,}ê±´")
        print(f"   ì°¨ì´: {count_difference:+,}ê±´ ({changes_analysis['record_count_change']['change_percentage']:+.1f}%)")
        
        # 2. ì»¬ëŸ¼ êµ¬ì¡° ë³€í™” ë¶„ì„
        current_columns = set(df_hitachi.columns)
        expected_columns = set(self.previous_expectations['HITACHI']['expected_columns'])
        
        new_columns = current_columns - expected_columns
        missing_columns = expected_columns - current_columns
        common_columns = current_columns & expected_columns
        
        changes_analysis['column_structure_change'] = {
            'total_columns': len(current_columns),
            'new_columns': list(new_columns),
            'missing_columns': list(missing_columns),
            'common_columns': list(common_columns),
            'structure_changed': len(new_columns) > 0 or len(missing_columns) > 0
        }
        
        print(f"\nğŸ“‹ ì»¬ëŸ¼ êµ¬ì¡° ë³€í™”:")
        print(f"   ì „ì²´ ì»¬ëŸ¼: {len(current_columns)}ê°œ")
        print(f"   ìƒˆ ì»¬ëŸ¼: {len(new_columns)}ê°œ {list(new_columns)[:5]}")
        print(f"   ëˆ„ë½ ì»¬ëŸ¼: {len(missing_columns)}ê°œ {list(missing_columns)}")
        print(f"   ê³µí†µ ì»¬ëŸ¼: {len(common_columns)}ê°œ")
        
        # 3. ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
        quality_metrics = {}
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ì™„ì „ì„± í™•ì¸
        essential_columns = ['Case No.', 'Package']
        for col in essential_columns:
            if col in df_hitachi.columns:
                completeness = df_hitachi[col].notna().sum() / len(df_hitachi)
                quality_metrics[f'{col}_completeness'] = completeness
                print(f"   {col} ì™„ì „ì„±: {completeness:.1%}")
        
        # ì¤‘ë³µ ë°ì´í„° í™•ì¸
        if 'Case No.' in df_hitachi.columns:
            duplicates = df_hitachi.duplicated(subset=['Case No.']).sum()
            duplicate_rate = duplicates / len(df_hitachi)
            quality_metrics['duplicate_rate'] = duplicate_rate
            print(f"   ì¤‘ë³µë¥ : {duplicate_rate:.1%} ({duplicates}ê±´)")
        
        changes_analysis['data_quality_metrics'] = quality_metrics
        
        return changes_analysis
    
    def analyze_flow_code_impact(self, df_hitachi: pd.DataFrame) -> Dict[str, Any]:
        """Flow Code ì˜í–¥ë„ ë¶„ì„"""
        print("\n" + "="*80)
        print("ğŸ¯ Flow Code ì˜í–¥ë„ ë¶„ì„")
        print("="*80)
        
        if not self.flow_system:
            print("   âš ï¸ Flow Code ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
            return {}
        
        flow_impact = {
            'current_distribution': {},
            'expected_distribution': {},
            'discrepancies': {},
            'improvement_opportunities': []
        }
        
        try:
            # í˜„ì¬ ë°ì´í„°ë¡œ Flow Code ê³„ì‚°
            print("   ğŸ”§ í˜„ì¬ ë°ì´í„°ë¡œ Flow Code ê³„ì‚° ì¤‘...")
            processed_df = self.flow_system.process_data_with_improved_logic_v2(df_hitachi)
            
            if 'FLOW_CODE_IMPROVED_V2' in processed_df.columns:
                current_distribution = processed_df['FLOW_CODE_IMPROVED_V2'].value_counts().sort_index()
                flow_impact['current_distribution'] = dict(current_distribution)
                
                # ê¸°ëŒ€ê°’ê³¼ ë¹„êµ
                expected_distribution = self.previous_expectations['HITACHI']['flow_distribution']
                flow_impact['expected_distribution'] = expected_distribution
                
                print("   ğŸ“ˆ Flow Code ë¶„í¬ ë¶„ì„:")
                print("   Code | í˜„ì¬    | ê¸°ëŒ€    | ì°¨ì´     | ì •í™•ë„")
                print("   -----|---------|---------|----------|--------")
                
                total_error = 0
                for code in [0, 1, 2, 3]:
                    current = flow_impact['current_distribution'].get(code, 0)
                    expected = expected_distribution.get(code, 0)
                    difference = current - expected
                    accuracy = 1 - (abs(difference) / expected) if expected > 0 else 1
                    
                    total_error += abs(difference)
                    
                    flow_impact['discrepancies'][code] = {
                        'current': current,
                        'expected': expected,
                        'difference': difference,
                        'accuracy': accuracy
                    }
                    
                    status = "âœ…" if abs(difference) <= 50 else "âš ï¸" if abs(difference) <= 200 else "âŒ"
                    print(f"   {code:4} | {current:7,} | {expected:7,} | {difference:+8,} | {accuracy:6.1%} {status}")
                
                # ì „ì²´ ì •í™•ë„
                overall_accuracy = 1 - (total_error / sum(expected_distribution.values()))
                flow_impact['overall_accuracy'] = overall_accuracy
                print(f"\n   ğŸ“Š ì „ì²´ ì •í™•ë„: {overall_accuracy:.1%}")
                
                # ê°œì„  ê¸°íšŒ ì‹ë³„
                if flow_impact['discrepancies'][0]['accuracy'] < 0.9:
                    flow_impact['improvement_opportunities'].append("FLOW CODE 0 ë¡œì§ ê°œì„  í•„ìš”")
                if flow_impact['discrepancies'][1]['accuracy'] < 0.9:
                    flow_impact['improvement_opportunities'].append("FLOW CODE 1 ë¡œì§ ê°œì„  í•„ìš”")
                if flow_impact['discrepancies'][3]['accuracy'] < 0.9:
                    flow_impact['improvement_opportunities'].append("FLOW CODE 3 ë¡œì§ ê°œì„  í•„ìš”")
                
            else:
                print("   âŒ Flow Code ê³„ì‚° ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"   âŒ Flow Code ë¶„ì„ ì˜¤ë¥˜: {e}")
            self.logger.error(f"Flow Code ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return flow_impact
    
    def identify_logic_enhancement_priorities(self, changes_analysis: Dict, flow_impact: Dict) -> List[Dict[str, Any]]:
        """ë¡œì§ ë³´ê°• ìš°ì„ ìˆœìœ„ ì‹ë³„"""
        print("\n" + "="*80)
        print("ğŸš€ ë¡œì§ ë³´ê°• ìš°ì„ ìˆœìœ„ ë¶„ì„")
        print("="*80)
        
        priorities = []
        
        # 1. Flow Code 0 ë¡œì§ (Pre Arrival) ê°œì„ 
        if flow_impact.get('discrepancies', {}).get(0, {}).get('accuracy', 1) < 0.8:
            difference = flow_impact['discrepancies'][0]['difference']
            priority_score = abs(difference) / 100  # 100ê±´ë‹¹ 1ì 
            
            priorities.append({
                'priority': 1,
                'category': 'FLOW_CODE_0_LOGIC',
                'title': 'Pre Arrival ë¡œì§ ì •êµí™”',
                'description': f"í˜„ì¬ {difference:+,}ê±´ ì˜¤ì°¨, Pre Arrival ì‹ë³„ ë¡œì§ ê°œì„  í•„ìš”",
                'impact_score': priority_score,
                'estimated_effort': 'HIGH',
                'business_value': 'CRITICAL',
                'implementation_complexity': 'MEDIUM'
            })
        
        # 2. Flow Code 1 ë¡œì§ (ì§ì†¡) ê°œì„   
        if flow_impact.get('discrepancies', {}).get(1, {}).get('accuracy', 1) < 0.8:
            difference = flow_impact['discrepancies'][1]['difference']
            priority_score = abs(difference) / 100
            
            priorities.append({
                'priority': 2,
                'category': 'FLOW_CODE_1_LOGIC',
                'title': 'Port â†’ Site ì§ì†¡ ë¡œì§ ê°œì„ ',
                'description': f"í˜„ì¬ {difference:+,}ê±´ ì˜¤ì°¨, ì§ì†¡ ê²½ë¡œ ì‹ë³„ ì •êµí™” í•„ìš”",
                'impact_score': priority_score,
                'estimated_effort': 'MEDIUM',
                'business_value': 'HIGH',
                'implementation_complexity': 'LOW'
            })
        
        # 3. Flow Code 3 ë¡œì§ (ë‹¤ë‹¨ê³„) ê°œì„ 
        if flow_impact.get('discrepancies', {}).get(3, {}).get('accuracy', 1) < 0.8:
            difference = flow_impact['discrepancies'][3]['difference']
            priority_score = abs(difference) / 100
            
            priorities.append({
                'priority': 3,
                'category': 'FLOW_CODE_3_LOGIC',
                'title': 'ë‹¤ë‹¨ê³„ ê²½ìœ  ë¡œì§ ìµœì í™”',
                'description': f"í˜„ì¬ {difference:+,}ê±´ ì˜¤ì°¨, MOSB ê²½ìœ  ë° ë³µì¡ ê²½ë¡œ ë¡œì§ ê°œì„ ",
                'impact_score': priority_score,
                'estimated_effort': 'HIGH',
                'business_value': 'MEDIUM',
                'implementation_complexity': 'HIGH'
            })
        
        # 4. ë°ì´í„° í’ˆì§ˆ ê°œì„ 
        if changes_analysis.get('data_quality_metrics', {}).get('duplicate_rate', 0) > 0.01:
            priorities.append({
                'priority': 4,
                'category': 'DATA_QUALITY',
                'title': 'ë°ì´í„° í’ˆì§ˆ ê°œì„ ',
                'description': "ì¤‘ë³µ ë°ì´í„° ì œê±° ë° ë°ì´í„° ì •í•©ì„± ê°•í™”",
                'impact_score': changes_analysis['data_quality_metrics']['duplicate_rate'] * 100,
                'estimated_effort': 'LOW',
                'business_value': 'MEDIUM',
                'implementation_complexity': 'LOW'
            })
        
        # 5. ì‹œìŠ¤í…œ í†µí•© ìµœì í™”
        overall_accuracy = flow_impact.get('overall_accuracy', 1.0)
        if overall_accuracy < 0.95:
            priorities.append({
                'priority': 5,
                'category': 'SYSTEM_INTEGRATION',
                'title': 'ì „ì²´ ì‹œìŠ¤í…œ ê· í˜• ì¡°ì •',
                'description': f"í˜„ì¬ {overall_accuracy:.1%} ì •í™•ë„, 95% ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì¢…í•© ìµœì í™”",
                'impact_score': (0.95 - overall_accuracy) * 1000,
                'estimated_effort': 'HIGH',
                'business_value': 'CRITICAL',
                'implementation_complexity': 'HIGH'
            })
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬ (impact_score ê¸°ì¤€)
        priorities.sort(key=lambda x: x['impact_score'], reverse=True)
        
        # ìš°ì„ ìˆœìœ„ ì¬ë¶€ì—¬
        for i, priority in enumerate(priorities, 1):
            priority['priority'] = i
        
        print("ğŸ“‹ ì‹ë³„ëœ ë¡œì§ ë³´ê°• ìš°ì„ ìˆœìœ„:")
        for priority in priorities:
            print(f"   {priority['priority']}. {priority['title']}")
            print(f"      ğŸ“Š ì˜í–¥ë„: {priority['impact_score']:.1f}")
            print(f"      ğŸ”§ ë…¸ë ¥: {priority['estimated_effort']}")
            print(f"      ğŸ’° ê°€ì¹˜: {priority['business_value']}")
            print(f"      ğŸ—ï¸ ë³µì¡ë„: {priority['implementation_complexity']}")
            print()
        
        return priorities
    
    def generate_enhancement_roadmap(self, priorities: List[Dict]) -> Dict[str, Any]:
        """ë¡œì§ ë³´ê°• ë¡œë“œë§µ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ—ºï¸ ë¡œì§ ë³´ê°• ë¡œë“œë§µ ìƒì„±")
        print("="*80)
        
        roadmap = {
            'phase_1_immediate': [],
            'phase_2_short_term': [],
            'phase_3_long_term': [],
            'implementation_timeline': {},
            'resource_requirements': {},
            'success_metrics': {}
        }
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ í˜ì´ì¦ˆ ë¶„ë¥˜
        for priority in priorities:
            if priority['priority'] <= 2 and priority['business_value'] == 'CRITICAL':
                roadmap['phase_1_immediate'].append(priority)
            elif priority['priority'] <= 4 or priority['estimated_effort'] in ['LOW', 'MEDIUM']:
                roadmap['phase_2_short_term'].append(priority)
            else:
                roadmap['phase_3_long_term'].append(priority)
        
        # êµ¬í˜„ íƒ€ì„ë¼ì¸
        roadmap['implementation_timeline'] = {
            'phase_1': '1-2ì£¼ (ì¦‰ì‹œ ì‹œì‘)',
            'phase_2': '3-6ì£¼ (ë‹¨ê¸° ê°œì„ )',
            'phase_3': '2-3ê°œì›” (ì¥ê¸° ìµœì í™”)'
        }
        
        # ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­
        roadmap['resource_requirements'] = {
            'development_time': 'ì´ 8-12ì£¼',
            'testing_time': 'ì´ 3-4ì£¼',
            'key_skills': ['Python/Pandas', 'TDD', 'ë¬¼ë¥˜ ë„ë©”ì¸ ì§€ì‹', 'Excel ë¡œì§'],
            'team_size': '2-3ëª… (ì‹œë‹ˆì–´ ê°œë°œì 1ëª…, ì£¼ë‹ˆì–´ 1-2ëª…)'
        }
        
        # ì„±ê³µ ì§€í‘œ
        roadmap['success_metrics'] = {
            'overall_accuracy': 'â‰¥95%',
            'flow_code_0_accuracy': 'â‰¥90%',
            'flow_code_1_accuracy': 'â‰¥90%',
            'flow_code_3_accuracy': 'â‰¥85%',
            'processing_speed': 'â‰¥1000ê±´/ì´ˆ',
            'data_quality': 'â‰¥98% ì™„ì „ì„±, <1% ì¤‘ë³µë¥ '
        }
        
        print("ğŸ¯ Phase 1 - ì¦‰ì‹œ ê°œì„  (1-2ì£¼):")
        for item in roadmap['phase_1_immediate']:
            print(f"   âœ… {item['title']}")
        
        print("\nğŸ¯ Phase 2 - ë‹¨ê¸° ê°œì„  (3-6ì£¼):")
        for item in roadmap['phase_2_short_term']:
            print(f"   ğŸ”§ {item['title']}")
        
        print("\nğŸ¯ Phase 3 - ì¥ê¸° ìµœì í™” (2-3ê°œì›”):")
        for item in roadmap['phase_3_long_term']:
            print(f"   ğŸš€ {item['title']}")
        
        return roadmap
    
    def save_analysis_report(self, data_analysis: Dict, flow_impact: Dict, priorities: List, roadmap: Dict):
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥"""
        print("\n" + "="*80)
        print("ğŸ“ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥")
        print("="*80)
        
        report = {
            'analysis_timestamp': self.timestamp,
            'mode': self.mode,
            'data_analysis': data_analysis,
            'flow_code_impact': flow_impact,
            'enhancement_priorities': priorities,
            'implementation_roadmap': roadmap,
            'next_actions': [
                '1. Phase 1 ì¦‰ì‹œ ê°œì„  í•­ëª© êµ¬í˜„',
                '2. TDD ë°©ë²•ë¡ ìœ¼ë¡œ ê° ë¡œì§ ê°œì„ ',
                '3. ì‹¤ì œ ë°ì´í„°ë¡œ ê²€ì¦ ë° ì„±ëŠ¥ ì¸¡ì •',
                '4. í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„'
            ]
        }
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_filename = f"HITACHI_Updated_Analysis_Report_{self.timestamp}.json"
        try:
            import json
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"   ğŸ“Š JSON ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")
        except Exception as e:
            print(f"   âš ï¸ JSON ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        md_filename = f"HITACHI_Updated_Analysis_Report_{self.timestamp}.md"
        try:
            with open(md_filename, 'w', encoding='utf-8') as f:
                f.write(f"# HITACHI ë°ì´í„° ì—…ë°ì´íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
                f.write(f"**ë¶„ì„ ì‹œê°**: {self.timestamp}\n")
                f.write(f"**ë¶„ì„ ëª¨ë“œ**: {self.mode}\n\n")
                
                f.write("## ğŸ“Š ë°ì´í„° ë³€ê²½ì‚¬í•­\n\n")
                f.write(f"- **ë ˆì½”ë“œ ìˆ˜ ë³€í™”**: {data_analysis['record_count_change']['difference']:+,}ê±´\n")
                f.write(f"- **ì»¬ëŸ¼ êµ¬ì¡° ë³€í™”**: {'ìˆìŒ' if data_analysis['column_structure_change']['structure_changed'] else 'ì—†ìŒ'}\n")
                
                f.write("\n## ğŸ¯ Flow Code ì˜í–¥ë„\n\n")
                if flow_impact.get('overall_accuracy'):
                    f.write(f"- **ì „ì²´ ì •í™•ë„**: {flow_impact['overall_accuracy']:.1%}\n")
                
                f.write("\n## ğŸš€ ê°œì„  ìš°ì„ ìˆœìœ„\n\n")
                for i, priority in enumerate(priorities, 1):
                    f.write(f"{i}. **{priority['title']}**\n")
                    f.write(f"   - ì˜í–¥ë„: {priority['impact_score']:.1f}\n")
                    f.write(f"   - ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜: {priority['business_value']}\n\n")
                
            print(f"   ğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {md_filename}")
        except Exception as e:
            print(f"   âš ï¸ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return report
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ MACHO-GPT v3.4-mini | ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ì¢…í•© ë¶„ì„")
        print("ğŸ¯ LATTICE ëª¨ë“œ: ì •ë°€ ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„")
        print("Samsung C&T Ã— ADNOC DSV Partnership | HVDC í”„ë¡œì íŠ¸")
        
        try:
            # 1. ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ë¡œë“œ
            data_frames = self.load_updated_data()
            
            if 'HITACHI' not in data_frames:
                print("âŒ HITACHI ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return {'error': 'DATA_LOAD_FAILED'}
            
            df_hitachi = data_frames['HITACHI']
            
            # 2. ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¶„ì„
            data_analysis = self.analyze_data_changes(df_hitachi)
            
            # 3. Flow Code ì˜í–¥ë„ ë¶„ì„
            flow_impact = self.analyze_flow_code_impact(df_hitachi)
            
            # 4. ë¡œì§ ë³´ê°• ìš°ì„ ìˆœìœ„ ì‹ë³„
            priorities = self.identify_logic_enhancement_priorities(data_analysis, flow_impact)
            
            # 5. êµ¬í˜„ ë¡œë“œë§µ ìƒì„±
            roadmap = self.generate_enhancement_roadmap(priorities)
            
            # 6. ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
            final_report = self.save_analysis_report(data_analysis, flow_impact, priorities, roadmap)
            
            # 7. ìµœì¢… ê²°ê³¼ ìš”ì•½
            print("\n" + "="*100)
            print("ğŸ† HITACHI ë°ì´í„° ì—…ë°ì´íŠ¸ ë¶„ì„ ì™„ë£Œ")
            print("="*100)
            
            overall_accuracy = flow_impact.get('overall_accuracy', 0)
            critical_changes = data_analysis['record_count_change']['significant_change']
            high_priority_items = len([p for p in priorities if p['business_value'] == 'CRITICAL'])
            
            print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
            print(f"   - ì „ì²´ ì •í™•ë„: {overall_accuracy:.1%}")
            print(f"   - ì¤‘ëŒ€í•œ ë°ì´í„° ë³€í™”: {'ìˆìŒ' if critical_changes else 'ì—†ìŒ'}")
            print(f"   - ê¸´ê¸‰ ê°œì„  í•­ëª©: {high_priority_items}ê°œ")
            print(f"   - ì „ì²´ ê°œì„  í•­ëª©: {len(priorities)}ê°œ")
            
            # ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ
            print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ:")
            if high_priority_items > 0:
                print(f"   ğŸš¨ ì¦‰ì‹œ Phase 1 ê°œì„  í•­ëª© êµ¬í˜„ ì‹œì‘")
                print(f"   ğŸ”§ TDD ë°©ë²•ë¡ ìœ¼ë¡œ ë¡œì§ ë³´ì •")
                print(f"   ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ êµ¬ì¶•")
            else:
                print(f"   âœ… í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì–‘í˜¸")
                print(f"   ğŸ”§ ì ì§„ì  ì„±ëŠ¥ ìµœì í™” ì§„í–‰")
                print(f"   ğŸ“ˆ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„")
            
            return final_report
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return {'error': str(e), 'traceback': traceback.format_exc()}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”Œ MACHO-GPT v3.4-mini ì—…ë°ì´íŠ¸ëœ HITACHI ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ")
    print("Enhanced MCP Integration | Samsung C&T Logistics")
    print("="*80)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = UpdatedHitachiDataAnalyzer()
    
    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    final_report = analyzer.run_comprehensive_analysis()
    
    # ì¢…ë£Œ ì½”ë“œ ê²°ì •
    if 'error' in final_report:
        exit_code = 2  # ì˜¤ë¥˜
    elif final_report.get('enhancement_priorities', []):
        priority_count = len([p for p in final_report['enhancement_priorities'] if p['business_value'] == 'CRITICAL'])
        exit_code = 1 if priority_count > 0 else 0  # ê°œì„  í•„ìš” vs ì–‘í˜¸
    else:
        exit_code = 0  # ì–‘í˜¸
    
    print(f"\nğŸ ë¶„ì„ ì™„ë£Œ (ì¢…ë£Œ ì½”ë“œ: {exit_code})")
    return exit_code

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 
#!/usr/bin/env python3
"""
ğŸ” HVDC í´ë¦¬ë‹ëœ ë°ì´í„° ì¬ê²€ì¦ ì‹œìŠ¤í…œ
MACHO-GPT v3.4-mini â”‚ Samsung C&T & ADNOCÂ·DSV Partnership

í´ë¦¬ë‹ í›„ í’ˆì§ˆ ê°œì„  ê²€ì¦:
1. SPARQL ê·œì¹™ ì¬ì ìš©
2. í’ˆì§ˆ ì ìˆ˜ ì¬ê³„ì‚°
3. ê°œì„  íš¨ê³¼ ì¸¡ì •
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
import json
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CleanedDataValidator:
    """í´ë¦¬ë‹ëœ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self, cleaned_data_dir="data_cleaned"):
        self.cleaned_data_dir = cleaned_data_dir
        self.validation_results = {}
        
    def validate_cleaned_data(self):
        """í´ë¦¬ë‹ëœ ë°ì´í„° ê²€ì¦ ì‹¤í–‰"""
        logger.info("ğŸ” í´ë¦¬ë‹ëœ ë°ì´í„° ì¬ê²€ì¦ ì‹œì‘")
        
        validation_summary = {
            'timestamp': datetime.now().isoformat(),
            'validation_type': 'post_cleaning',
            'files_validated': {},
            'overall_quality_score': 0,
            'improvement_metrics': {},
            'sparql_compliance': {}
        }
        
        # í´ë¦¬ë‹ëœ íŒŒì¼ë“¤ ê²€ì¦
        hitachi_result = self._validate_hitachi_cleaned()
        validation_summary['files_validated']['HITACHI'] = hitachi_result
        
        simense_result = self._validate_simense_cleaned()
        validation_summary['files_validated']['SIMENSE'] = simense_result
        
        invoice_result = self._validate_invoice_cleaned()
        validation_summary['files_validated']['INVOICE'] = invoice_result
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        validation_summary['overall_quality_score'] = self._calculate_overall_quality(validation_summary)
        
        # ê°œì„  íš¨ê³¼ ì¸¡ì •
        validation_summary['improvement_metrics'] = self._calculate_improvement_metrics(validation_summary)
        
        # SPARQL ì»´í”Œë¼ì´ì–¸ìŠ¤ ê²€ì‚¬
        validation_summary['sparql_compliance'] = self._check_sparql_compliance(validation_summary)
        
        self._display_validation_results(validation_summary)
        self._save_validation_results(validation_summary)
        
        return validation_summary
    
    def _validate_hitachi_cleaned(self):
        """í´ë¦¬ë‹ëœ HITACHI íŒŒì¼ ê²€ì¦"""
        logger.info("ğŸ”§ HITACHI í´ë¦¬ë‹ëœ íŒŒì¼ ê²€ì¦")
        
        try:
            # ìµœì‹  í´ë¦¬ë‹ëœ íŒŒì¼ ì°¾ê¸°
            files = [f for f in os.listdir(self.cleaned_data_dir) if 'HITACHI_CLEANED' in f]
            if not files:
                raise FileNotFoundError("HITACHI í´ë¦¬ë‹ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            file_path = os.path.join(self.cleaned_data_dir, files[0])
            df = pd.read_excel(file_path, sheet_name='Case List')
            
            # ê²€ì¦ ë©”íŠ¸ë¦­
            total_records = len(df)
            missing_data_rate = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            duplicate_rate = (df.duplicated().sum() / len(df)) * 100
            
            # ì´ìƒì¹˜ ê²€ì¦
            outlier_rate = self._calculate_outlier_rate(df)
            
            # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
            type_consistency = self._check_type_consistency(df)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = 100 - missing_data_rate - duplicate_rate - outlier_rate - (100 - type_consistency)
            
            result = {
                'file_name': 'HITACHI_CLEANED',
                'file_path': file_path,
                'total_records': total_records,
                'missing_data_rate': round(missing_data_rate, 2),
                'duplicate_rate': round(duplicate_rate, 2),
                'outlier_rate': round(outlier_rate, 2),
                'type_consistency': round(type_consistency, 2),
                'quality_score': round(max(0, quality_score), 2),
                'validation_status': 'PASSED' if quality_score > 80 else 'NEEDS_IMPROVEMENT'
            }
            
            logger.info(f"  âœ… HITACHI ê²€ì¦ ì™„ë£Œ: í’ˆì§ˆ ì ìˆ˜ {result['quality_score']}%")
            return result
            
        except Exception as e:
            logger.error(f"  âŒ HITACHI ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'file_name': 'HITACHI_CLEANED', 'error': str(e)}
    
    def _validate_simense_cleaned(self):
        """í´ë¦¬ë‹ëœ SIMENSE íŒŒì¼ ê²€ì¦ - CBM ê²€ì¦ í¬í•¨"""
        logger.info("ğŸ”§ SIMENSE í´ë¦¬ë‹ëœ íŒŒì¼ ê²€ì¦ - CBM ê²€ì¦")
        
        try:
            # ìµœì‹  í´ë¦¬ë‹ëœ íŒŒì¼ ì°¾ê¸°
            files = [f for f in os.listdir(self.cleaned_data_dir) if 'SIMENSE_CLEANED' in f]
            if not files:
                raise FileNotFoundError("SIMENSE í´ë¦¬ë‹ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            file_path = os.path.join(self.cleaned_data_dir, files[0])
            df = pd.read_excel(file_path, sheet_name='Case List')
            
            # ê¸°ë³¸ ê²€ì¦ ë©”íŠ¸ë¦­
            total_records = len(df)
            missing_data_rate = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            duplicate_rate = (df.duplicated().sum() / len(df)) * 100
            
            # CBM íŠ¹ë³„ ê²€ì¦
            cbm_compliance_rate = 100
            if 'CBM' in df.columns:
                cbm_series = pd.to_numeric(df['CBM'], errors='coerce')
                cbm_violations = (cbm_series <= 0).sum()
                cbm_compliance_rate = ((len(df) - cbm_violations) / len(df)) * 100
            
            # íŒ¨í‚¤ì§€ ìˆ˜ ê²€ì¦
            pkg_compliance_rate = 100
            if 'pkg' in df.columns:
                pkg_series = pd.to_numeric(df['pkg'], errors='coerce')
                pkg_violations = (pkg_series <= 0).sum()
                pkg_compliance_rate = ((len(df) - pkg_violations) / len(df)) * 100
            
            # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
            type_consistency = self._check_type_consistency(df)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (CBM ì»´í”Œë¼ì´ì–¸ìŠ¤ ê°€ì¤‘ì¹˜ ë†’ìŒ)
            quality_score = (
                cbm_compliance_rate * 0.4 +  # CBM ì»´í”Œë¼ì´ì–¸ìŠ¤ 40%
                pkg_compliance_rate * 0.2 +  # PKG ì»´í”Œë¼ì´ì–¸ìŠ¤ 20%
                (100 - missing_data_rate) * 0.2 +  # ëˆ„ë½ ë°ì´í„° 20%
                (100 - duplicate_rate) * 0.1 +     # ì¤‘ë³µ ë°ì´í„° 10%
                type_consistency * 0.1              # íƒ€ì… ì¼ê´€ì„± 10%
            )
            
            result = {
                'file_name': 'SIMENSE_CLEANED',
                'file_path': file_path,
                'total_records': total_records,
                'missing_data_rate': round(missing_data_rate, 2),
                'duplicate_rate': round(duplicate_rate, 2),
                'cbm_compliance_rate': round(cbm_compliance_rate, 2),
                'pkg_compliance_rate': round(pkg_compliance_rate, 2),
                'type_consistency': round(type_consistency, 2),
                'quality_score': round(quality_score, 2),
                'validation_status': 'PASSED' if quality_score > 80 else 'NEEDS_IMPROVEMENT'
            }
            
            logger.info(f"  âœ… SIMENSE ê²€ì¦ ì™„ë£Œ: í’ˆì§ˆ ì ìˆ˜ {result['quality_score']}% (CBM: {result['cbm_compliance_rate']}%)")
            return result
            
        except Exception as e:
            logger.error(f"  âŒ SIMENSE ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'file_name': 'SIMENSE_CLEANED', 'error': str(e)}
    
    def _validate_invoice_cleaned(self):
        """í´ë¦¬ë‹ëœ INVOICE íŒŒì¼ ê²€ì¦"""
        logger.info("ğŸ”§ INVOICE í´ë¦¬ë‹ëœ íŒŒì¼ ê²€ì¦")
        
        try:
            # ìµœì‹  í´ë¦¬ë‹ëœ íŒŒì¼ ì°¾ê¸°
            files = [f for f in os.listdir(self.cleaned_data_dir) if 'INVOICE_CLEANED' in f]
            if not files:
                raise FileNotFoundError("INVOICE í´ë¦¬ë‹ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            file_path = os.path.join(self.cleaned_data_dir, files[0])
            xl_file = pd.ExcelFile(file_path)
            df = pd.read_excel(file_path, sheet_name=xl_file.sheet_names[0])
            
            # ê²€ì¦ ë©”íŠ¸ë¦­
            total_records = len(df)
            missing_data_rate = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            duplicate_rate = (df.duplicated().sum() / len(df)) * 100
            
            # ê¸ˆì•¡ ë°ì´í„° ê²€ì¦
            amount_compliance = self._check_amount_compliance(df)
            
            # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
            type_consistency = self._check_type_consistency(df)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = (
                amount_compliance * 0.3 +
                (100 - missing_data_rate) * 0.3 +
                (100 - duplicate_rate) * 0.2 +
                type_consistency * 0.2
            )
            
            result = {
                'file_name': 'INVOICE_CLEANED',
                'file_path': file_path,
                'total_records': total_records,
                'missing_data_rate': round(missing_data_rate, 2),
                'duplicate_rate': round(duplicate_rate, 2),
                'amount_compliance': round(amount_compliance, 2),
                'type_consistency': round(type_consistency, 2),
                'quality_score': round(quality_score, 2),
                'validation_status': 'PASSED' if quality_score > 80 else 'NEEDS_IMPROVEMENT'
            }
            
            logger.info(f"  âœ… INVOICE ê²€ì¦ ì™„ë£Œ: í’ˆì§ˆ ì ìˆ˜ {result['quality_score']}%")
            return result
            
        except Exception as e:
            logger.error(f"  âŒ INVOICE ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'file_name': 'INVOICE_CLEANED', 'error': str(e)}
    
    def _calculate_outlier_rate(self, df):
        """ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°"""
        outlier_count = 0
        total_numeric_values = 0
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            values = df[col].dropna()
            if len(values) > 0:
                Q1 = values.quantile(0.25)
                Q3 = values.quantile(0.75)
                IQR = Q3 - Q1
                if IQR > 0:
                    outliers = ((values < Q1 - 1.5 * IQR) | (values > Q3 + 1.5 * IQR)).sum()
                    outlier_count += outliers
                    total_numeric_values += len(values)
        
        return (outlier_count / total_numeric_values * 100) if total_numeric_values > 0 else 0
    
    def _check_type_consistency(self, df):
        """ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì‚¬"""
        consistent_types = 0
        total_cols = len(df.columns)
        
        for col in df.columns:
            # ì˜ˆìƒ íƒ€ì…ê³¼ ì‹¤ì œ íƒ€ì… ë¹„êµ
            if 'date' in col.lower():
                if pd.api.types.is_datetime64_any_dtype(df[col]):
                    consistent_types += 1
            elif any(keyword in col.lower() for keyword in ['qty', 'amount', 'weight', 'cbm', 'pkg']):
                if pd.api.types.is_numeric_dtype(df[col]):
                    consistent_types += 1
            else:
                consistent_types += 1  # ê¸°íƒ€ ì»¬ëŸ¼ì€ ì¼ê´€ì„± ìˆìŒìœ¼ë¡œ ê°„ì£¼
        
        return (consistent_types / total_cols * 100) if total_cols > 0 else 0
    
    def _check_amount_compliance(self, df):
        """ê¸ˆì•¡ ë°ì´í„° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê²€ì‚¬"""
        compliance_count = 0
        total_amount_values = 0
        
        amount_cols = [col for col in df.columns if 'amount' in col.lower() or 'cost' in col.lower()]
        for col in amount_cols:
            if col in df.columns:
                numeric_series = pd.to_numeric(df[col], errors='coerce')
                valid_amounts = (numeric_series >= 0).sum()
                compliance_count += valid_amounts
                total_amount_values += len(numeric_series.dropna())
        
        return (compliance_count / total_amount_values * 100) if total_amount_values > 0 else 100
    
    def _calculate_overall_quality(self, validation_summary):
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_scores = []
        for file_name, result in validation_summary['files_validated'].items():
            if 'quality_score' in result:
                quality_scores.append(result['quality_score'])
        
        return round(sum(quality_scores) / len(quality_scores), 2) if quality_scores else 0
    
    def _calculate_improvement_metrics(self, validation_summary):
        """ê°œì„  íš¨ê³¼ ì¸¡ì •"""
        # í´ë¦¬ë‹ ì „ í’ˆì§ˆ ì ìˆ˜ (54.4%)ì™€ ë¹„êµ
        before_score = 54.4
        after_score = validation_summary['overall_quality_score']
        
        return {
            'quality_before': before_score,
            'quality_after': after_score,
            'improvement_percentage': round(after_score - before_score, 2),
            'improvement_factor': round(after_score / before_score, 2)
        }
    
    def _check_sparql_compliance(self, validation_summary):
        """SPARQL ê·œì¹™ ì»´í”Œë¼ì´ì–¸ìŠ¤ ê²€ì‚¬"""
        compliance_results = {}
        
        for file_name, result in validation_summary['files_validated'].items():
            if 'error' not in result:
                compliance_score = 0
                
                # CBM ì–‘ìˆ˜ ê²€ì¦ (SIMENSE)
                if 'cbm_compliance_rate' in result:
                    compliance_score += result['cbm_compliance_rate'] * 0.4
                else:
                    compliance_score += 100 * 0.4
                
                # íŒ¨í‚¤ì§€ ìˆ˜ ì–‘ìˆ˜ ê²€ì¦
                if 'pkg_compliance_rate' in result:
                    compliance_score += result['pkg_compliance_rate'] * 0.3
                else:
                    compliance_score += 100 * 0.3
                
                # ê¸ˆì•¡ ë¹„ìŒìˆ˜ ê²€ì¦
                if 'amount_compliance' in result:
                    compliance_score += result['amount_compliance'] * 0.3
                else:
                    compliance_score += 100 * 0.3
                
                compliance_results[file_name] = {
                    'compliance_score': round(compliance_score, 2),
                    'status': 'COMPLIANT' if compliance_score > 95 else 'PARTIAL_COMPLIANCE'
                }
        
        return compliance_results
    
    def _save_validation_results(self, validation_summary):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"HVDC_Cleaned_Data_Validation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(validation_summary, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ ê²€ì¦ ê²°ê³¼ ì €ì¥: {filename}")
    
    def _display_validation_results(self, validation_summary):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ” HVDC í´ë¦¬ë‹ëœ ë°ì´í„° ê²€ì¦ ì™„ë£Œ ë³´ê³ ì„œ")
        print("="*80)
        
        improvement = validation_summary['improvement_metrics']
        print(f"ğŸ“Š í´ë¦¬ë‹ ì „ í’ˆì§ˆ ì ìˆ˜: {improvement['quality_before']:.1f}%")
        print(f"ğŸ“ˆ í´ë¦¬ë‹ í›„ í’ˆì§ˆ ì ìˆ˜: {improvement['quality_after']:.1f}%")
        print(f"ğŸ¯ í’ˆì§ˆ ê°œì„ ë¥ : +{improvement['improvement_percentage']:.1f}%")
        print(f"ğŸ“ ê²€ì¦ ëŒ€ìƒ: {self.cleaned_data_dir}/")
        
        print("\nğŸ“‹ íŒŒì¼ë³„ ê²€ì¦ ê²°ê³¼:")
        for file_name, result in validation_summary['files_validated'].items():
            if 'error' not in result:
                print(f"  ğŸ“„ {result['file_name']}:")
                print(f"    - í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.1f}%")
                print(f"    - ê²€ì¦ ìƒíƒœ: {result['validation_status']}")
                print(f"    - ëˆ„ë½ ë°ì´í„°: {result['missing_data_rate']:.1f}%")
                print(f"    - ì¤‘ë³µ ë°ì´í„°: {result['duplicate_rate']:.1f}%")
                
                # íŠ¹ë³„ ë©”íŠ¸ë¦­
                if 'cbm_compliance_rate' in result:
                    print(f"    - CBM ì»´í”Œë¼ì´ì–¸ìŠ¤: {result['cbm_compliance_rate']:.1f}%")
                if 'amount_compliance' in result:
                    print(f"    - ê¸ˆì•¡ ì»´í”Œë¼ì´ì–¸ìŠ¤: {result['amount_compliance']:.1f}%")
            else:
                print(f"  âŒ {file_name}: {result['error']}")
        
        print("\nğŸ¯ SPARQL ì»´í”Œë¼ì´ì–¸ìŠ¤:")
        for file_name, compliance in validation_summary['sparql_compliance'].items():
            print(f"  ğŸ“„ {file_name}: {compliance['compliance_score']:.1f}% ({compliance['status']})")
        
        overall_quality = validation_summary['overall_quality_score']
        print(f"\nğŸ† ì „ì²´ í‰ê°€:")
        if overall_quality >= 90:
            print("  ğŸŸ¢ ìš°ìˆ˜: ë°ì´í„° í’ˆì§ˆì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤")
        elif overall_quality >= 80:
            print("  ğŸŸ¡ ì–‘í˜¸: ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
        elif overall_quality >= 70:
            print("  ğŸŸ  ë³´í†µ: ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        else:
            print("  ğŸ”´ ê°œì„ í•„ìš”: ì¶”ê°€ í´ë¦¬ë‹ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        print("  âœ… í´ë¦¬ë‹ëœ ë°ì´í„°ë¥¼ í”„ë¡œë•ì…˜ì— í™œìš© ê°€ëŠ¥")
        print("  ğŸ“Š ì •ê¸°ì  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•")
        print("  ğŸ”„ ë°ì´í„° ì…ìˆ˜ ì‹œ ìë™ í´ë¦¬ë‹ íŒŒì´í”„ë¼ì¸ ì ìš©")
        
        print("\n" + "="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    validator = CleanedDataValidator()
    results = validator.validate_cleaned_data()
    
    print(f"\nğŸ”§ **ì¶”ì²œ ëª…ë ¹ì–´:**")
    print(f"/generate-dashboard cleaned-data [í´ë¦¬ë‹ëœ ë°ì´í„° ëŒ€ì‹œë³´ë“œ ìƒì„±]")
    print(f"/export-quality-report {datetime.now().strftime('%Y%m%d')} [í’ˆì§ˆ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°]")
    print(f"/monitor-quality-trends [í’ˆì§ˆ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ì‹œì‘]")
    
    return results


if __name__ == "__main__":
    main() 
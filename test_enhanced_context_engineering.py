#!/usr/bin/env python3
"""
ì—…ê³„ í‘œì¤€ Context Engineering ì‹œìŠ¤í…œ ìƒì„¸ í…ŒìŠ¤íŠ¸
===============================================

ì—…ê·¸ë ˆì´ë“œëœ Context Engineering ì‹œìŠ¤í…œì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³ 
ì—…ê³„ í‘œì¤€ ì§€í‘œì˜ ì •í™•ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any
from src.context_engineering_integration_enhanced import (
    EnhancedHVDCContextWindow, EnhancedHVDCContextScoring, 
    EnhancedHVDCContextProtocol, EnhancedHVDCContextEngineeringIntegration
)

class EnhancedContextEngineeringTester:
    """ì—…ê³„ í‘œì¤€ Context Engineering í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.scoring = EnhancedHVDCContextScoring()
        self.protocol = EnhancedHVDCContextProtocol()
        self.test_results = []
        
    def test_industry_standard_metrics(self):
        """ì—…ê³„ í‘œì¤€ ì§€í‘œ í…ŒìŠ¤íŠ¸"""
        print("ðŸ” ì—…ê³„ í‘œì¤€ ì§€í‘œ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # 1. Context Precision í…ŒìŠ¤íŠ¸
        context = EnhancedHVDCContextWindow()
        context.examples = [
            {"HVDC": "project", "HITACHI": "equipment", "warehouse": "location"},
            {"SIEMENS": "vendor", "logistics": "operation", "FANR": "compliance"},
            {"unrelated": "data", "random": "information"}  # ê´€ë ¨ ì—†ëŠ” ì˜ˆì‹œ
        ]
        
        precision = self.scoring.calculate_context_precision(context)
        print(f"âœ… Context Precision: {precision:.3f} (ì˜ˆìƒ: 0.667)")
        
        # 2. Context Recall í…ŒìŠ¤íŠ¸
        recall = self.scoring.calculate_context_recall(context)
        print(f"âœ… Context Recall: {recall:.3f}")
        
        # 3. Groundedness í…ŒìŠ¤íŠ¸
        context.prompt = "HVDC warehouse HITACHI equipment logistics"
        groundedness = self.scoring.calculate_groundedness(context)
        print(f"âœ… Groundedness: {groundedness:.3f}")
        
        # 4. ë©”ëª¨ë¦¬ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        context.memory = {
            "recent_command": {
                "command": "enhance_dashboard",
                "status": "SUCCESS",
                "timestamp": datetime.now().isoformat()
            },
            "old_command": {
                "command": "get_kpi",
                "status": "SUCCESS", 
                "timestamp": (datetime.now() - timedelta(days=2)).isoformat()
            }
        }
        
        memory_quality = self.scoring.calculate_memory_quality(context)
        print(f"âœ… Memory Freshness: {memory_quality['freshness']:.3f}")
        print(f"âœ… Memory Relevance: {memory_quality['relevance']:.3f}")
        print(f"âœ… Memory Coherence: {memory_quality['coherence']:.3f}")
        
        return {
            "precision": precision,
            "recall": recall,
            "groundedness": groundedness,
            "memory_quality": memory_quality
        }
    
    def test_enhanced_scoring_system(self):
        """í–¥ìƒëœ ì ìˆ˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("\nðŸŽ¯ í–¥ìƒëœ ì ìˆ˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ì™„ì „í•œ Context í…ŒìŠ¤íŠ¸
        complete_context = EnhancedHVDCContextWindow()
        complete_context.prompt = "HVDC warehouse optimization"
        complete_context.examples = [
            {"HVDC": "project", "optimization": "success", "performance": 0.95}
        ]
        complete_context.tools = ["optimizer", "analyzer"]
        complete_context.state = {"mode": "LATTICE"}
        complete_context.logistics_context = {"project": "HVDC"}
        complete_context.fanr_compliance = {"status": "valid"}
        complete_context.kpi_metrics = {"efficiency": 0.92}
        complete_context.field_resonance = 0.9
        complete_context.attractor_strength = 0.85
        
        complete_score = self.scoring.score_context_quality_enhanced(complete_context)
        print(f"âœ… ì™„ì „í•œ Context ì ìˆ˜: {complete_score:.3f}")
        
        # ë¶€ë¶„ì  Context í…ŒìŠ¤íŠ¸
        partial_context = EnhancedHVDCContextWindow()
        partial_context.prompt = "basic query"
        partial_context.examples = [{"basic": "data"}]
        
        partial_score = self.scoring.score_context_quality_enhanced(partial_context)
        print(f"âœ… ë¶€ë¶„ì  Context ì ìˆ˜: {partial_score:.3f}")
        
        # ë¹ˆ Context í…ŒìŠ¤íŠ¸
        empty_context = EnhancedHVDCContextWindow()
        empty_score = self.scoring.score_context_quality_enhanced(empty_context)
        print(f"âœ… ë¹ˆ Context ì ìˆ˜: {empty_score:.3f}")
        
        return {
            "complete_score": complete_score,
            "partial_score": partial_score,
            "empty_score": empty_score
        }
    
    def test_response_quality_enhanced(self):
        """í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        print("\nðŸ“Š í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ìš°ìˆ˜í•œ ì‘ë‹µ
        excellent_response = {
            "status": "SUCCESS",
            "confidence": 0.95,
            "mode": "PRIME",
            "recommended_commands": ["get_kpi", "enhance_dashboard"],
            "timestamp": datetime.now().isoformat(),
            "context_engineering": {"score": 0.9}
        }
        
        excellent_score = self.scoring.score_response_quality_enhanced(excellent_response)
        print(f"âœ… ìš°ìˆ˜í•œ ì‘ë‹µ ì ìˆ˜: {excellent_score:.3f}")
        
        # ë³´í†µ ì‘ë‹µ
        average_response = {
            "status": "SUCCESS",
            "confidence": 0.8,
            "mode": "PRIME"
        }
        
        average_score = self.scoring.score_response_quality_enhanced(average_response)
        print(f"âœ… ë³´í†µ ì‘ë‹µ ì ìˆ˜: {average_score:.3f}")
        
        # ì˜¤ë¥˜ ì‘ë‹µ
        error_response = {
            "status": "ERROR",
            "error_message": "Invalid command",
            "confidence": 0.0
        }
        
        error_score = self.scoring.score_response_quality_enhanced(error_response)
        print(f"âœ… ì˜¤ë¥˜ ì‘ë‹µ ì ìˆ˜: {error_score:.3f}")
        
        return {
            "excellent_score": excellent_score,
            "average_score": average_score,
            "error_score": error_score
        }
    
    def test_dynamic_thresholds(self):
        """ë™ì  ìž„ê³„ê°’ ì¡°ì • í…ŒìŠ¤íŠ¸"""
        print("\nâš™ï¸ ë™ì  ìž„ê³„ê°’ ì¡°ì • í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ì´ˆê¸° ìž„ê³„ê°’
        initial_threshold = self.scoring.dynamic_thresholds["confidence"]
        print(f"âœ… ì´ˆê¸° confidence ìž„ê³„ê°’: {initial_threshold:.3f}")
        
        # ê°€ìƒì˜ ì„±ëŠ¥ ë°ì´í„°ë¡œ ìž„ê³„ê°’ ì—…ë°ì´íŠ¸
        recent_scores = [0.85, 0.88, 0.92, 0.89, 0.91, 0.87, 0.93, 0.90, 0.86, 0.94] * 3  # 30ê°œ ë°ì´í„°
        
        self.scoring.update_dynamic_thresholds(recent_scores)
        updated_threshold = self.scoring.dynamic_thresholds["confidence"]
        print(f"âœ… ì—…ë°ì´íŠ¸ëœ confidence ìž„ê³„ê°’: {updated_threshold:.3f}")
        
        return {
            "initial_threshold": initial_threshold,
            "updated_threshold": updated_threshold,
            "threshold_change": updated_threshold - initial_threshold
        }
    
    def test_memory_penalty_system(self):
        """ë©”ëª¨ë¦¬ íŒ¨ë„í‹° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("\nðŸ§  ë©”ëª¨ë¦¬ íŒ¨ë„í‹° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ë©”ëª¨ë¦¬ê°€ ìžˆëŠ” Context
        context_with_memory = EnhancedHVDCContextWindow()
        context_with_memory.prompt = "test"
        context_with_memory.memory = {
            "command_1": {"status": "SUCCESS", "timestamp": datetime.now().isoformat()}
        }
        
        score_with_memory = self.scoring.score_context_quality_enhanced(context_with_memory)
        print(f"âœ… ë©”ëª¨ë¦¬ ìžˆëŠ” Context ì ìˆ˜: {score_with_memory:.3f}")
        
        # ë©”ëª¨ë¦¬ê°€ ì—†ëŠ” Context
        context_without_memory = EnhancedHVDCContextWindow()
        context_without_memory.prompt = "test"
        # memoryëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¹ˆ ë”•ì…”ë„ˆë¦¬
        
        score_without_memory = self.scoring.score_context_quality_enhanced(context_without_memory)
        print(f"âœ… ë©”ëª¨ë¦¬ ì—†ëŠ” Context ì ìˆ˜: {score_without_memory:.3f}")
        
        penalty_effect = score_with_memory - score_without_memory
        print(f"âœ… ë©”ëª¨ë¦¬ íŒ¨ë„í‹° íš¨ê³¼: {penalty_effect:.3f}")
        
        return {
            "score_with_memory": score_with_memory,
            "score_without_memory": score_without_memory,
            "penalty_effect": penalty_effect
        }
    
    async def test_enhanced_integration(self):
        """í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("\nðŸ”— í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ê°€ìƒì˜ LogiMaster ì‹œìŠ¤í…œ
        class MockLogiMasterSystem:
            async def initialize(self):
                pass
            
            async def execute_command(self, command: str, parameters: Dict[str, Any] = None):
                return {
                    "status": "SUCCESS",
                    "confidence": 0.92,
                    "mode": "PRIME",
                    "command": command,
                    "parameters": parameters,
                    "recommended_commands": ["get_kpi", "enhance_dashboard"],
                    "timestamp": datetime.now().isoformat()
                }
        
        # í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        mock_logi_master = MockLogiMasterSystem()
        enhanced_integration = EnhancedHVDCContextEngineeringIntegration(mock_logi_master)
        
        # ëª…ë ¹ì–´ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        result = await enhanced_integration.execute_command_with_context(
            "enhance_dashboard",
            {"dashboard_id": "main", "enhancement_type": "weather_integration"}
        )
        
        print(f"âœ… ëª…ë ¹ì–´ ì‹¤í–‰ ìƒíƒœ: {result['status']}")
        print(f"âœ… Context ì ìˆ˜: {result['enhanced_context_engineering']['context_score']:.3f}")
        print(f"âœ… ì‘ë‹µ ì ìˆ˜: {result['enhanced_context_engineering']['response_score']:.3f}")
        print(f"âœ… Context Precision: {result['enhanced_context_engineering']['context_precision']:.3f}")
        print(f"âœ… Context Recall: {result['enhanced_context_engineering']['context_recall']:.3f}")
        print(f"âœ… Groundedness: {result['enhanced_context_engineering']['groundedness']:.3f}")
        
        # ë¶„ì„ ë°ì´í„° í…ŒìŠ¤íŠ¸
        analytics = await enhanced_integration.get_enhanced_context_analytics()
        print(f"âœ… ì´ Context ìˆ˜: {analytics['total_contexts']}")
        print(f"âœ… í‰ê·  Context ì ìˆ˜: {analytics['average_context_score']:.3f}")
        print(f"âœ… í‰ê·  ì‘ë‹µ ì ìˆ˜: {analytics['average_response_score']:.3f}")
        
        return {
            "execution_result": result,
            "analytics": analytics
        }
    
    def test_quality_distribution(self):
        """í’ˆì§ˆ ë¶„í¬ í…ŒìŠ¤íŠ¸"""
        print("\nðŸ“ˆ í’ˆì§ˆ ë¶„í¬ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ë‹¤ì–‘í•œ í’ˆì§ˆì˜ Context ìƒì„±
        contexts = []
        
        # ìš°ìˆ˜í•œ Contextë“¤
        for i in range(5):
            ctx = EnhancedHVDCContextWindow()
            ctx.prompt = f"excellent context {i}"
            ctx.examples = [{"HVDC": "project", "quality": "excellent"}]
            ctx.tools = ["tool1", "tool2"]
            ctx.logistics_context = {"project": "HVDC"}
            ctx.field_resonance = 0.9
            ctx.attractor_strength = 0.85
            contexts.append(ctx)
        
        # ë³´í†µ Contextë“¤
        for i in range(8):
            ctx = EnhancedHVDCContextWindow()
            ctx.prompt = f"good context {i}"
            ctx.examples = [{"data": "good"}]
            ctx.tools = ["tool1"]
            contexts.append(ctx)
        
        # ë‚®ì€ í’ˆì§ˆ Contextë“¤
        for i in range(3):
            ctx = EnhancedHVDCContextWindow()
            ctx.prompt = f"poor context {i}"
            # ìµœì†Œí•œì˜ ì •ë³´ë§Œ
            contexts.append(ctx)
        
        # ì ìˆ˜ ê³„ì‚° ë° ë¶„í¬ ë¶„ì„
        scores = [self.scoring.score_context_quality_enhanced(ctx) for ctx in contexts]
        
        excellent_count = sum(1 for score in scores if score >= 0.9)
        good_count = sum(1 for score in scores if 0.7 <= score < 0.9)
        fair_count = sum(1 for score in scores if 0.5 <= score < 0.7)
        poor_count = sum(1 for score in scores if score < 0.5)
        
        print(f"âœ… ìš°ìˆ˜ (â‰¥0.9): {excellent_count}ê°œ")
        print(f"âœ… ì–‘í˜¸ (0.7-0.9): {good_count}ê°œ")
        print(f"âœ… ë³´í†µ (0.5-0.7): {fair_count}ê°œ")
        print(f"âœ… ë¯¸í¡ (<0.5): {poor_count}ê°œ")
        print(f"âœ… í‰ê·  ì ìˆ˜: {np.mean(scores):.3f}")
        print(f"âœ… í‘œì¤€íŽ¸ì°¨: {np.std(scores):.3f}")
        
        return {
            "scores": scores,
            "distribution": {
                "excellent": excellent_count,
                "good": good_count,
                "fair": fair_count,
                "poor": poor_count
            },
            "statistics": {
                "mean": np.mean(scores),
                "std": np.std(scores),
                "min": np.min(scores),
                "max": np.max(scores)
            }
        }
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ðŸš€ ì—…ê³„ í‘œì¤€ Context Engineering ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œìž‘")
        print("=" * 60)
        
        # 1. ì—…ê³„ í‘œì¤€ ì§€í‘œ í…ŒìŠ¤íŠ¸
        metrics_result = self.test_industry_standard_metrics()
        self.test_results.append(("ì—…ê³„ í‘œì¤€ ì§€í‘œ", metrics_result))
        
        # 2. í–¥ìƒëœ ì ìˆ˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        scoring_result = self.test_enhanced_scoring_system()
        self.test_results.append(("í–¥ìƒëœ ì ìˆ˜ ì‹œìŠ¤í…œ", scoring_result))
        
        # 3. ì‘ë‹µ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸
        response_result = self.test_response_quality_enhanced()
        self.test_results.append(("ì‘ë‹µ í’ˆì§ˆ í‰ê°€", response_result))
        
        # 4. ë™ì  ìž„ê³„ê°’ í…ŒìŠ¤íŠ¸
        threshold_result = self.test_dynamic_thresholds()
        self.test_results.append(("ë™ì  ìž„ê³„ê°’", threshold_result))
        
        # 5. ë©”ëª¨ë¦¬ íŒ¨ë„í‹° í…ŒìŠ¤íŠ¸
        memory_result = self.test_memory_penalty_system()
        self.test_results.append(("ë©”ëª¨ë¦¬ íŒ¨ë„í‹°", memory_result))
        
        # 6. í’ˆì§ˆ ë¶„í¬ í…ŒìŠ¤íŠ¸
        distribution_result = self.test_quality_distribution()
        self.test_results.append(("í’ˆì§ˆ ë¶„í¬", distribution_result))
        
        return self.test_results
    
    async def run_async_tests(self):
        """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\nðŸ”„ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("=" * 60)
        
        # í–¥ìƒëœ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        integration_result = await self.test_enhanced_integration()
        self.test_results.append(("í–¥ìƒëœ í†µí•©", integration_result))
        
        return integration_result
    
    def generate_test_report(self):
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nðŸ“‹ ì—…ê³„ í‘œì¤€ Context Engineering í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = 0
        
        for test_name, result in self.test_results:
            print(f"\nðŸ” {test_name} í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            
            if isinstance(result, dict):
                for key, value in result.items():
                    if isinstance(value, float):
                        print(f"  âœ… {key}: {value:.3f}")
                    elif isinstance(value, dict):
                        print(f"  âœ… {key}: {json.dumps(value, indent=4, ensure_ascii=False)}")
                    else:
                        print(f"  âœ… {key}: {value}")
                
                # í…ŒìŠ¤íŠ¸ í†µê³¼ ì—¬ë¶€ íŒë‹¨
                if "score" in str(result) or "precision" in str(result):
                    passed_tests += 1
            else:
                print(f"  âœ… ê²°ê³¼: {result}")
                passed_tests += 1
        
        print(f"\nðŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        print(f"  ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"  í†µê³¼: {passed_tests}ê°œ")
        print(f"  í†µê³¼ìœ¨: {(passed_tests/total_tests)*100:.1f}%")
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "pass_rate": (passed_tests/total_tests)*100,
            "results": self.test_results
        }

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    tester = EnhancedContextEngineeringTester()
    
    # ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    sync_results = tester.run_comprehensive_test()
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async_results = await tester.run_async_tests()
    
    # í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
    report = tester.generate_test_report()
    
    # ê²°ê³¼ ì €ìž¥
    with open("enhanced_context_engineering_test_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nðŸ’¾ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ê°€ 'enhanced_context_engineering_test_report.json'ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return report

if __name__ == "__main__":
    asyncio.run(main()) 
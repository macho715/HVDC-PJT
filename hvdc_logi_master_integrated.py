# hvdc_logi_master_integrated.py - HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹œìŠ¤í…œ
"""
HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í†µí•© ì‹œìŠ¤í…œ v3.4-mini
- ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì—”ì§„ í†µí•©
- ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡° ì™„ì „ ì§€ì›
- MACHO-GPT ë¬¼ë¥˜ ë„ë©”ì¸ ì „ì²´ ì»¤ë²„ë¦¬ì§€
- Samsung C&T Â· ADNOC Â· DSV íŒŒíŠ¸ë„ˆì‹­ ëŒ€ì‘
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
import json
import logging
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
import re
import sqlite3
import warnings
warnings.filterwarnings('ignore')

# RDF ì˜¨í†¨ë¡œì§€ ì§€ì›
try:
    from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
    RDF_AVAILABLE = True
    # ì˜¨í†¨ë¡œì§€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
    HVDC = Namespace("http://samsung.com/project-logistics#")
    EX = Namespace("http://example.org/hvdc#")
    MACHO = Namespace("http://macho-gpt.com/logistics#")
except ImportError:
    RDF_AVAILABLE = False
    print("âš ï¸ RDFLib ë¯¸ì„¤ì¹˜ - RDF ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Event-Based Outbound Logic ì§€ì›
try:
    from scripts.event_based_outbound import EventBasedOutboundResolver
    EVENT_OUTBOUND_AVAILABLE = True
    logger.info("âœ… Event-Based Outbound Logic ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    EVENT_OUTBOUND_AVAILABLE = False
    logger.warning("âš ï¸ Event-Based Outbound Logic ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ë¡œì§ ì‚¬ìš©")

# ===== 1. í•µì‹¬ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜ =====

@dataclass
class HVDCItem:
    """HVDC í”„ë¡œì íŠ¸ ì•„ì´í…œ í´ë˜ìŠ¤"""
    hvdc_code: str
    vendor: str
    category: str
    description: str
    weight: float
    dimensions: Dict[str, float]
    location: str
    status: str
    risk_level: str = "NORMAL"
    sqm: float = 0.0
    flow_code: int = 0
    
    def to_rdf(self, graph) -> 'URIRef':
        """RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜"""
        if not RDF_AVAILABLE:
            return None
        item_uri = EX[f"item_{self.hvdc_code}"]
        
        # ê¸°ë³¸ í´ë˜ìŠ¤ ì„ ì–¸
        graph.add((item_uri, RDF.type, HVDC.Item))
        
        # ì†ì„± ì¶”ê°€
        graph.add((item_uri, HVDC.hvdcCode, Literal(self.hvdc_code)))
        graph.add((item_uri, HVDC.vendor, Literal(self.vendor)))
        graph.add((item_uri, HVDC.category, Literal(self.category)))
        graph.add((item_uri, HVDC.description, Literal(self.description)))
        graph.add((item_uri, HVDC.weight, Literal(self.weight, datatype=XSD.decimal)))
        graph.add((item_uri, HVDC.currentLocation, Literal(self.location)))
        graph.add((item_uri, HVDC.status, Literal(self.status)))
        graph.add((item_uri, HVDC.riskLevel, Literal(self.risk_level)))
        graph.add((item_uri, HVDC.sqm, Literal(self.sqm, datatype=XSD.decimal)))
        graph.add((item_uri, HVDC.flowCode, Literal(self.flow_code, datatype=XSD.integer)))
        
        # ì¤‘ëŸ‰ ê¸°ë°˜ ìë™ ë¶„ë¥˜
        if self.weight > 25000:
            graph.add((item_uri, HVDC.isHeavyItem, Literal(True, datatype=XSD.boolean)))
            
        return item_uri

@dataclass
class Warehouse:
    """ì°½ê³  ì •ë³´ í´ë˜ìŠ¤"""
    name: str
    warehouse_type: str  # Indoor, Outdoor, Site, Dangerous
    capacity_sqm: float
    current_utilization: float
    handling_fee: float
    
    def to_rdf(self, graph) -> 'URIRef':
        if not RDF_AVAILABLE:
            return None
        warehouse_uri = EX[f"warehouse_{self.name.replace(' ', '_')}"]
        
        # ì°½ê³  íƒ€ì…ì— ë”°ë¥¸ í´ë˜ìŠ¤ ë¶„ë¥˜
        if self.warehouse_type == "Indoor":
            graph.add((warehouse_uri, RDF.type, HVDC.IndoorWarehouse))
        elif self.warehouse_type == "Outdoor":
            graph.add((warehouse_uri, RDF.type, HVDC.OutdoorWarehouse))
        elif self.warehouse_type == "Site":
            graph.add((warehouse_uri, RDF.type, HVDC.Site))
        elif self.warehouse_type == "Dangerous":
            graph.add((warehouse_uri, RDF.type, HVDC.DangerousCargoWarehouse))
            
        graph.add((warehouse_uri, HVDC.name, Literal(self.name)))
        graph.add((warehouse_uri, HVDC.capacitySQM, Literal(self.capacity_sqm, datatype=XSD.decimal)))
        graph.add((warehouse_uri, HVDC.currentUtilization, Literal(self.current_utilization, datatype=XSD.decimal)))
        graph.add((warehouse_uri, HVDC.handlingFee, Literal(self.handling_fee, datatype=XSD.decimal)))
        
        return warehouse_uri

@dataclass
class MonthlySiteReport:
    """ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡° ì „ìš© ë¦¬í¬íŠ¸ í´ë˜ìŠ¤"""
    report_month: str
    warehouse_data: Dict[str, Dict[str, int]]  # warehouse_name -> {inbound, outbound}
    site_data: Dict[str, Dict[str, int]]       # site_name -> {inbound, inventory}
    total_transactions: int
    confidence_score: float

class ContainmentMode(Enum):
    """MACHO-GPT ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ"""
    PRIME = "PRIME"
    ORACLE = "ORACLE"
    ZERO = "ZERO"
    LATTICE = "LATTICE"
    RHYTHM = "RHYTHM"
    COST_GUARD = "COST_GUARD"

# ===== 2. ë§¤í•‘ ë° ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ì =====

class MappingManager:
    """í†µí•© ë§¤í•‘ ê´€ë¦¬ì v3.4"""
    
    def __init__(self, mapping_file: str = "mapping_rules_v2.8.json"):
        self.mapping_file = mapping_file
        self.mapping_rules = self._load_mapping_rules()
        self.warehouse_classification = self.mapping_rules.get("warehouse_classification", {})
        self.logistics_flow_definition = self.mapping_rules.get("logistics_flow_definition", {})
        
        # ì°½ê³  ìœ„ì¹˜ ë§¤í•‘
        self.location_columns = {
            'sites': ['AGI', 'DAS', 'MIR', 'SHU'],
            'warehouses': ['DSV Indoor', 'DSV Outdoor', 'DSV Al Markaz', 'MOSB', 'DSV MZP']
        }
        
    def _load_mapping_rules(self) -> Dict:
        """ë§¤í•‘ ê·œì¹™ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.mapping_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"ë§¤í•‘ ê·œì¹™ íŒŒì¼ {self.mapping_file} ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self._get_default_mapping_rules()
    
    def _get_default_mapping_rules(self) -> Dict:
        """ê¸°ë³¸ ë§¤í•‘ ê·œì¹™"""
        return {
            "warehouse_classification": {
                "Indoor": ["DSV Indoor", "Hauler Indoor"],
                "Outdoor": ["DSV Outdoor"], 
                "Site": ["AGI", "DAS", "MIR", "SHU"],
                "OffshoreBase": ["MOSB"],
                "Others": ["DSV Al Markaz", "DSV MZP", "AAA Storage"]
            },
            "vendor_mappings": {
                "HE": "Hitachi",
                "SIM": "Siemens",
                "SCNT": "Samsung C&T"
            },
            "field_map": {
                "Case No.": "hasCaseNo",
                "Vendor": "hasVendor",
                "Category": "hasCategory",
                "Weight": "hasWeight",
                "CBM": "hasCBM",
                "SQM": "hasSQM",
                "FLOW_CODE": "hasFlowCode",
                "Status_Location_Date": "hasStatusLocationDate"
            }
        }
    
    def classify_storage_type(self, location: str) -> str:
        """Location â†’ Storage Type ë¶„ë¥˜"""
        if not location or pd.isna(location):
            return "Unknown"
        
        loc = str(location).strip()
        
        # ì •í™•í•œ ë§¤ì¹­ í™•ì¸
        for storage_type, locations in self.warehouse_classification.items():
            if loc in locations:
                return storage_type
        
        # ë¶€ë¶„ ë§¤ì¹­ í™•ì¸
        loc_lower = loc.lower()
        for storage_type, locations in self.warehouse_classification.items():
            for pattern in locations:
                if pattern.lower() in loc_lower:
                    return storage_type
        
        return "Unknown"

class HVDCOntologyEngine:
    """HVDC ì˜¨í†¨ë¡œì§€ ì—”ì§„ - ì™„ì „ í†µí•©"""
    
    def __init__(self, db_path: str = "hvdc_ontology.db"):
        self.graph = Graph() if RDF_AVAILABLE else None
        self.db_path = db_path
        self.init_database()
        if RDF_AVAILABLE:
            self.setup_ontology_schema()
        
    def init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()
        
        # ì•„ì´í…œ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS items (
                hvdc_code TEXT PRIMARY KEY,
                vendor TEXT,
                category TEXT,
                weight REAL,
                location TEXT,
                status TEXT,
                risk_level TEXT,
                sqm REAL,
                flow_code INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì°½ê³  í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS warehouses (
                name TEXT PRIMARY KEY,
                warehouse_type TEXT,
                capacity_sqm REAL,
                current_utilization REAL,
                handling_fee REAL,
                monthly_inbound INTEGER DEFAULT 0,
                monthly_outbound INTEGER DEFAULT 0
            )
        ''')
        
        # ì›”ë³„ ì§‘ê³„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS monthly_aggregates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                report_month TEXT,
                location_name TEXT,
                location_type TEXT,
                inbound_count INTEGER,
                outbound_count INTEGER,
                inventory_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.conn.commit()
    
    def setup_ontology_schema(self):
        """ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì„¤ì •"""
        if not RDF_AVAILABLE:
            return
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
        self.graph.bind("hvdc", HVDC)
        self.graph.bind("ex", EX)
        self.graph.bind("macho", MACHO)
        
        # í´ë˜ìŠ¤ ì •ì˜
        classes = [
            (HVDC.Item, "HVDC ì•„ì´í…œ"),
            (HVDC.Warehouse, "ì°½ê³ "),
            (HVDC.IndoorWarehouse, "ì‹¤ë‚´ ì°½ê³ "),
            (HVDC.OutdoorWarehouse, "ì‹¤ì™¸ ì°½ê³ "),
            (HVDC.Site, "í˜„ì¥"),
            (HVDC.DangerousCargoWarehouse, "ìœ„í—˜ë¬¼ ì°½ê³ "),
            (HVDC.TransportEvent, "ìš´ì†¡ ì´ë²¤íŠ¸"),
            (HVDC.MonthlySiteReport, "ì›”ë³„ í˜„ì¥ ë¦¬í¬íŠ¸"),
            (MACHO.LogiMaster, "ë¬¼ë¥˜ ë§ˆìŠ¤í„°"),
            (MACHO.ContainerStow, "ì»¨í…Œì´ë„ˆ ì ì¬"),
            (MACHO.WeatherTie, "ë‚ ì”¨ ì—°ë™")
        ]
        
        for class_uri, label in classes:
            self.graph.add((class_uri, RDF.type, OWL.Class))
            self.graph.add((class_uri, RDFS.label, Literal(label, lang="ko")))
        
        # ì†ì„± ì •ì˜
        properties = [
            (HVDC.hvdcCode, "HVDC ì½”ë“œ"),
            (HVDC.vendor, "ë²¤ë”"),
            (HVDC.category, "ì¹´í…Œê³ ë¦¬"),
            (HVDC.weight, "ì¤‘ëŸ‰"),
            (HVDC.sqm, "ë©´ì "),
            (HVDC.flowCode, "í”Œë¡œìš° ì½”ë“œ"),
            (HVDC.currentLocation, "í˜„ì¬ ìœ„ì¹˜"),
            (HVDC.statusLocationDate, "ìƒíƒœ ìœ„ì¹˜ ë‚ ì§œ"),
            (MACHO.containmentMode, "ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ"),
            (MACHO.confidenceScore, "ì‹ ë¢°ë„ ì ìˆ˜")
        ]
        
        for prop_uri, label in properties:
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(label, lang="ko")))
    
    def add_item(self, item: HVDCItem) -> bool:
        """ì•„ì´í…œ ì¶”ê°€"""
        try:
            # RDF ê·¸ë˜í”„ì— ì¶”ê°€
            if RDF_AVAILABLE:
                item_uri = item.to_rdf(self.graph)
            
            # SQLiteì— ì €ì¥
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO items 
                (hvdc_code, vendor, category, weight, location, status, risk_level, sqm, flow_code)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (item.hvdc_code, item.vendor, item.category, item.weight, 
                  item.location, item.status, item.risk_level, item.sqm, item.flow_code))
            
            self.conn.commit()
            logger.info(f"ì•„ì´í…œ {item.hvdc_code} ì¶”ê°€ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì•„ì´í…œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

# ===== 3. ë©”ì¸ HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° í´ë˜ìŠ¤ =====

class HVDCLogiMaster:
    """HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° ì‹œìŠ¤í…œ - ì™„ì „ í†µí•©"""
    
    def __init__(self, 
                 mode: ContainmentMode = ContainmentMode.PRIME,
                 enable_ontology: bool = True,
                 mapping_file: str = "mapping_rules_v2.8.json"):
        
        self.mode = mode
        self.confidence_threshold = 0.90
        self.success_rate_target = 0.95
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.mapping_manager = MappingManager(mapping_file)
        self.ontology_engine = HVDCOntologyEngine() if enable_ontology else None
        
        # ìœ„ì¹˜ ì •ë³´
        self.location_columns = {
            'sites': ['AGI', 'DAS', 'MIR', 'SHU'],
            'warehouses': ['DSV Indoor', 'DSV Outdoor', 'DSV Al Markaz', 'MOSB', 'DSV MZP']
        }
        
        # KPI ì„ê³„ê°’
        self.kpi_thresholds = {
            'delta_rate_threshold': 10,  # % change
            'eta_delay_threshold': 24,   # hours
            'pressure_threshold': 4,     # t/mÂ²
            'utilization_threshold': 85, # %
            'cert_expiry_days': 30       # days
        }
        
        logger.info(f"HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ - Mode: {mode.value}")
    
    def process_macho_data(self, 
                          source_file: str,
                          output_file: str = None) -> Dict[str, Any]:
        """MACHO í†µí•© ë°ì´í„° ì²˜ë¦¬"""
        try:
            print(f'ğŸ“‚ MACHO ë°ì´í„° ë¡œë“œ: {source_file}')
            df = pd.read_excel(source_file, sheet_name=0)
            print(f'âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼')
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = self._preprocess_data(df)
            
            # ì˜¨í†¨ë¡œì§€ ë§¤í•‘
            if self.ontology_engine:
                df = self._apply_ontology_mapping(df)
            
            # ì›”ë³„ ì§‘ê³„ ìƒì„±
            monthly_report = self._create_monthly_site_report(df)
            
            # Excel ë¦¬í¬íŠ¸ ìƒì„±
            if output_file:
                self._create_excel_report(df, monthly_report, output_file)
            
            return {
                'status': 'SUCCESS',
                'confidence': self._calculate_confidence(df),
                'mode': self.mode.value,
                'processed_records': len(df),
                'monthly_report': monthly_report,
                'next_cmds': self._get_next_commands(df)
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'status': 'FAIL',
                'error': str(e),
                'mode': self.mode.value
            }
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print('ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...')
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['Case No.', 'FLOW_CODE', 'Status_Current', 'Status_Location']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f'âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}')
        
        # ìœ„ì¹˜ ì •ë³´ ì²˜ë¦¬
        for site in self.location_columns['sites']:
            if site in df.columns:
                df[site] = pd.to_datetime(df[site], errors='coerce')
        
        for warehouse in self.location_columns['warehouses']:
            if warehouse in df.columns:
                df[warehouse] = pd.to_datetime(df[warehouse], errors='coerce')
        
        # Event-Based Outbound Logic: Final_Location ì¬êµ¬ì„± (ì›”ë³„ ì§‘ê³„ ì „ í˜¸ì¶œ)
        if EVENT_OUTBOUND_AVAILABLE:
            try:
                print('ğŸ¯ Event-Based Final_Location ì¬êµ¬ì„± ì¤‘...')
                resolver = EventBasedOutboundResolver(config_path='config/wh_priority.yaml')
                df = resolver.resolve_final_location(df)
                print(f'âœ… Final_Location ì¬êµ¬ì„± ì™„ë£Œ - {df["Final_Location"].value_counts().to_dict()}')
            except Exception as e:
                logger.warning(f"Final_Location ì¬êµ¬ì„± ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë¡œì§ ì‚¬ìš©")
                # ê¸°ë³¸ Final_Location ë¡œì§ (Status_Location ì‚¬ìš©)
                df['Final_Location'] = df['Status_Location'].fillna('Unknown')
        else:
            # ê¸°ë³¸ Final_Location ë¡œì§
            df['Final_Location'] = df['Status_Location'].fillna('Unknown')
        
        # Flow Code ê²€ì¦
        if 'FLOW_CODE' in df.columns:
            df['FLOW_CODE'] = df['FLOW_CODE'].apply(self._validate_flow_code)
        
        # SQM ê³„ì‚°
        if 'CBM' in df.columns and 'SQM' not in df.columns:
            df['SQM'] = df['CBM'] / 0.5  # CBM to SQM conversion
        
        print(f'âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê±´')
        return df
    
    def _apply_ontology_mapping(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš©"""
        if not self.ontology_engine:
            return df
        
        print('ğŸ”— ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš© ì¤‘...')
        
        # ê° í–‰ì„ HVDCItemìœ¼ë¡œ ë³€í™˜ í›„ ì˜¨í†¨ë¡œì§€ì— ì¶”ê°€
        for idx, row in df.iterrows():
            try:
                item = HVDCItem(
                    hvdc_code=str(row.get('Case No.', f'ITEM_{idx:05d}')),
                    vendor=str(row.get('Vendor', 'Unknown')),
                    category=str(row.get('Category', 'General')),
                    description=str(row.get('Description', '')),
                    weight=float(row.get('G.W(kgs)', 0)),
                    dimensions={'length': 0, 'width': 0, 'height': 0},
                    location=str(row.get('Status_Location', 'Unknown')),
                    status=str(row.get('Status_Current', 'warehouse')),
                    sqm=float(row.get('SQM', 0)),
                    flow_code=int(row.get('FLOW_CODE', 0))
                )
                
                self.ontology_engine.add_item(item)
                
            except Exception as e:
                logger.warning(f"í–‰ {idx} ë§¤í•‘ ì‹¤íŒ¨: {e}")
                continue
        
        print('âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì™„ë£Œ')
        return df
    
    def _create_monthly_site_report(self, df: pd.DataFrame) -> MonthlySiteReport:
        """ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡° ë¦¬í¬íŠ¸ ìƒì„±"""
        print('ğŸ“Š ì›”ë³„ í˜„ì¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...')
        
        # í˜„ì¬ ì›” ê³„ì‚°
        current_month = datetime.now().strftime('%Y-%m')
        
        # ì°½ê³ ë³„ ì§‘ê³„
        warehouse_data = {}
        for warehouse in self.location_columns['warehouses']:
            if warehouse in df.columns:
                inbound_count = df[warehouse].notna().sum()
                # ì¶œê³ ëŠ” Status_Currentê°€ 'site'ì¸ ê²½ìš°ë¡œ ì¶”ì •
                outbound_count = df[(df['Status_Current'] == 'site') & 
                                   (df[warehouse].notna())].shape[0]
                warehouse_data[warehouse] = {
                    'inbound': inbound_count,
                    'outbound': outbound_count
                }
        
        # í˜„ì¥ë³„ ì§‘ê³„
        site_data = {}
        for site in self.location_columns['sites']:
            if site in df.columns:
                inbound_count = df[site].notna().sum()
                # ì¬ê³ ëŠ” í˜„ì¬ í•´ë‹¹ ì‚¬ì´íŠ¸ì— ìˆëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •
                inventory_count = df[df['Status_Location'] == site].shape[0]
                site_data[site] = {
                    'inbound': inbound_count,
                    'inventory': inventory_count
                }
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(df)
        
        return MonthlySiteReport(
            report_month=current_month,
            warehouse_data=warehouse_data,
            site_data=site_data,
            total_transactions=len(df),
            confidence_score=confidence
        )
    
    def _create_excel_report(self, 
                            df: pd.DataFrame, 
                            monthly_report: MonthlySiteReport,
                            output_file: str):
        """ì°½ê³ _í˜„ì¥_ì›”ë³„_ì‹œíŠ¸_êµ¬ì¡°.xlsx ë™ì¼í•œ Excel ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f'ğŸ“Š Excel ë¦¬í¬íŠ¸ ìƒì„±: {output_file}')
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        final_output = f'HVDC_LogiMaster_Report_{timestamp}.xlsx'
        
        with pd.ExcelWriter(final_output, engine='openpyxl') as writer:
            
            # Sheet 1: ì „ì²´ íŠ¸ëœì­ì…˜ ë°ì´í„°
            df.to_excel(writer, sheet_name='ì „ì²´_íŠ¸ëœì­ì…˜_FLOWCODE0-4', index=False)
            
            # Sheet 2: ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³ 
            warehouse_monthly = self._create_warehouse_monthly_sheet(monthly_report)
            warehouse_monthly.to_excel(writer, sheet_name='ì°½ê³ _ì›”ë³„_ì…ì¶œê³ ', index=False)
            
            # Sheet 3: í˜„ì¥ë³„ ì›”ë³„ ì…ê³ ì¬ê³ 
            site_monthly = self._create_site_monthly_sheet(monthly_report)
            site_monthly.to_excel(writer, sheet_name='í˜„ì¥_ì›”ë³„_ì…ê³ ì¬ê³ ', index=False)
            
            # Sheet 4: ìœ„ì¹˜ë³„ ë¶„í¬ í†µê³„
            location_stats = self._create_location_stats(df)
            location_stats.to_excel(writer, sheet_name='ìœ„ì¹˜ë³„_í™”ë¬¼ë¶„í¬_í†µê³„', index=False)
            
            # Sheet 5: ìš”ì•½ í†µê³„
            summary_stats = self._create_summary_stats(df, monthly_report)
            summary_stats.to_excel(writer, sheet_name='ìš”ì•½_í†µê³„', index=False)
        
        print(f'âœ… Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {final_output}')
        return final_output
    
    def _create_warehouse_monthly_sheet(self, report: MonthlySiteReport) -> pd.DataFrame:
        """ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  ì‹œíŠ¸ ìƒì„±"""
        # Multi-level header êµ¬ì¡° ìƒì„±
        warehouses = list(report.warehouse_data.keys())
        
        # í—¤ë” êµ¬ì„±
        columns = ['Location']
        for warehouse in warehouses:
            columns.extend([f'ì…ê³ _{warehouse}', f'ì¶œê³ _{warehouse}'])
        
        # ë°ì´í„° êµ¬ì„± (2023-02 ~ 2025-06 + Total)
        months = pd.date_range('2023-02', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        months.append('Total')
        
        data = []
        for month in months:
            row = [month]
            for warehouse in warehouses:
                if month == 'Total':
                    # ì´í•© ê³„ì‚°
                    inbound_total = report.warehouse_data[warehouse]['inbound']
                    outbound_total = report.warehouse_data[warehouse]['outbound']
                    row.extend([inbound_total, outbound_total])
                else:
                    # í˜„ì¬ëŠ” ìµœì‹  ì›”ë§Œ ë°ì´í„° ìˆìŒ
                    if month == report.report_month:
                        row.extend([
                            report.warehouse_data[warehouse]['inbound'],
                            report.warehouse_data[warehouse]['outbound']
                        ])
                    else:
                        row.extend([0, 0])
            data.append(row)
        
        return pd.DataFrame(data, columns=columns)
    
    def _create_site_monthly_sheet(self, report: MonthlySiteReport) -> pd.DataFrame:
        """í˜„ì¥ë³„ ì›”ë³„ ì…ê³ ì¬ê³  ì‹œíŠ¸ ìƒì„±"""
        sites = list(report.site_data.keys())
        
        # í—¤ë” êµ¬ì„±
        columns = ['Location']
        for site in sites:
            columns.extend([f'ì…ê³ _{site}', f'ì¬ê³ _{site}'])
        
        # ë°ì´í„° êµ¬ì„± (2024-01 ~ 2025-06 + í•©ê³„)
        months = pd.date_range('2024-01', '2025-06', freq='MS').strftime('%Y-%m').tolist()
        months.append('í•©ê³„')
        
        data = []
        for month in months:
            row = [month]
            for site in sites:
                if month == 'í•©ê³„':
                    # ì´í•© ê³„ì‚°
                    inbound_total = report.site_data[site]['inbound']
                    inventory_total = report.site_data[site]['inventory']
                    row.extend([inbound_total, inventory_total])
                else:
                    # í˜„ì¬ëŠ” ìµœì‹  ì›”ë§Œ ë°ì´í„° ìˆìŒ
                    if month == report.report_month:
                        row.extend([
                            report.site_data[site]['inbound'],
                            report.site_data[site]['inventory']
                        ])
                    else:
                        row.extend([0, 0])
            data.append(row)
        
        return pd.DataFrame(data, columns=columns)
    
    def _create_location_stats(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìœ„ì¹˜ë³„ ë¶„í¬ í†µê³„ ìƒì„±"""
        stats_data = []
        
        all_locations = self.location_columns['sites'] + self.location_columns['warehouses']
        
        for location in all_locations:
            if location in df.columns:
                count = df[location].notna().sum()
                percentage = count / len(df) * 100
                
                location_type = 'í˜„ì¥' if location in self.location_columns['sites'] else 'ì°½ê³ '
                
                stats_data.append({
                    'ìœ„ì¹˜': location,
                    'ê±´ìˆ˜': count,
                    'ë¹„ìœ¨(%)': round(percentage, 1),
                    'ìœ„ì¹˜_ìœ í˜•': location_type,
                    'íŠ¹ì§•': self._get_location_feature(location)
                })
        
        return pd.DataFrame(stats_data).sort_values('ê±´ìˆ˜', ascending=False)
    
    def _create_summary_stats(self, df: pd.DataFrame, report: MonthlySiteReport) -> pd.DataFrame:
        """ìš”ì•½ í†µê³„ ìƒì„±"""
        summary_data = [
            {'êµ¬ë¶„': 'ì´ í™”ë¬¼ ê±´ìˆ˜', 'ê°’': f'{len(df):,}ê±´'},
            {'êµ¬ë¶„': 'ì´ ì»¬ëŸ¼ ìˆ˜', 'ê°’': f'{len(df.columns)}ê°œ'},
            {'êµ¬ë¶„': 'í˜„ì¥ ìœ„ì¹˜ ìˆ˜', 'ê°’': f'{len(self.location_columns["sites"])}ê°œ'},
            {'êµ¬ë¶„': 'ì°½ê³  ìœ„ì¹˜ ìˆ˜', 'ê°’': f'{len(self.location_columns["warehouses"])}ê°œ'},
            {'êµ¬ë¶„': 'ë³´ê³ ì„œ ìƒì„±ì¼ì‹œ', 'ê°’': datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
            {'êµ¬ë¶„': 'ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ', 'ê°’': self.mode.value},
            {'êµ¬ë¶„': 'ì‹ ë¢°ë„ ì ìˆ˜', 'ê°’': f'{report.confidence_score:.2f}'},
            {'êµ¬ë¶„': 'Flow Code ë²”ìœ„', 'ê°’': f'{df["FLOW_CODE"].min()}-{df["FLOW_CODE"].max()}' if 'FLOW_CODE' in df.columns else 'N/A'}
        ]
        
        return pd.DataFrame(summary_data)
    
    def _get_location_feature(self, location: str) -> str:
        """ìœ„ì¹˜ë³„ íŠ¹ì§• ë°˜í™˜"""
        features = {
            'SHU': 'ìµœëŒ€ ì§‘ì¤‘ í˜„ì¥ (ìš©ëŸ‰ ê´€ë¦¬ í•„ìš”)',
            'DSV Outdoor': 'ì™¸ë¶€ ì°½ê³  (ë‚ ì”¨ ì˜í–¥ ê³ ë ¤)',
            'DSV Indoor': 'ë‚´ë¶€ ì°½ê³  (ì•ˆì „ ë³´ê´€)',
            'DSV Al Markaz': 'Al Markaz ì°½ê³  (ì¤‘ê°„ ê²½ìœ )',
            'MIR': 'ì£¼ìš” í˜„ì¥ (ì•ˆì •ì  ìš´ì˜)',
            'DAS': 'ì£¼ìš” í˜„ì¥ (íš¨ìœ¨ì  ìš´ì˜)',
            'MOSB': 'MOSB ì°½ê³  (ì „ë¬¸ ë³´ê´€)',
            'AGI': 'AGI í˜„ì¥ (íŠ¹ìˆ˜ ì¥ë¹„)',
            'DSV MZP': 'ì†Œê·œëª¨ ì°½ê³  (íŠ¹ìˆ˜ ìš©ë„)'
        }
        return features.get(location, 'ì¼ë°˜ ìš´ì˜')
    
    def _validate_flow_code(self, code: Any) -> int:
        """Flow Code ê²€ì¦"""
        try:
            flow_code = int(code)
            return flow_code if 0 <= flow_code <= 4 else 0
        except:
            return 0
    
    def _calculate_confidence(self, df: pd.DataFrame) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        total_fields = len(df.columns)
        valid_fields = sum(1 for col in df.columns if df[col].notna().sum() > 0)
        return (valid_fields / total_fields) * 100 if total_fields > 0 else 0.0
    
    def _get_next_commands(self, df: pd.DataFrame) -> List[str]:
        """ë‹¤ìŒ ì¶”ì²œ ëª…ë ¹ì–´ ìƒì„±"""
        commands = []
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
        if 'FLOW_CODE' in df.columns:
            commands.append('/flow_code_analysis [í”Œë¡œìš° ì½”ë“œ ë¶„ì„]')
        
        if 'SQM' in df.columns:
            commands.append('/warehouse_capacity_check [ì°½ê³  ìš©ëŸ‰ í™•ì¸]')
        
        commands.append('/generate_kpi_dashboard [KPI ëŒ€ì‹œë³´ë“œ ìƒì„±]')
        
        return commands

    def generate_kpi_dash(self) -> Dict[str, Any]:
        """KPI ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤/Generate KPI dashboard."""
        kpi_data = {
            'confidence_threshold': round(self.confidence_threshold, 2),
            'success_rate_target': round(self.success_rate_target, 2),
            'kpi_thresholds': self.kpi_thresholds,
        }
        return {
            'status': 'SUCCESS',
            'confidence': round(self.confidence_threshold, 2),
            'mode': self.mode.value,
            'triggers': [],
            'next_cmds': self._get_next_commands(pd.DataFrame()),
            'data': kpi_data,
        }
    
    def switch_mode(self, new_mode: ContainmentMode, reason: str = "") -> Dict[str, Any]:
        """ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ ì „í™˜"""
        old_mode = self.mode
        self.mode = new_mode
        
        logger.info(f"ì»¨í…Œì¸ë¨¼íŠ¸ ëª¨ë“œ ì „í™˜: {old_mode.value} â†’ {new_mode.value}")
        if reason:
            logger.info(f"ì „í™˜ ì‚¬ìœ : {reason}")
        
        return {
            'status': 'SUCCESS',
            'old_mode': old_mode.value,
            'new_mode': new_mode.value,
            'reason': reason,
            'timestamp': datetime.now().isoformat()
        }
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        validation_results = {
            'total_records': len(df),
            'missing_data': {},
            'data_types': {},
            'quality_score': 0.0,
            'recommendations': []
        }
        
        # ëˆ„ë½ ë°ì´í„° í™•ì¸
        for col in df.columns:
            missing_count = df[col].isna().sum()
            missing_percentage = (missing_count / len(df)) * 100
            validation_results['missing_data'][col] = {
                'count': missing_count,
                'percentage': round(missing_percentage, 2)
            }
            
            # ë°ì´í„° íƒ€ì… í™•ì¸
            validation_results['data_types'][col] = str(df[col].dtype)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        total_cells = len(df) * len(df.columns)
        filled_cells = total_cells - df.isna().sum().sum()
        validation_results['quality_score'] = (filled_cells / total_cells) * 100
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if validation_results['quality_score'] < 80:
            validation_results['recommendations'].append('ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš”')
        
        return validation_results

# ===== 4. ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ =====

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ë¬¼ë¥˜ ë§ˆìŠ¤í„° ì´ˆê¸°í™”
    logi_master = HVDCLogiMaster(
        mode=ContainmentMode.LATTICE,
        enable_ontology=True
    )
    
    # ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬
    source_file = 'MACHO_Final_Report_Complete_20250703_230904.xlsx'
    
    if Path(source_file).exists():
        result = logi_master.process_macho_data(source_file)
        print(f"ì²˜ë¦¬ ê²°ê³¼: {result}")
    else:
        print(f"âš ï¸ ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {source_file}")
    
    print("ğŸ‰ HVDC ë¬¼ë¥˜ ë§ˆìŠ¤í„° ì‹œìŠ¤í…œ ì™„ë£Œ")

if __name__ == "__main__":
    main() 

#!/usr/bin/env python3
"""
DHL Warehouse ë°ì´í„° ë³µêµ¬ ì‹œìŠ¤í…œ v1.0.0 (Green Phase)
- TDD í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼ì‹œí‚¤ëŠ” ìµœì†Œ êµ¬í˜„
- 143ê°œ DHL Warehouse ë ˆì½”ë“œ ì™„ì „ ë³µêµ¬
- ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë° ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DHLWarehouseDataRecoverySystem:
    """DHL Warehouse ë°ì´í„° ë³µêµ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.original_hitachi_file = "hvdc_macho_gpt/WAREHOUSE/data/HVDC WAREHOUSE_HITACHI(HE).xlsx"
        self.current_data_file = "HVDC_DHL_Warehouse_ì „ì²´ë³µêµ¬ì™„ë£Œ_20250704_122156.xlsx"
        self.ontology_mapping_file = "hvdc_integrated_mapping_rules_v3.0.json"
        self.expected_dhl_records = 143
        
        logger.info("DHL Warehouse ë°ì´í„° ë³µêµ¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_dhl_warehouse_records(self, df):
        """
        ì›ë³¸ ë°ì´í„°ì—ì„œ DHL Warehouse ë ˆì½”ë“œ ì¶”ì¶œ
        
        Args:
            df: ì›ë³¸ HITACHI ë°ì´í„°í”„ë ˆì„
            
        Returns:
            DataFrame: DHL Warehouse ë ˆì½”ë“œ (143ê°œ)
        """
        try:
            # DHL Warehouse ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if 'DHL Warehouse' not in df.columns:
                raise ValueError("DHL Warehouse ì»¬ëŸ¼ì´ ì›ë³¸ ë°ì´í„°ì— ì—†ìŒ")
            
            # DHL Warehouse ê°’ì´ ìˆëŠ” ë ˆì½”ë“œë§Œ ì¶”ì¶œ
            dhl_records = df[df['DHL Warehouse'].notna()].copy()
            
            logger.info(f"ì›ë³¸ ë°ì´í„°ì—ì„œ DHL Warehouse ë ˆì½”ë“œ {len(dhl_records)}ê°œ ì¶”ì¶œ")
            
            # ë°ì´í„° ê²€ì¦
            if len(dhl_records) != self.expected_dhl_records:
                logger.warning(f"ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ì™€ ë‹¤ë¦„: {len(dhl_records)} != {self.expected_dhl_records}")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
            required_columns = ['Case_No', 'Date', 'Location', 'Qty']
            for col in required_columns:
                if col not in dhl_records.columns:
                    # ì»¬ëŸ¼ ë§¤í•‘ ì‹œë„
                    mapped_col = self._map_column_name(col, dhl_records.columns)
                    if mapped_col:
                        dhl_records = dhl_records.rename(columns={mapped_col: col})
                        logger.info(f"ì»¬ëŸ¼ ë§¤í•‘: {mapped_col} -> {col}")
                    else:
                        logger.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {col}")
            
            # Case_No ì¤‘ë³µ ì œê±°
            if 'Case_No' in dhl_records.columns:
                before_dedup = len(dhl_records)
                dhl_records = dhl_records.drop_duplicates(subset=['Case_No'])
                after_dedup = len(dhl_records)
                if before_dedup != after_dedup:
                    logger.info(f"Case_No ì¤‘ë³µ ì œê±°: {before_dedup} -> {after_dedup}")
            
            # DHL Warehouse ë‚ ì§œ í˜•ì‹ ì •ê·œí™”
            dhl_records['DHL Warehouse'] = pd.to_datetime(
                dhl_records['DHL Warehouse'], 
                errors='coerce'
            )
            
            # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ í•„í„°ë§
            valid_dates = dhl_records['DHL Warehouse'].notna()
            dhl_records = dhl_records[valid_dates]
            
            logger.info(f"ìœ íš¨í•œ DHL Warehouse ë‚ ì§œê°€ ìˆëŠ” ë ˆì½”ë“œ: {len(dhl_records)}ê°œ")
            
            return dhl_records
            
        except Exception as e:
            logger.error(f"DHL Warehouse ë ˆì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _map_column_name(self, target_col, available_columns):
        """ì»¬ëŸ¼ ì´ë¦„ ë§¤í•‘ (HVDC ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)"""
        mapping = {
            'Case_No': ['Case No.', 'Case No', 'case_no', 'CASE_NO', 'Case Number', 'ID', 'no.'],
            'Date': ['Date', 'DATE', 'date', 'Created Date', 'Transaction Date', 'ETD/ATD', 'ETA/ATA'],
            'Location': ['Location', 'LOCATION', 'location', 'Current Location', 'Site', 'Status_Location'],
            'Qty': ['Qty', 'QTY', 'qty', 'Quantity', 'QUANTITY', 'Amount', 'Pkg']
        }
        
        if target_col in mapping:
            for candidate in mapping[target_col]:
                if candidate in available_columns:
                    return candidate
        
        return None
    
    def merge_dhl_records_safely(self, current_df, dhl_records):
        """
        DHL ë ˆì½”ë“œë¥¼ í˜„ì¬ ë°ì´í„°ì™€ ì•ˆì „í•˜ê²Œ ë³‘í•©
        
        Args:
            current_df: í˜„ì¬ ë°ì´í„°í”„ë ˆì„
            dhl_records: DHL Warehouse ë ˆì½”ë“œ
            
        Returns:
            DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info(f"DHL ë ˆì½”ë“œ ë³‘í•© ì‹œì‘: í˜„ì¬ {len(current_df)}ê°œ + DHL {len(dhl_records)}ê°œ")
            
            # ì»¬ëŸ¼ ì •ë ¬ (í˜„ì¬ ë°ì´í„° ê¸°ì¤€)
            current_columns = current_df.columns.tolist()
            
            # DHL ë ˆì½”ë“œì˜ ì»¬ëŸ¼ì„ í˜„ì¬ ë°ì´í„°ì— ë§ì¶¤
            aligned_dhl_records = pd.DataFrame(columns=current_columns)
            
            for col in current_columns:
                if col in dhl_records.columns:
                    aligned_dhl_records[col] = dhl_records[col]
                else:
                    # ë¹ˆ ì»¬ëŸ¼ì— ëŒ€í•´ ì ì ˆí•œ ê¸°ë³¸ê°’ ì„¤ì •
                    if col in ['Amount', 'Qty', 'Weight']:
                        aligned_dhl_records[col] = 0
                    elif col in ['Date', 'Status_Location_Date']:
                        aligned_dhl_records[col] = pd.NaT
                    else:
                        aligned_dhl_records[col] = np.nan
            
            # Case_No ì¤‘ë³µ ê²€ì‚¬
            if 'Case_No' in current_df.columns and 'Case_No' in aligned_dhl_records.columns:
                current_case_nos = set(current_df['Case_No'].dropna())
                dhl_case_nos = set(aligned_dhl_records['Case_No'].dropna())
                
                duplicates = current_case_nos.intersection(dhl_case_nos)
                if duplicates:
                    logger.warning(f"Case_No ì¤‘ë³µ ë°œê²¬: {len(duplicates)}ê°œ")
                    # ì¤‘ë³µ ì œê±°
                    aligned_dhl_records = aligned_dhl_records[
                        ~aligned_dhl_records['Case_No'].isin(duplicates)
                    ]
                    logger.info(f"ì¤‘ë³µ ì œê±° í›„ DHL ë ˆì½”ë“œ: {len(aligned_dhl_records)}ê°œ")
            
            # ë°ì´í„° ë³‘í•©
            merged_df = pd.concat([current_df, aligned_dhl_records], ignore_index=True)
            
            logger.info(f"DHL ë ˆì½”ë“œ ë³‘í•© ì™„ë£Œ: {len(merged_df)}ê°œ ë ˆì½”ë“œ")
            
            return merged_df
            
        except Exception as e:
            logger.error(f"DHL ë ˆì½”ë“œ ë³‘í•© ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_final_integrated_dataset(self, current_df, original_df):
        """
        ìµœì¢… í†µí•© ë°ì´í„°ì…‹ ìƒì„±
        
        Args:
            current_df: í˜„ì¬ ë°ì´í„°í”„ë ˆì„
            original_df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            DataFrame: ìµœì¢… í†µí•© ë°ì´í„°ì…‹
        """
        try:
            logger.info("ìµœì¢… í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
            
            # 1. DHL Warehouse ë ˆì½”ë“œ ì¶”ì¶œ
            dhl_records = self.extract_dhl_warehouse_records(original_df)
            
            # 2. ì•ˆì „í•˜ê²Œ ë³‘í•©
            merged_df = self.merge_dhl_records_safely(current_df, dhl_records)
            
            # 3. ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦
            final_dataset = self._clean_and_validate_dataset(merged_df)
            
            # 4. ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš©
            final_dataset = self._apply_ontology_mapping(final_dataset)
            
            logger.info(f"ìµœì¢… í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(final_dataset)}ê°œ ë ˆì½”ë“œ")
            
            return final_dataset
            
        except Exception as e:
            logger.error(f"ìµœì¢… í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _clean_and_validate_dataset(self, df):
        """ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦"""
        try:
            logger.info("ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ì‹œì‘")
            
            # 1. ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°
            before_dedup = len(df)
            if 'Case_No' in df.columns:
                df = df.drop_duplicates(subset=['Case_No'])
            else:
                df = df.drop_duplicates()
            
            after_dedup = len(df)
            if before_dedup != after_dedup:
                logger.info(f"ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°: {before_dedup} -> {after_dedup}")
            
            # 2. í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
            required_columns = ['Case_No', 'Date', 'Location', 'Qty']
            for col in required_columns:
                if col not in df.columns:
                    logger.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {col}")
            
            # 3. ë°ì´í„° íƒ€ì… ì •ê·œí™”
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            
            if 'DHL Warehouse' in df.columns:
                df['DHL Warehouse'] = pd.to_datetime(df['DHL Warehouse'], errors='coerce')
            
            if 'Status_Location_Date' in df.columns:
                df['Status_Location_Date'] = pd.to_datetime(df['Status_Location_Date'], errors='coerce')
            
            # 4. ìˆ˜ì¹˜í˜• ë°ì´í„° ì •ë¦¬
            numeric_columns = ['Qty', 'Amount', 'Weight']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            logger.info("ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ì™„ë£Œ")
            
            return df
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _apply_ontology_mapping(self, df):
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš©"""
        try:
            logger.info("ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš© ì‹œì‘")
            
            # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê·œì¹™ ë¡œë“œ
            if Path(self.ontology_mapping_file).exists():
                with open(self.ontology_mapping_file, 'r', encoding='utf-8') as f:
                    mapping_rules = json.load(f)
                
                field_mappings = mapping_rules.get('field_mappings', {})
                
                # DHL Warehouse ë§¤í•‘ ê·œì¹™ ì¶”ê°€ (ì—†ìœ¼ë©´)
                if 'DHL Warehouse' not in field_mappings:
                    field_mappings['DHL Warehouse'] = {
                        "ontology_property": "hasWarehouseEntry",
                        "data_type": "datetime", 
                        "description": "DHL warehouse entry timestamp"
                    }
                    
                    # ë§¤í•‘ ê·œì¹™ ì—…ë°ì´íŠ¸
                    mapping_rules['field_mappings'] = field_mappings
                    with open(self.ontology_mapping_file, 'w', encoding='utf-8') as f:
                        json.dump(mapping_rules, f, indent=2, ensure_ascii=False)
                    
                    logger.info("DHL Warehouse ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê·œì¹™ ì¶”ê°€")
                
                # ë§¤í•‘ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
                mapped_columns = sum(1 for col in df.columns if col in field_mappings)
                coverage = mapped_columns / len(df.columns)
                
                logger.info(f"ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì»¤ë²„ë¦¬ì§€: {coverage:.2f} ({mapped_columns}/{len(df.columns)})")
                
            else:
                logger.warning("ì˜¨í†¨ë¡œì§€ ë§¤í•‘ íŒŒì¼ì´ ì—†ìŒ")
            
            logger.info("ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš© ì™„ë£Œ")
            
            return df
            
        except Exception as e:
            logger.error(f"ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì ìš© ì‹¤íŒ¨: {str(e)}")
            return df  # ë§¤í•‘ ì‹¤íŒ¨í•´ë„ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    
    def run_full_recovery(self):
        """ì „ì²´ DHL Warehouse ë°ì´í„° ë³µêµ¬ ì‹¤í–‰"""
        try:
            logger.info("=== DHL Warehouse ë°ì´í„° ì „ì²´ ë³µêµ¬ ì‹œì‘ ===")
            
            # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ
            if not Path(self.original_hitachi_file).exists():
                raise FileNotFoundError(f"ì›ë³¸ íŒŒì¼ì´ ì—†ìŒ: {self.original_hitachi_file}")
            
            original_df = pd.read_excel(self.original_hitachi_file)
            logger.info(f"ì›ë³¸ HITACHI ë°ì´í„° ë¡œë“œ: {len(original_df)}ê°œ ë ˆì½”ë“œ")
            
            # 2. í˜„ì¬ ë°ì´í„° ë¡œë“œ
            if not Path(self.current_data_file).exists():
                logger.warning(f"í˜„ì¬ ë°ì´í„° íŒŒì¼ì´ ì—†ìŒ: {self.current_data_file}")
                current_df = pd.DataFrame()
            else:
                current_df = pd.read_excel(self.current_data_file)
                logger.info(f"í˜„ì¬ ë°ì´í„° ë¡œë“œ: {len(current_df)}ê°œ ë ˆì½”ë“œ")
            
            # 3. ìµœì¢… í†µí•© ë°ì´í„°ì…‹ ìƒì„±
            final_dataset = self.create_final_integrated_dataset(current_df, original_df)
            
            # 4. ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"HVDC_DHL_Warehouse_ì™„ì „ë³µêµ¬_{timestamp}.xlsx"
            
            final_dataset.to_excel(output_file, index=False)
            logger.info(f"ìµœì¢… ê²°ê³¼ ì €ì¥: {output_file}")
            
            # 5. ë³µêµ¬ ê²°ê³¼ ë¦¬í¬íŠ¸
            self._generate_recovery_report(final_dataset, output_file)
            
            logger.info("=== DHL Warehouse ë°ì´í„° ì „ì²´ ë³µêµ¬ ì™„ë£Œ ===")
            
            return final_dataset, output_file
            
        except Exception as e:
            logger.error(f"ì „ì²´ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _generate_recovery_report(self, df, output_file):
        """ë³µêµ¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            dhl_records = df[df['DHL Warehouse'].notna()]
            
            report = {
                "ë³µêµ¬_ì™„ë£Œ_ì‹œê°„": datetime.now().isoformat(),
                "ì „ì²´_ë ˆì½”ë“œ_ìˆ˜": len(df),
                "DHL_Warehouse_ë ˆì½”ë“œ_ìˆ˜": len(dhl_records),
                "ë³µêµ¬_ì„±ê³µë¥ ": f"{len(dhl_records) / self.expected_dhl_records * 100:.1f}%",
                "ì¶œë ¥_íŒŒì¼": output_file,
                "DHL_ë‚ ì§œ_ë²”ìœ„": {
                    "ìµœì†Œ": dhl_records['DHL Warehouse'].min().isoformat() if not dhl_records.empty else None,
                    "ìµœëŒ€": dhl_records['DHL Warehouse'].max().isoformat() if not dhl_records.empty else None
                }
            }
            
            # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"DHL_Warehouse_ë³µêµ¬_ë¦¬í¬íŠ¸_{timestamp}.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ë³µêµ¬ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            logger.info(f"DHL Warehouse ë ˆì½”ë“œ ë³µêµ¬: {len(dhl_records)}ê°œ / {self.expected_dhl_records}ê°œ")
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # DHL Warehouse ë°ì´í„° ë³µêµ¬ ì‹œìŠ¤í…œ ìƒì„±
        recovery_system = DHLWarehouseDataRecoverySystem()
        
        # ì „ì²´ ë³µêµ¬ ì‹¤í–‰
        final_dataset, output_file = recovery_system.run_full_recovery()
        
        print(f"âœ… DHL Warehouse ë°ì´í„° ë³µêµ¬ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"ğŸ“Š ì´ ë ˆì½”ë“œ ìˆ˜: {len(final_dataset)}")
        
        # DHL ë ˆì½”ë“œ í™•ì¸
        dhl_records = final_dataset[final_dataset['DHL Warehouse'].notna()]
        print(f"ğŸ¢ DHL Warehouse ë ˆì½”ë“œ: {len(dhl_records)}ê°œ")
        
        return final_dataset, output_file
        
    except Exception as e:
        print(f"âŒ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    main() 
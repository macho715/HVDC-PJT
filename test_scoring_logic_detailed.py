#!/usr/bin/env python3
"""
Context Engineering í‰ê°€ ë¡œì§ ìƒì„¸ í…ŒìŠ¤íŠ¸
========================================

HVDC í”„ë¡œì íŠ¸ì˜ Context Engineering í‰ê°€ ì‹œìŠ¤í…œì˜ ëª¨ë“  ì¸¡ë©´ì„
ìƒì„¸í•˜ê²Œ í…ŒìŠ¤íŠ¸í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any
from src.context_engineering_integration import (
    HVDCContextWindow, HVDCContextScoring, HVDCContextProtocol,
    HVDCContextEngineeringIntegration
)
from src.logi_master_system import LogiMasterSystem

class ScoringLogicTester:
    """í‰ê°€ ë¡œì§ ìƒì„¸ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.scoring = HVDCContextScoring()
        self.protocol = HVDCContextProtocol()
        self.test_results = []
        
    def test_context_quality_scoring_breakdown(self):
        """Context í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” Context í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        test_cases = [
            {
                "name": "ì™„ì „í•œ Context (ìµœê³  ì ìˆ˜)",
                "context": HVDCContextWindow(
                    prompt="ì™„ì „í•œ í”„ë¡¬í”„íŠ¸",
                    examples=[{"test": "data"}],
                    memory={"key": "value"},
                    tools=["tool1", "tool2"],
                    state={"status": "active"},
                    logistics_context={"project": "HVDC"},
                    fanr_compliance={"status": "compliant"},
                    kpi_metrics={"performance": 0.95},
                    field_resonance=0.8,
                    attractor_strength=0.7
                ),
                "expected_score": 1.0
            },
            {
                "name": "ê¸°ë³¸ Context (ì¤‘ê°„ ì ìˆ˜)",
                "context": HVDCContextWindow(
                    prompt="ê¸°ë³¸ í”„ë¡¬í”„íŠ¸",
                    examples=[{"test": "data"}],
                    tools=["tool1"],
                    logistics_context={"project": "HVDC"},
                    field_resonance=0.6,
                    attractor_strength=0.6
                ),
                "expected_score": 0.65
            },
            {
                "name": "ìµœì†Œ Context (ë‚®ì€ ì ìˆ˜)",
                "context": HVDCContextWindow(
                    prompt="ìµœì†Œ í”„ë¡¬í”„íŠ¸"
                ),
                "expected_score": 0.2
            },
            {
                "name": "ë„ë©”ì¸ íŠ¹í™” Context",
                "context": HVDCContextWindow(
                    prompt="HVDC ë¬¼ë¥˜ ë¶„ì„",
                    examples=[{"vendor": "HITACHI", "result": "success"}],
                    tools=["warehouse_api", "weather_api"],
                    logistics_context={
                        "project": "HVDC",
                        "vendor": "HITACHI/SIEMENS",
                        "location": "UAE"
                    },
                    fanr_compliance={"certification": "valid"},
                    kpi_metrics={"efficiency": 0.92},
                    field_resonance=0.9,
                    attractor_strength=0.8
                ),
                "expected_score": 1.0
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}: {test_case['name']}")
            print("-" * 40)
            
            context = test_case["context"]
            actual_score = self.scoring.score_context_quality(context)
            expected_score = test_case["expected_score"]
            
            # ì„¸ë¶€ ì ìˆ˜ ë¶„ì„
            scores = []
            score_details = []
            
            if context.prompt:
                scores.append(0.2)
                score_details.append("âœ… Prompt ì¡´ì¬ (+0.2)")
            else:
                score_details.append("âŒ Prompt ì—†ìŒ (+0.0)")
            
            if context.examples:
                scores.append(0.15)
                score_details.append("âœ… Examples ì¡´ì¬ (+0.15)")
            else:
                score_details.append("âŒ Examples ì—†ìŒ (+0.0)")
            
            if context.memory:
                scores.append(0.15)
                score_details.append("âœ… Memory ì¡´ì¬ (+0.15)")
            else:
                score_details.append("âŒ Memory ì—†ìŒ (+0.0)")
            
            if context.tools:
                scores.append(0.1)
                score_details.append("âœ… Tools ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ Tools ì—†ìŒ (+0.0)")
            
            if context.state:
                scores.append(0.1)
                score_details.append("âœ… State ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ State ì—†ìŒ (+0.0)")
            
            if context.logistics_context:
                scores.append(0.1)
                score_details.append("âœ… Logistics Context ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ Logistics Context ì—†ìŒ (+0.0)")
            
            if context.fanr_compliance:
                scores.append(0.1)
                score_details.append("âœ… FANR Compliance ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ FANR Compliance ì—†ìŒ (+0.0)")
            
            if context.kpi_metrics:
                scores.append(0.1)
                score_details.append("âœ… KPI Metrics ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ KPI Metrics ì—†ìŒ (+0.0)")
            
            if context.field_resonance > 0.5:
                scores.append(0.05)
                score_details.append(f"âœ… Field Resonance > 0.5 ({context.field_resonance:.1f}) (+0.05)")
            else:
                score_details.append(f"âŒ Field Resonance <= 0.5 ({context.field_resonance:.1f}) (+0.0)")
            
            if context.attractor_strength > 0.5:
                scores.append(0.05)
                score_details.append(f"âœ… Attractor Strength > 0.5 ({context.attractor_strength:.1f}) (+0.05)")
            else:
                score_details.append(f"âŒ Attractor Strength <= 0.5 ({context.attractor_strength:.1f}) (+0.0)")
            
            calculated_score = min(sum(scores), 1.0)
            
            print("ğŸ“‹ ì„¸ë¶€ ì ìˆ˜ ë¶„ì„:")
            for detail in score_details:
                print(f"   {detail}")
            
            print(f"\nğŸ“Š ì ìˆ˜ ê²°ê³¼:")
            print(f"   ê³„ì‚°ëœ ì ìˆ˜: {calculated_score:.3f}")
            print(f"   ì‹¤ì œ ì ìˆ˜: {actual_score:.3f}")
            print(f"   ì˜ˆìƒ ì ìˆ˜: {expected_score:.3f}")
            
            # ê²€ì¦
            if abs(actual_score - calculated_score) < 0.001:
                print("   âœ… ì ìˆ˜ ê³„ì‚° ì •í™•")
            else:
                print("   âŒ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜")
            
            if abs(actual_score - expected_score) < 0.1:
                print("   âœ… ì˜ˆìƒ ì ìˆ˜ì™€ ì¼ì¹˜")
            else:
                print("   âš ï¸ ì˜ˆìƒ ì ìˆ˜ì™€ ì°¨ì´")
            
            self.test_results.append({
                "test_case": test_case["name"],
                "actual_score": actual_score,
                "expected_score": expected_score,
                "calculated_score": calculated_score,
                "passed": abs(actual_score - expected_score) < 0.1
            })
    
    def test_response_quality_scoring_breakdown(self):
        """ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        test_responses = [
            {
                "name": "ì™„ë²½í•œ ì‘ë‹µ",
                "response": {
                    "status": "SUCCESS",
                    "confidence": 0.95,
                    "recommended_commands": ["get_kpi", "switch_mode"],
                    "mode": "PRIME",
                    "timestamp": datetime.now().isoformat()
                },
                "expected_score": 1.0
            },
            {
                "name": "ì„±ê³µ ì‘ë‹µ (ì‹ ë¢°ë„ ë‚®ìŒ)",
                "response": {
                    "status": "SUCCESS",
                    "confidence": 0.7,
                    "recommended_commands": ["get_kpi"],
                    "mode": "PRIME"
                },
                "expected_score": 0.7
            },
            {
                "name": "ì‹¤íŒ¨ ì‘ë‹µ",
                "response": {
                    "status": "ERROR",
                    "error_message": "Command not found",
                    "confidence": 0.0
                },
                "expected_score": 0.0
            },
            {
                "name": "ë¶€ë¶„ ì„±ê³µ ì‘ë‹µ",
                "response": {
                    "status": "SUCCESS",
                    "confidence": 0.85,
                    "mode": "LATTICE"
                },
                "expected_score": 0.5
            }
        ]
        
        for i, test_case in enumerate(test_responses, 1):
            print(f"\nğŸ“Š ì‘ë‹µ í…ŒìŠ¤íŠ¸ {i}: {test_case['name']}")
            print("-" * 40)
            
            response = test_case["response"]
            actual_score = self.scoring.score_response_quality(response)
            expected_score = test_case["expected_score"]
            
            # ì„¸ë¶€ ì ìˆ˜ ë¶„ì„
            scores = []
            score_details = []
            
            if response.get("status") == "SUCCESS":
                scores.append(0.3)
                score_details.append("âœ… Status = SUCCESS (+0.3)")
            else:
                score_details.append("âŒ Status != SUCCESS (+0.0)")
            
            if response.get("confidence", 0) > 0.9:
                scores.append(0.3)
                score_details.append(f"âœ… Confidence > 0.9 ({response.get('confidence', 0):.2f}) (+0.3)")
            else:
                score_details.append(f"âŒ Confidence <= 0.9 ({response.get('confidence', 0):.2f}) (+0.0)")
            
            if response.get("recommended_commands"):
                scores.append(0.2)
                score_details.append("âœ… Recommended Commands ì¡´ì¬ (+0.2)")
            else:
                score_details.append("âŒ Recommended Commands ì—†ìŒ (+0.0)")
            
            if response.get("mode"):
                scores.append(0.1)
                score_details.append("âœ… Mode ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ Mode ì—†ìŒ (+0.0)")
            
            if response.get("timestamp"):
                scores.append(0.1)
                score_details.append("âœ… Timestamp ì¡´ì¬ (+0.1)")
            else:
                score_details.append("âŒ Timestamp ì—†ìŒ (+0.0)")
            
            calculated_score = min(sum(scores), 1.0)
            
            print("ğŸ“‹ ì„¸ë¶€ ì ìˆ˜ ë¶„ì„:")
            for detail in score_details:
                print(f"   {detail}")
            
            print(f"\nğŸ“Š ì ìˆ˜ ê²°ê³¼:")
            print(f"   ê³„ì‚°ëœ ì ìˆ˜: {calculated_score:.3f}")
            print(f"   ì‹¤ì œ ì ìˆ˜: {actual_score:.3f}")
            print(f"   ì˜ˆìƒ ì ìˆ˜: {expected_score:.3f}")
            
            # ê²€ì¦
            if abs(actual_score - calculated_score) < 0.001:
                print("   âœ… ì ìˆ˜ ê³„ì‚° ì •í™•")
            else:
                print("   âŒ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜")
            
            if abs(actual_score - expected_score) < 0.1:
                print("   âœ… ì˜ˆìƒ ì ìˆ˜ì™€ ì¼ì¹˜")
            else:
                print("   âš ï¸ ì˜ˆìƒ ì ìˆ˜ì™€ ì°¨ì´")
    
    async def test_command_context_creation(self):
        """ëª…ë ¹ì–´ë³„ Context ìƒì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” ëª…ë ¹ì–´ë³„ Context ìƒì„± í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        commands = [
            ("enhance_dashboard", {"dashboard_id": "main", "enhancement_type": "weather_integration"}),
            ("excel_query", {"query": "Show me all Hitachi equipment"}),
            ("weather_tie", {"weather_data": "storm_warning"}),
            ("get_kpi", {"metric": "efficiency"}),
            ("switch_mode", {"mode": "LATTICE"})
        ]
        
        for i, (command, parameters) in enumerate(commands, 1):
            print(f"\nğŸ“Š ëª…ë ¹ì–´ í…ŒìŠ¤íŠ¸ {i}: {command}")
            print("-" * 40)
            
            context = await self.protocol.create_context_for_command(command, parameters)
            score = self.scoring.score_context_quality(context)
            
            print(f"ğŸ“ ìƒì„±ëœ Context:")
            print(f"   Prompt: {context.prompt[:50]}...")
            print(f"   Examples: {len(context.examples)}ê°œ")
            print(f"   Tools: {context.tools}")
            print(f"   Logistics Context: {context.logistics_context}")
            print(f"   Field Resonance: {context.field_resonance}")
            print(f"   Attractor Strength: {context.attractor_strength}")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {score:.3f}")
            
            # í’ˆì§ˆ í‰ê°€
            if score >= 0.7:
                print("   ğŸŸ¢ ë†’ì€ í’ˆì§ˆ")
            elif score >= 0.5:
                print("   ğŸŸ¡ ì¤‘ê°„ í’ˆì§ˆ")
            else:
                print("   ğŸ”´ ë‚®ì€ í’ˆì§ˆ")
    
    async def test_integration_workflow(self):
        """í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # LogiMaster ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logi_master = LogiMasterSystem()
        await logi_master.initialize()
        
        # Context Engineering í†µí•©
        context_integration = HVDCContextEngineeringIntegration(logi_master)
        
        # í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ ì‹¤í–‰
        test_commands = [
            ("enhance_dashboard", {"dashboard_id": "main", "enhancement_type": "weather_integration"}),
            ("get_kpi", {"metric": "efficiency"}),
            ("switch_mode", {"mode": "LATTICE"})
        ]
        
        for i, (command, parameters) in enumerate(test_commands, 1):
            print(f"\nğŸ“Š í†µí•© í…ŒìŠ¤íŠ¸ {i}: {command}")
            print("-" * 40)
            
            result = await context_integration.execute_command_with_context(command, parameters)
            
            print(f"ğŸ“ ì‹¤í–‰ ê²°ê³¼:")
            print(f"   Status: {result.get('status')}")
            print(f"   Context Score: {result.get('context_engineering', {}).get('context_score', 0):.3f}")
            print(f"   Response Score: {result.get('context_engineering', {}).get('response_score', 0):.3f}")
            print(f"   Field Resonance: {result.get('context_engineering', {}).get('field_resonance', 0):.3f}")
            print(f"   Attractor Strength: {result.get('context_engineering', {}).get('attractor_strength', 0):.3f}")
            
            # í’ˆì§ˆ í‰ê°€
            context_score = result.get('context_engineering', {}).get('context_score', 0)
            response_score = result.get('context_engineering', {}).get('response_score', 0)
            
            if context_score >= 0.7 and response_score >= 0.7:
                print("   ğŸŸ¢ ìš°ìˆ˜í•œ í’ˆì§ˆ")
            elif context_score >= 0.5 and response_score >= 0.5:
                print("   ğŸŸ¡ ë³´í†µ í’ˆì§ˆ")
            else:
                print("   ğŸ”´ ê°œì„  í•„ìš”")
        
        # Context ë¶„ì„
        analytics = await context_integration.get_context_analytics()
        print(f"\nğŸ“Š Context ë¶„ì„ ê²°ê³¼:")
        print(f"   ì´ Context ìˆ˜: {analytics.get('total_contexts', 0)}")
        print(f"   í‰ê·  Context ì ìˆ˜: {analytics.get('average_context_score', 0):.3f}")
        print(f"   í‰ê·  ì‘ë‹µ ì ìˆ˜: {analytics.get('average_response_score', 0):.3f}")
        print(f"   Field Resonance íŠ¸ë Œë“œ: {analytics.get('field_resonance_trend', [])}")
        print(f"   Attractor Strength íŠ¸ë Œë“œ: {analytics.get('attractor_strength_trend', [])}")
        print(f"   ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë„êµ¬: {analytics.get('most_used_tools', [])}")
        print(f"   í’ˆì§ˆ ë¶„í¬: {analytics.get('context_quality_distribution', {})}")
    
    def generate_test_report(self):
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result["passed"])
        failed_tests = total_tests - passed_tests
        
        print(f"ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"   í†µê³¼: {passed_tests}ê°œ")
        print(f"   ì‹¤íŒ¨: {failed_tests}ê°œ")
        print(f"   í†µê³¼ìœ¨: {passed_tests/total_tests*100:.1f}%")
        
        if failed_tests > 0:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for result in self.test_results:
                if not result["passed"]:
                    print(f"   - {result['test_case']}: ì˜ˆìƒ {result['expected_score']:.3f}, ì‹¤ì œ {result['actual_score']:.3f}")
        
        print(f"\nğŸ“ˆ ì ìˆ˜ ë¶„í¬:")
        scores = [result["actual_score"] for result in self.test_results]
        if scores:
            print(f"   ìµœê³  ì ìˆ˜: {max(scores):.3f}")
            print(f"   ìµœì € ì ìˆ˜: {min(scores):.3f}")
            print(f"   í‰ê·  ì ìˆ˜: {sum(scores)/len(scores):.3f}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Context Engineering í‰ê°€ ë¡œì§ ìƒì„¸ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    tester = ScoringLogicTester()
    
    # 1. Context í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„
    tester.test_context_quality_scoring_breakdown()
    
    # 2. ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ì„¸ë¶€ ë¶„ì„
    tester.test_response_quality_scoring_breakdown()
    
    # 3. ëª…ë ¹ì–´ë³„ Context ìƒì„± í…ŒìŠ¤íŠ¸
    await tester.test_command_context_creation()
    
    # 4. í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    await tester.test_integration_workflow()
    
    # 5. í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
    tester.generate_test_report()
    
    print("\nğŸ‰ Context Engineering í‰ê°€ ë¡œì§ ìƒì„¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(main()) 
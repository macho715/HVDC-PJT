#!/usr/bin/env python3
"""
ì˜¨í†¨ë¡œì§€ í†µí•© í‘œì¤€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸° - BACKUP v1.0.0
- ê¸°ì¡´ í‘œì¤€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê¸°ëŠ¥
- ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë° RDF ë³€í™˜ ê¸°ëŠ¥
- HVDC ì˜¨í†¨ë¡œì§€ ì—”ì§„ê³¼ì˜ ì™„ì „ í†µí•©

BACKUP DATE: 2025-01-03
MACHO-GPT: v3.4-mini
STATUS: Production Ready
"""

import pandas as pd
import numpy as np
from datetime import datetime, date
import json
import sqlite3
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import logging

# RDF ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
    RDF_AVAILABLE = True
    # ì˜¨í†¨ë¡œì§€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
    HVDC = Namespace("http://samsung.com/project-logistics#")
    EX = Namespace("http://example.org/hvdc#")
except ImportError:
    RDF_AVAILABLE = False
    print("âš ï¸ RDFLib ë¯¸ì„¤ì¹˜ - RDF ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    # ë”ë¯¸ í´ë˜ìŠ¤ ì •ì˜
    class DummyNamespace:
        def __getitem__(self, key):
            return f"dummy:{key}"
    HVDC = DummyNamespace()
    EX = DummyNamespace()

# ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ê²€ì¦ í´ë˜ìŠ¤ë“¤ import
from standard_data_schema_validator import (
    DataType, ValidationLevel, FieldSchema, ValidationResult, 
    StandardDataSchemaValidator
)

@dataclass
class OntologyMapping:
    """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì •ë³´"""
    field_name: str
    rdf_property: str
    rdf_class: Optional[str] = None
    data_transform: Optional[str] = None
    semantic_rules: List[str] = field(default_factory=list)

class OntologyIntegratedSchemaValidator(StandardDataSchemaValidator):
    """ì˜¨í†¨ë¡œì§€ í†µí•© ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸° - MACHO-GPT v3.4-mini ì‹œìŠ¤í…œ"""
    
    def __init__(self, enable_ontology: bool = True):
        """ì´ˆê¸°í™”"""
        super().__init__()
        self.enable_ontology = enable_ontology and RDF_AVAILABLE
        
        if self.enable_ontology:
            self.graph = Graph()
            self.setup_ontology_namespaces()
            self.ontology_mappings = self._define_ontology_mappings()
            self.init_ontology_database()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # MACHO-GPT í†µê³„
        self.macho_stats = {
            'processed_records': 0,
            'confidence_score': 0.0,
            'success_rate': 0.0,
            'start_time': datetime.now()
        }
        
    def setup_ontology_namespaces(self):
        """ì˜¨í†¨ë¡œì§€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
        if not self.enable_ontology:
            return
            
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
        self.graph.bind("hvdc", HVDC)
        self.graph.bind("ex", EX)
        
        # ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        self._load_base_ontology_schema()
        
    def _load_base_ontology_schema(self):
        """ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ - Samsung C&T Ã— ADNOCÂ·DSV í‘œì¤€"""
        # í´ë˜ìŠ¤ ì •ì˜
        classes = [
            (HVDC.InvoiceRecord, "Invoice Record"),
            (HVDC.TransportEvent, "Transport Event"),
            (HVDC.Warehouse, "Warehouse"),
            (HVDC.IndoorWarehouse, "Indoor Warehouse"),
            (HVDC.OutdoorWarehouse, "Outdoor Warehouse"),
            (HVDC.Site, "Site"),
            (HVDC.DangerousCargoWarehouse, "Dangerous Cargo Warehouse"),
            (HVDC.Cargo, "Cargo"),
            (HVDC.HitachiCargo, "Hitachi Cargo"),
            (HVDC.SiemensCargo, "Siemens Cargo"),
            (HVDC.StockSnapshot, "Stock Snapshot")
        ]
        
        for class_uri, label in classes:
            self.graph.add((class_uri, RDF.type, OWL.Class))
            self.graph.add((class_uri, RDFS.label, Literal(label)))
            
        # ì†ì„± ì •ì˜ - HVDC PROJECT í‘œì¤€
        properties = [
            (HVDC.hasRecordId, "Record ID"),
            (HVDC.hasOperationMonth, "Operation Month"),
            (HVDC.hasHVDCCode, "HVDC Code"),
            (HVDC.hasWorkType, "Work Type"),
            (HVDC.hasCargoType, "Cargo Type"),
            (HVDC.hasWarehouseName, "Warehouse Name"),
            (HVDC.hasPackageCount, "Package Count"),
            (HVDC.hasWeight, "Weight"),
            (HVDC.hasVolume, "Volume"),
            (HVDC.hasArea, "Area"),
            (HVDC.hasAmount, "Amount"),
            (HVDC.hasHandlingIn, "Handling In"),
            (HVDC.hasHandlingOut, "Handling Out"),
            (HVDC.hasRent, "Rent"),
            (HVDC.hasFlowCode, "Flow Code"),
            (HVDC.hasWHHandling, "WH Handling")
        ]
        
        for prop_uri, label in properties:
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(label)))
            
        # ê³„ì¸µ ê´€ê³„ ì •ì˜
        warehouse_hierarchy = [
            (HVDC.IndoorWarehouse, HVDC.Warehouse),
            (HVDC.OutdoorWarehouse, HVDC.Warehouse),
            (HVDC.Site, HVDC.Warehouse),
            (HVDC.DangerousCargoWarehouse, HVDC.Warehouse),
            (HVDC.HitachiCargo, HVDC.Cargo),
            (HVDC.SiemensCargo, HVDC.Cargo)
        ]
        
        for subclass, superclass in warehouse_hierarchy:
            self.graph.add((subclass, RDFS.subClassOf, superclass))
    
    def _define_ontology_mappings(self) -> Dict[str, OntologyMapping]:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì •ì˜ - MACHO-GPT v3.4-mini í‘œì¤€"""
        return {
            'record_id': OntologyMapping(
                field_name='record_id',
                rdf_property='hasRecordId',
                rdf_class='InvoiceRecord',
                semantic_rules=['unique_identifier']
            ),
            'operation_month': OntologyMapping(
                field_name='operation_month',
                rdf_property='hasOperationMonth',
                data_transform='datetime_to_xsd'
            ),
            'hvdc_project_code': OntologyMapping(
                field_name='hvdc_project_code',
                rdf_property='hasHVDCCode',
                semantic_rules=['project_classification']
            ),
            'work_type': OntologyMapping(
                field_name='work_type',
                rdf_property='hasWorkType',
                semantic_rules=['operation_type_classification']
            ),
            'cargo_type': OntologyMapping(
                field_name='cargo_type',
                rdf_property='hasCargoType',
                rdf_class='Cargo',
                semantic_rules=['cargo_type_hierarchy']
            ),
            'warehouse_name': OntologyMapping(
                field_name='warehouse_name',
                rdf_property='hasWarehouseName',
                rdf_class='Warehouse',
                semantic_rules=['warehouse_type_classification']
            ),
            'package_count': OntologyMapping(
                field_name='package_count',
                rdf_property='hasPackageCount',
                data_transform='integer_to_xsd'
            ),
            'weight_kg': OntologyMapping(
                field_name='weight_kg',
                rdf_property='hasWeight',
                data_transform='decimal_to_xsd',
                semantic_rules=['weight_risk_classification']
            ),
            'volume_cbm': OntologyMapping(
                field_name='volume_cbm',
                rdf_property='hasVolume',
                data_transform='decimal_to_xsd'
            ),
            'area_sqm': OntologyMapping(
                field_name='area_sqm',
                rdf_property='hasArea',
                data_transform='decimal_to_xsd'
            ),
            'amount_aed': OntologyMapping(
                field_name='amount_aed',
                rdf_property='hasAmount',
                data_transform='decimal_to_xsd',
                semantic_rules=['cost_classification']
            ),
            'handling_in': OntologyMapping(
                field_name='handling_in',
                rdf_property='hasHandlingIn',
                data_transform='decimal_to_xsd'
            ),
            'handling_out': OntologyMapping(
                field_name='handling_out',
                rdf_property='hasHandlingOut',
                data_transform='decimal_to_xsd'
            ),
            'rent': OntologyMapping(
                field_name='rent',
                rdf_property='hasRent',
                data_transform='decimal_to_xsd',
                semantic_rules=['cost_classification']
            ),
            'flow_code': OntologyMapping(
                field_name='flow_code',
                rdf_property='hasFlowCode',
                semantic_rules=['flow_classification']
            ),
            'wh_handling': OntologyMapping(
                field_name='wh_handling',
                rdf_property='hasWHHandling',
                data_transform='integer_to_xsd'
            )
        }
        
    def init_ontology_database(self):
        """ì˜¨í†¨ë¡œì§€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.db_path = 'hvdc_ontology_instances.db'
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì˜¨í†¨ë¡œì§€ ì¸ìŠ¤í„´ìŠ¤ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ontology_instances (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    record_id TEXT UNIQUE,
                    rdf_subject TEXT,
                    rdf_type TEXT,
                    properties TEXT,  -- JSON í˜•íƒœ
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì‹œë§¨í‹± ë¶„ë¥˜ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS semantic_classifications (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    record_id TEXT,
                    field_name TEXT,
                    original_value TEXT,
                    semantic_class TEXT,
                    confidence_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ì˜¨í†¨ë¡œì§€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
    def validate_with_ontology(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ìˆ˜í–‰"""
        self.macho_stats['processed_records'] = len(data)
        self.logger.info(f"ğŸ” ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì‹œì‘ - {len(data)}ê±´")
        
        # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦
        base_results = self.validate_data(data)
        
        # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ìˆ˜í–‰ (RDF ê¸°ëŠ¥ í™œì„±í™”ì‹œ)
        ontology_results = {}
        if self.enable_ontology:
            ontology_results = self._perform_ontology_mapping(data)
            
        # í†µí•© ê²°ê³¼ êµ¬ì„±
        integrated_results = {
            **base_results,
            'ontology_enabled': self.enable_ontology,
            'macho_gpt_version': 'v3.4-mini',
            'processing_time': (datetime.now() - self.macho_stats['start_time']).total_seconds()
        }
        
        if ontology_results:
            integrated_results['ontology_mapping'] = ontology_results
            
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (MACHO-GPT ê¸°ì¤€ â‰¥90%)
        confidence = self._calculate_confidence_score(integrated_results)
        self.macho_stats['confidence_score'] = confidence
        integrated_results['macho_confidence'] = confidence
        
        self.logger.info(f"âœ… ì˜¨í†¨ë¡œì§€ í†µí•© ê²€ì¦ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.1f}%")
        return integrated_results
        
    def _perform_ontology_mapping(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ìˆ˜í–‰"""
        mapping_results = {
            'mapped_records': 0,
            'mapping_errors': [],
            'semantic_classifications': {},
            'rdf_graph_size': 0,
            'mapping_success_rate': 0.0
        }
        
        try:
            mapped_count = 0
            
            for idx, row in data.iterrows():
                try:
                    record_id = row.get('record_id', f'HVDC_INV_{idx:06d}')
                    
                    # ê° í•„ë“œë¥¼ ì˜¨í†¨ë¡œì§€ ë§¤í•‘
                    mapped_properties = {}
                    semantic_classes = {}
                    
                    for field_name, mapping in self.ontology_mappings.items():
                        if field_name in row and pd.notna(row[field_name]):
                            value = row[field_name]
                            
                            # ë°ì´í„° ë³€í™˜
                            transformed_value = self._transform_value(value, mapping.data_transform)
                            mapped_properties[mapping.rdf_property] = transformed_value
                            
                            # ì‹œë§¨í‹± ê·œì¹™ ì ìš©
                            semantic_class = self._apply_semantic_rules(field_name, value, mapping.semantic_rules)
                            if semantic_class:
                                semantic_classes[field_name] = semantic_class
                                
                    # RDF ê·¸ë˜í”„ì— ì¶”ê°€
                    if self.enable_ontology:
                        self._add_to_rdf_graph(record_id, mapped_properties, semantic_classes)
                        
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    self._save_ontology_instance(record_id, mapped_properties, semantic_classes)
                    
                    mapped_count += 1
                    
                except Exception as e:
                    mapping_results['mapping_errors'].append({
                        'record_id': record_id,
                        'error': str(e)
                    })
                    
            mapping_results['mapped_records'] = mapped_count
            mapping_results['mapping_success_rate'] = (mapped_count / len(data)) * 100
            
            if self.enable_ontology:
                mapping_results['rdf_graph_size'] = len(self.graph)
                
        except Exception as e:
            self.logger.error(f"ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            
        return mapping_results
        
    def _transform_value(self, value: Any, transform_type: Optional[str]) -> Any:
        """ë°ì´í„° ë³€í™˜"""
        if not transform_type:
            return value
            
        try:
            if transform_type == 'integer_to_xsd':
                return int(float(value)) if pd.notna(value) else 0
            elif transform_type == 'decimal_to_xsd':
                return float(value) if pd.notna(value) else 0.0
            elif transform_type == 'datetime_to_xsd':
                if isinstance(value, str):
                    return pd.to_datetime(value).isoformat()
                elif isinstance(value, (date, datetime)):
                    return value.isoformat()
                return str(value)
        except:
            pass
            
        return value
        
    def _apply_semantic_rules(self, field_name: str, value: Any, rules: List[str]) -> Optional[str]:
        """ì‹œë§¨í‹± ê·œì¹™ ì ìš©"""
        for rule in rules:
            if rule == 'warehouse_type_classification':
                if 'Indoor' in str(value):
                    return 'IndoorWarehouse'
                elif 'Outdoor' in str(value):
                    return 'OutdoorWarehouse'
                elif 'Site' in str(value):
                    return 'Site'
                else:
                    return 'Warehouse'
                    
            elif rule == 'cargo_type_hierarchy':
                if 'HITACHI' in str(value).upper() or 'HE' in str(value):
                    return 'HitachiCargo'
                elif 'SIEMENS' in str(value).upper() or 'SIM' in str(value):
                    return 'SiemensCargo'
                else:
                    return 'Cargo'
                    
            elif rule == 'weight_risk_classification':
                if isinstance(value, (int, float)):
                    if value > 5000:  # 5í†¤ ì´ìƒ
                        return 'HighRiskWeight'
                    elif value > 1000:  # 1í†¤ ì´ìƒ
                        return 'MediumRiskWeight'
                    else:
                        return 'StandardWeight'
                        
            elif rule == 'cost_classification':
                if isinstance(value, (int, float)):
                    if value > 100000:  # 10ë§Œ AED ì´ìƒ
                        return 'HighCost'
                    elif value > 10000:  # 1ë§Œ AED ì´ìƒ
                        return 'MediumCost'
                    else:
                        return 'StandardCost'
                        
        return None
        
    def _calculate_confidence_score(self, results: Dict[str, Any]) -> float:
        """MACHO-GPT ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (â‰¥90% ëª©í‘œ)"""
        base_score = results.get('validation_rate', 0)
        quality_score = results.get('overall_quality_score', 0)
        
        if 'ontology_mapping' in results:
            mapping_rate = results['ontology_mapping'].get('mapping_success_rate', 0)
            confidence = (base_score * 0.4 + quality_score * 0.3 + mapping_rate * 0.3)
        else:
            confidence = (base_score * 0.6 + quality_score * 0.4)
            
        return min(confidence, 100.0)
        
    def export_ontology_report(self, validation_summary: Dict[str, Any], output_path: str = None) -> str:
        """ì˜¨í†¨ë¡œì§€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"MACHO_GPT_ì˜¨í†¨ë¡œì§€í†µí•©ë¦¬í¬íŠ¸_{timestamp}.xlsx"
            
        # ... ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§ êµ¬í˜„ ...
        
        return output_path

# MACHO-GPT v3.4-mini í˜¸í™˜ì„± í™•ì¸
def validate_macho_compatibility():
    """MACHO-GPT v3.4-mini í˜¸í™˜ì„± ê²€ì¦"""
    print("ğŸ”§ MACHO-GPT v3.4-mini ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ í˜¸í™˜ì„± ê²€ì¦")
    
    # ì˜ì¡´ì„± í™•ì¸
    dependencies = {
        'pandas': True,
        'numpy': True, 
        'rdflib': RDF_AVAILABLE,
        'sqlite3': True
    }
    
    print("ğŸ“¦ **ì˜ì¡´ì„± í™•ì¸:**")
    for dep, available in dependencies.items():
        status = "âœ…" if available else "âŒ"
        print(f"   {status} {dep}")
        
    # ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ
    ready_score = sum(dependencies.values()) / len(dependencies) * 100
    print(f"\nğŸ¯ **ì‹œìŠ¤í…œ ì¤€ë¹„ë„: {ready_score:.1f}%**")
    
    if ready_score >= 75:
        print("âœ… Production Ready - í”„ë¡œë•ì…˜ ë°°í¬ ê°€ëŠ¥")
    elif ready_score >= 50:
        print("âš ï¸ Limited Mode - ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ìš´ì˜ ê°€ëŠ¥")
    else:
        print("âŒ Setup Required - ì¶”ê°€ ì„¤ì • í•„ìš”")
        
    return ready_score

if __name__ == "__main__":
    validate_macho_compatibility()
    print("\n" + "="*70)
    print("MACHO-GPT v3.4-mini ì˜¨í†¨ë¡œì§€ í†µí•© ì‹œìŠ¤í…œ - BACKUP v1.0.0")
    print("Samsung C&T Ã— ADNOCÂ·DSV Partnership | HVDC Project")
    print("="*70) 